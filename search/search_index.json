{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Abstract Dataloader: Dataloader Not Included","text":""},{"location":"#what-is-the-abstract-dataloader","title":"What is the Abstract Dataloader?","text":"Why Abstract? <p>Loading, preprocessing, and training models on time-series data is ubiquitous in machine learning for cyber-physical systems. However, unlike mainstream machine learning research, which has largely standardized around \"canonical modalities\" in computer vision (RGB images) and natural language processing (ordinary unstructured text), each new setting, dataset, and modality comes with a new set of tasks, questions, challenges - and data types which must be loaded and processed.</p> <p>This poses a substantial software engineering challenge. With many different modalities, processing algorithms which operate on the power set of those different modalities, and downstream tasks which also each depend on some subset of modalities, two undesirable potential outcomes emerge:</p> <ol> <li>Data loading and processing components fragment into an exponential number of incompatible chunks, each of which encapsulates its required loading and processing functionality in a slightly different way. The barrier this presents to rapid prototyping needs no further explanation.</li> <li>The various software components coalesce into a monolith which nominally supports the power set of all functionality. However, in addition to the compatibility issues that come with bundling heterogeneous requirements such as managing \"non-dependencies\" (i.e. dependencies which are required by the monolith, but not a particular task), this also presents a hidden challenge in that by support exponentially many possible configurations, such an architecture is also exponentially hard to debug and verify.</li> </ol> <p>However, we do not believe that these outcomes are a foregone conclusion. In particular, we believe that it's possible to write \"one true dataloader\" which can scale while maintaining intercompability by not writing a common dataloader at all -- but rather a common specification for writing dataloaders. We call this the \"abstract dataloader\".</p> <p>The abstract dataloader (ADL) is a minimalist specification for creating composable and interoperable dataloaders and data transformations, along with abstract template implementations and reusable generic components, including a pytorch interface.</p> <p></p> <p>The ADL's specifications and bundled implementations lean heavily on generic type annotations in order to enable type checking using static type checkers such as mypy or pyright and runtime (dynamic) type checkers such as beartype and typeguard, even when applying functor-like generic transforms such as sequence loading and transforms.</p> <p>Structural Subtyping</p> <p>Since the abstract dataloader uses python's structural subtyping - <code>Protocol</code> - feature, the <code>abstract_dataloader</code> is not a required dependency for using the abstract dataloader! Implementations which follow the specifications are fully interoperable, including with type checkers, even if they do not have any mutual dependencies - including this library.</p> <p>Type Checking is Optional</p> <p>Static and runtime type checking are fully optional, in line with Python's gradual typing paradigm. Users also do not need to fully define the abstract dataloader's typed interfaces: for example, specifying a <code>Sensor</code> instead of a <code>Sensor[TData, TMetadata]</code> is perfectly valid, as type checkers will simply interpret the sensor as loading <code>Any</code> data and accepting <code>Any</code> metadata.</p>"},{"location":"#setup","title":"Setup","text":"<p>While it is not necessary to install the <code>abstract_dataloader</code> in order to take advantage of ADL-compliant components, installing this library provides access to <code>Protocol</code> types which describe each interface, as well as generic components which may be useful for working with ADL-compliant components.</p> PyPIGithub <p>To get the latest version: <pre><code>pip install abstract-dataloader\n</code></pre></p> <p>To get the latest development version: <pre><code>pip install git+git@github.com:WiseLabCMU/abstract-dataloader.git\n</code></pre></p> <p>Missing Component?</p> <p>If you have any ideas or requests for commonly used, generic components to add to <code>abstract_dataloader.generic</code>, please open an issue!</p>"},{"location":"#dependencies","title":"Dependencies","text":"<p>As an explicit goal is to minimize dependency constraints, only the following dependencies are required:</p> <ul> <li> <p><code>python &gt;= 3.10</code>: a somewhat recent version of python is required, since the python type annotation specifications are rapidly evolving. </p> </li> <li> <p><code>numpy &gt;= 1.14</code>: any remotely recent version of numpy is compatible, with the <code>1.14</code> minimum version only being required since this version first defined the <code>np.integer</code> type.</p> </li> <li> <p><code>jaxtyping &gt;= 0.2.32</code>: a fairly recent version of jaxtyping is also required due to the rapid pace of type annotation tooling. In particular, <code>jaxtyping 0.2.32</code> added support for <code>TypeVar</code> as array types, which is helpful for expressing array type polymorphisms.</p> </li> <li> <p><code>typing_extensions &gt;= 3.12</code>: we pull forward typing features from Python 3.12. This minimum version may be increased as we use newer typing features.</p> </li> </ul> <p>Minimum Python Version</p> <p>We may consider upgrading our minimum python version in the future, since <code>3.11</code> and newer versions support useful typing-related features such as the <code>Self</code> type.</p> <p>Pytorch Integration</p> <p>To use the optional pytorch integrations, we also require either <code>torch &gt;= 2.2</code> (first version to add <code>torch.utils._pytree.tree_leaves</code>) or <code>torch</code> and <code>optree &gt;= 0.13</code> (first \"mostly stable\" version) in order to have access to a fully-featured tree manipulation module. The included <code>torch</code> extra will install the latest pytorch and optree, with constraints <code>torch &gt;= 2.2</code> and <code>optree &gt;= 0.13</code>.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Please report any bugs, type-related issues/inconsistencies, and feel free to suggest new generic components! Any issues or PRs are welcome.</p> EnvironmentRun TestsBuild Docs <p>Our development environment uses uv; assuming you have uv installed, you can set up the environment (and install the pre-commit hooks) with <pre><code>uv sync --extra dev\nuv run pre-commit install\n</code></pre></p> <p>Info</p> <p>You can test the hooks with <code>uv run pre-commit run</code>; these hooks (<code>ruff</code> + <code>pyright</code> + <code>pytest</code>) mirror the CI.</p> <p>Run tests with <pre><code>uv run --extra dev pytest -ra --cov --cov-report=html --cov-report=term -- tests\n</code></pre></p> <p>Tip</p> <p>You can serve a live copy of the coverage report with <pre><code>uv run python -m http.server 8001 -d htmlcov\n</code></pre></p> <p>Build docs with <pre><code>uv run --extra docs mkdocs serve\n</code></pre></p> <p>Push to github pages with <pre><code>uv run --extra docs mkdocs build\n./update_gh_pages.sh\n</code></pre></p> <p>Info</p> <p>The documentation builder fetches external inventories (i.e., <code>objects.inv</code>) in order to properly link external references. This requires internet access; if behind a firewall, make sure that these inventories are not blocked!</p>"},{"location":"abstract/","title":"Abstract","text":""},{"location":"abstract/#abstract_dataloader.abstract","title":"Abstract Base Classes","text":"<p>Abstract Dataloader Generic/Abstract Implementations.</p> <p>The implementations here provide abstract implementations of commonly reusable functions such as multi-trace datasets, and glue logic for synchronization.</p> <ul> <li>Where applicable, \"polyfill\" fallbacks also implement some     methods in terms of more basic ones to allow for extending implementations     to be more minimal, while still covering required functionality.</li> <li>In cases where fallbacks are sufficient to provide a minimal, non-crashing     implementation of the spec, we omit the <code>ABC</code> base class so that     the class is not technically abstract (though it still may be abstract,     in the sense that it may not be meaningful to use it directly.)</li> </ul> <p>Some other convenience methods are also provided which are not included in the core spec; software using the abstract data loader should not rely on these, and should always base their code on the <code>spec</code> types.</p> <p>Fallback</p> <p>Abstract base classes which provide default or \"fallback\" behavior, e.g. implementing some methods in terms of others, are documented with a <code>Fallback</code> section.</p> <p>Note</p> <p>Classes without separate abstract implementations are also aliased to their original protocol definitions, so that <code>abstract_dataloader.abstract</code> exposes an identical set of objects as <code>abstract_dataloader.spec</code>.</p>"},{"location":"abstract/#abstract_dataloader.abstract.Metadata","title":"abstract_dataloader.abstract.Metadata","text":"<p>               Bases: <code>Protocol</code></p> <p>Sensor metadata.</p> <p>All sensor metadata is expected to be held in memory during training, so great effort should be taken to minimize its memory usage. Any additional information which is not strictly necessary for book-keeping, or which takes more than negligible space, should be loaded as data instead.</p> <p>Note</p> <p>This can be a <code>@dataclass</code>, <code>typing.NamedTuple</code>, or a fully custom type - it just has to expose a <code>timestamps</code> attribute.</p> <p>Attributes:</p> Name Type Description <code>timestamps</code> <code>Float[ndarray, N]</code> <p>measurement timestamps, in seconds. Nominally in epoch time; must be consistent within each trace (but not necessarily across traces). Suggested type: <code>float64,</code> which gives precision of &lt;1us.</p> Source code in <code>src/abstract_dataloader/spec.py</code> <pre><code>@runtime_checkable\nclass Metadata(Protocol):\n    \"\"\"Sensor metadata.\n\n    All sensor metadata is expected to be held in memory during training, so\n    great effort should be taken to minimize its memory usage. Any additional\n    information which is not strictly necessary for book-keeping, or which\n    takes more than negligible space, should be loaded as data instead.\n\n    !!! note\n\n        This can be a `@dataclass`, [`typing.NamedTuple`][typing.NamedTuple],\n        or a fully custom type - it just has to expose a `timestamps`\n        attribute.\n\n    Attributes:\n        timestamps: measurement timestamps, in seconds. Nominally in epoch\n            time; must be consistent within each trace (but not necessarily\n            across traces). Suggested type: `float64,` which gives precision of\n            &lt;1us.\n    \"\"\"\n\n    timestamps: Float[np.ndarray, \"N\"]\n</code></pre>"},{"location":"abstract/#abstract_dataloader.abstract.Sensor","title":"abstract_dataloader.abstract.Sensor","text":"<p>               Bases: <code>ABC</code>, <code>Sensor[TSample, TMetadata]</code></p> <p>Abstract Sensor Implementation.</p> Type Parameters <ul> <li><code>TSample</code>: sample data type which this <code>Sensor</code> returns. As a     convention, we suggest returning \"batched\" data by default, i.e.     with a leading singleton axis.</li> <li><code>TMetadata</code>: metadata type associated with this sensor; must     implement <code>Metadata</code>.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>TMetadata</code> <p>sensor metadata, including timestamp information; must implement <code>Metadata</code>.</p> required <code>name</code> <code>str</code> <p>friendly name; should only be used for debugging and inspection.</p> <code>'sensor'</code> Source code in <code>src/abstract_dataloader/abstract.py</code> <pre><code>class Sensor(ABC, spec.Sensor[TSample, TMetadata]):\n    \"\"\"Abstract Sensor Implementation.\n\n    Type Parameters:\n        - `TSample`: sample data type which this `Sensor` returns. As a\n            convention, we suggest returning \"batched\" data by default, i.e.\n            with a leading singleton axis.\n        - `TMetadata`: metadata type associated with this sensor; must\n            implement [`Metadata`][abstract_dataloader.spec.].\n\n    Args:\n        metadata: sensor metadata, including timestamp information; must\n            implement [`Metadata`][abstract_dataloader.spec.].\n        name: friendly name; should only be used for debugging and inspection.\n    \"\"\"\n\n    def __init__(self, metadata: TMetadata, name: str = \"sensor\") -&gt; None:\n        self.metadata = metadata\n        self.name = name\n\n    @overload\n    def stream(self, batch: None = None) -&gt; Iterator[TSample]: ...\n\n    @overload\n    def stream(self, batch: int) -&gt; Iterator[list[TSample]]: ...\n\n    def stream(\n        self, batch: int | None = None\n    ) -&gt; Iterator[TSample | list[TSample]]:\n        \"\"\"Stream values recorded by this sensor.\n\n        Fallback:\n            Manually iterate through one sample at a time, loaded using the\n            provided `__getitem__` implementation.\n\n        Args:\n            batch: batch size; if `0`, returns single samples.\n\n        Returns:\n            Iterable of samples (or sequences of samples).\n        \"\"\"\n        if batch is None:\n            for i in range(len(self)):\n                yield self[i]\n        else:\n            for i in range(len(self) // batch):\n                yield [self[j] for j in range(i * batch, (i + 1) * batch)]\n\n    @abstractmethod\n    def __getitem__(\n        self, index: int | np.integer\n    ) -&gt; TSample:\n        \"\"\"Fetch measurements from this sensor, by index.\n\n        Args:\n            index: sample index.\n\n        Returns:\n            A single sample.\n        \"\"\"\n        ...\n\n    def __len__(self) -&gt; int:\n        \"\"\"Total number of measurements.\n\n        Fallback:\n            Return the length of the metadata timestamps.\n        \"\"\"\n        return self.metadata.timestamps.shape[0]\n\n    @property\n    def duration(self) -&gt; float:\n        \"\"\"Trace duration from the first to last sample, in seconds.\n\n        Fallback:\n            Compute using the first and last metadata timestamp.\n        \"\"\"\n        return self.metadata.timestamps[-1] - self.metadata.timestamps[0]\n\n    @property\n    def framerate(self) -&gt; float:\n        \"\"\"Framerate of this sensor, in samples/sec.\"\"\"\n        # `n` samples cover `n-1` periods!\n        return (len(self) - 1) / self.duration\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Get friendly representation for inspection and debugging.\"\"\"\n        return f\"{self.__class__.__name__}({self.name}, n={len(self)})\"\n</code></pre>"},{"location":"abstract/#abstract_dataloader.abstract.Sensor.duration","title":"duration  <code>property</code>","text":"<pre><code>duration: float\n</code></pre> <p>Trace duration from the first to last sample, in seconds.</p> Fallback <p>Compute using the first and last metadata timestamp.</p>"},{"location":"abstract/#abstract_dataloader.abstract.Sensor.framerate","title":"framerate  <code>property</code>","text":"<pre><code>framerate: float\n</code></pre> <p>Framerate of this sensor, in samples/sec.</p>"},{"location":"abstract/#abstract_dataloader.abstract.Sensor.__getitem__","title":"__getitem__  <code>abstractmethod</code>","text":"<pre><code>__getitem__(index: int | integer) -&gt; TSample\n</code></pre> <p>Fetch measurements from this sensor, by index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int | integer</code> <p>sample index.</p> required <p>Returns:</p> Type Description <code>TSample</code> <p>A single sample.</p> Source code in <code>src/abstract_dataloader/abstract.py</code> <pre><code>@abstractmethod\ndef __getitem__(\n    self, index: int | np.integer\n) -&gt; TSample:\n    \"\"\"Fetch measurements from this sensor, by index.\n\n    Args:\n        index: sample index.\n\n    Returns:\n        A single sample.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"abstract/#abstract_dataloader.abstract.Sensor.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Total number of measurements.</p> Fallback <p>Return the length of the metadata timestamps.</p> Source code in <code>src/abstract_dataloader/abstract.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Total number of measurements.\n\n    Fallback:\n        Return the length of the metadata timestamps.\n    \"\"\"\n    return self.metadata.timestamps.shape[0]\n</code></pre>"},{"location":"abstract/#abstract_dataloader.abstract.Sensor.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> <p>Get friendly representation for inspection and debugging.</p> Source code in <code>src/abstract_dataloader/abstract.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Get friendly representation for inspection and debugging.\"\"\"\n    return f\"{self.__class__.__name__}({self.name}, n={len(self)})\"\n</code></pre>"},{"location":"abstract/#abstract_dataloader.abstract.Sensor.stream","title":"stream","text":"<pre><code>stream(batch: None = None) -&gt; Iterator[TSample]\n</code></pre><pre><code>stream(batch: int) -&gt; Iterator[list[TSample]]\n</code></pre> <pre><code>stream(batch: int | None = None) -&gt; Iterator[TSample | list[TSample]]\n</code></pre> <p>Stream values recorded by this sensor.</p> Fallback <p>Manually iterate through one sample at a time, loaded using the provided <code>__getitem__</code> implementation.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>int | None</code> <p>batch size; if <code>0</code>, returns single samples.</p> <code>None</code> <p>Returns:</p> Type Description <code>Iterator[TSample | list[TSample]]</code> <p>Iterable of samples (or sequences of samples).</p> Source code in <code>src/abstract_dataloader/abstract.py</code> <pre><code>def stream(\n    self, batch: int | None = None\n) -&gt; Iterator[TSample | list[TSample]]:\n    \"\"\"Stream values recorded by this sensor.\n\n    Fallback:\n        Manually iterate through one sample at a time, loaded using the\n        provided `__getitem__` implementation.\n\n    Args:\n        batch: batch size; if `0`, returns single samples.\n\n    Returns:\n        Iterable of samples (or sequences of samples).\n    \"\"\"\n    if batch is None:\n        for i in range(len(self)):\n            yield self[i]\n    else:\n        for i in range(len(self) // batch):\n            yield [self[j] for j in range(i * batch, (i + 1) * batch)]\n</code></pre>"},{"location":"abstract/#abstract_dataloader.abstract.Synchronization","title":"abstract_dataloader.abstract.Synchronization","text":"<p>               Bases: <code>Protocol</code></p> <p>Synchronization protocol for asynchronous time-series.</p> <p>Defines a rule for creating matching sensor index tuples which correspond to some kind of global index.</p> <p>Generic Implementations</p> <p>The following generic implementations are included with <code>abstract_dataloader.generic</code>:</p> <ul> <li><code>Empty</code>: a no-op for     intializing a trace without any synchronization (i.e., just as a     container of sensors).</li> <li><code>Nearest</code>: find the nearest     measurement for each sensor relative to the reference sensor's     measurements.</li> <li><code>Next</code>: find the next     measurement for each sensor relative to the reference sensor's     measurements.</li> </ul> Source code in <code>src/abstract_dataloader/spec.py</code> <pre><code>@runtime_checkable\nclass Synchronization(Protocol):\n    \"\"\"Synchronization protocol for asynchronous time-series.\n\n    Defines a rule for creating matching sensor index tuples which correspond\n    to some kind of global index.\n\n    !!! info \"Generic Implementations\"\n\n        The following generic implementations are included with\n        [`abstract_dataloader.generic`][abstract_dataloader.generic]:\n\n        - [`Empty`][abstract_dataloader.generic.Empty]: a no-op for\n            intializing a trace without any synchronization (i.e., just as a\n            container of sensors).\n        - [`Nearest`][abstract_dataloader.generic.Nearest]: find the nearest\n            measurement for each sensor relative to the reference sensor's\n            measurements.\n        - [`Next`][abstract_dataloader.generic.Next]: find the next\n            measurement for each sensor relative to the reference sensor's\n            measurements.\n    \"\"\"\n\n    def __call__(\n        self, timestamps: dict[str, Float[np.ndarray, \"_N\"]]\n    ) -&gt; dict[str, Integer[np.ndarray, \"M\"]]:\n        \"\"\"Apply synchronization protocol.\n\n        Args:\n            timestamps: sensor timestamps. Each key denotes a different sensor\n                name, and the value denotes the timestamps for that sensor.\n\n        Returns:\n            A dictionary, where keys correspond to each sensor, and values\n                correspond to the indices which map global indices to sensor\n                indices, i.e. `global[sensor, i] = sensor[sync[sensor] [i]]`.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"abstract/#abstract_dataloader.abstract.Synchronization.__call__","title":"__call__","text":"<pre><code>__call__(\n    timestamps: dict[str, Float[ndarray, _N]],\n) -&gt; dict[str, Integer[ndarray, M]]\n</code></pre> <p>Apply synchronization protocol.</p> <p>Parameters:</p> Name Type Description Default <code>timestamps</code> <code>dict[str, Float[ndarray, _N]]</code> <p>sensor timestamps. Each key denotes a different sensor name, and the value denotes the timestamps for that sensor.</p> required <p>Returns:</p> Type Description <code>dict[str, Integer[ndarray, M]]</code> <p>A dictionary, where keys correspond to each sensor, and values correspond to the indices which map global indices to sensor indices, i.e. <code>global[sensor, i] = sensor[sync[sensor] [i]]</code>.</p> Source code in <code>src/abstract_dataloader/spec.py</code> <pre><code>def __call__(\n    self, timestamps: dict[str, Float[np.ndarray, \"_N\"]]\n) -&gt; dict[str, Integer[np.ndarray, \"M\"]]:\n    \"\"\"Apply synchronization protocol.\n\n    Args:\n        timestamps: sensor timestamps. Each key denotes a different sensor\n            name, and the value denotes the timestamps for that sensor.\n\n    Returns:\n        A dictionary, where keys correspond to each sensor, and values\n            correspond to the indices which map global indices to sensor\n            indices, i.e. `global[sensor, i] = sensor[sync[sensor] [i]]`.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"abstract/#abstract_dataloader.abstract.Trace","title":"abstract_dataloader.abstract.Trace","text":"<p>               Bases: <code>Trace[TSample]</code></p> <p>A trace, consisting of multiple simultaneously-recording sensors.</p> Type Parameters <p><code>Sample</code>: sample data type which this <code>Sensor</code> returns. As a     convention, we suggest returning \"batched\" data by default, i.e.     with a leading singleton axis.</p> <p>Parameters:</p> Name Type Description Default <code>sensors</code> <code>Mapping[str, Sensor]</code> <p>sensors which make up this trace.</p> required <code>sync</code> <code>Synchronization | Mapping[str, Integer[ndarray, N]] | None</code> <p>synchronization protocol used to create global samples from asynchronous time series. If <code>Mapping</code>; the provided indices are used directly; if <code>None</code>, sensors are expected to already be synchronous (equivalent to passing <code>{k: np.arange(N), ...}</code>).</p> <code>None</code> <code>name</code> <code>str</code> <p>friendly name; should only be used for debugging and inspection.</p> <code>'trace'</code> Source code in <code>src/abstract_dataloader/abstract.py</code> <pre><code>class Trace(spec.Trace[TSample]):\n    \"\"\"A trace, consisting of multiple simultaneously-recording sensors.\n\n    Type Parameters:\n        `Sample`: sample data type which this `Sensor` returns. As a\n            convention, we suggest returning \"batched\" data by default, i.e.\n            with a leading singleton axis.\n\n    Args:\n        sensors: sensors which make up this trace.\n        sync: synchronization protocol used to create global samples from\n            asynchronous time series. If `Mapping`; the provided indices are\n            used directly; if `None`, sensors are expected to already be\n            synchronous (equivalent to passing `{k: np.arange(N), ...}`).\n        name: friendly name; should only be used for debugging and inspection.\n    \"\"\"\n\n    def __init__(\n        self, sensors: Mapping[str, spec.Sensor],\n        sync: (\n            spec.Synchronization | Mapping[str, Integer[np.ndarray, \"N\"]]\n            | None) = None,\n        name: str = \"trace\"\n    ) -&gt; None:\n        self.sensors = sensors\n        self.name = name\n\n        if sync is None:\n            self.indices = None\n        elif isinstance(sync, Mapping):\n            self.indices = sync\n        else:\n            self.indices = sync(\n                {k: v.metadata.timestamps for k, v in sensors.items()})\n\n    @overload\n    def __getitem__(self, index: str) -&gt; Sensor: ...\n\n    @overload\n    def __getitem__(self, index: int | np.integer) -&gt; TSample: ...\n\n    def __getitem__(\n        self, index: int | np.integer | str\n    ) -&gt; TSample | spec.Sensor:\n        \"\"\"Get item from global index (or fetch a sensor by name).\n\n        !!! tip\n\n            For convenience, traces can be indexed by a `str` sensor name,\n            returning that [`Sensor`][abstract_dataloader.spec.].\n\n        Fallback:\n            Reference implementation which uses the computed\n            [`Synchronization`][abstract_dataloader.spec] to retrieve the\n            matching indices from each sensor. The returned samples have\n            sensor names as keys, and loaded data as values, matching the\n            format provided as the `sensors` parameter:\n\n            ```python\n            trace[i] = {\n                \"sensor_a\": sensor_a[synchronized_indices[\"sensor_a\"] [i]],\n                \"sensor_b\": sensor_a[synchronized_indices[\"sensor_b\"] [i]],\n                ...\n            }\n            ```\n\n        Args:\n            index: sample index, or sensor name.\n\n        Returns:\n            Loaded sample if `index` is an integer type, or the appropriate\n            [`Sensor`][abstract_dataloader.spec.] if `index` is a `str`.\n        \"\"\"\n        if isinstance(index, str):\n            return self.sensors[index]\n\n        if self.indices is None:\n            return cast(TSample, {\n                k: v[index] for k, v in self.sensors.items()})\n        else:\n            return cast(TSample, {\n                k: v[self.indices[k][index].item()]\n                for k, v in self.sensors.items()})\n\n    def __len__(self) -&gt; int:\n        \"\"\"Total number of sensor-tuple samples.\n\n        Fallback:\n            Returns the number of synchronized index tuples.\n        \"\"\"\n        if self.indices is None:\n            return len(list(self.sensors.values())[0])\n        else:\n            return list(self.indices.values())[0].shape[0]\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Friendly representation.\"\"\"\n        sensors = \", \".join(self.sensors.keys())\n        return (\n            f\"{self.__class__.__name__}({self.name}, {len(self)}x[{sensors}])\")\n</code></pre>"},{"location":"abstract/#abstract_dataloader.abstract.Trace.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(index: str) -&gt; Sensor\n</code></pre><pre><code>__getitem__(index: int | integer) -&gt; TSample\n</code></pre> <pre><code>__getitem__(index: int | integer | str) -&gt; TSample | Sensor\n</code></pre> <p>Get item from global index (or fetch a sensor by name).</p> <p>Tip</p> <p>For convenience, traces can be indexed by a <code>str</code> sensor name, returning that <code>Sensor</code>.</p> Fallback <p>Reference implementation which uses the computed <code>Synchronization</code> to retrieve the matching indices from each sensor. The returned samples have sensor names as keys, and loaded data as values, matching the format provided as the <code>sensors</code> parameter:</p> <pre><code>trace[i] = {\n    \"sensor_a\": sensor_a[synchronized_indices[\"sensor_a\"] [i]],\n    \"sensor_b\": sensor_a[synchronized_indices[\"sensor_b\"] [i]],\n    ...\n}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int | integer | str</code> <p>sample index, or sensor name.</p> required <p>Returns:</p> Type Description <code>TSample | Sensor</code> <p>Loaded sample if <code>index</code> is an integer type, or the appropriate</p> <code>TSample | Sensor</code> <p><code>Sensor</code> if <code>index</code> is a <code>str</code>.</p> Source code in <code>src/abstract_dataloader/abstract.py</code> <pre><code>def __getitem__(\n    self, index: int | np.integer | str\n) -&gt; TSample | spec.Sensor:\n    \"\"\"Get item from global index (or fetch a sensor by name).\n\n    !!! tip\n\n        For convenience, traces can be indexed by a `str` sensor name,\n        returning that [`Sensor`][abstract_dataloader.spec.].\n\n    Fallback:\n        Reference implementation which uses the computed\n        [`Synchronization`][abstract_dataloader.spec] to retrieve the\n        matching indices from each sensor. The returned samples have\n        sensor names as keys, and loaded data as values, matching the\n        format provided as the `sensors` parameter:\n\n        ```python\n        trace[i] = {\n            \"sensor_a\": sensor_a[synchronized_indices[\"sensor_a\"] [i]],\n            \"sensor_b\": sensor_a[synchronized_indices[\"sensor_b\"] [i]],\n            ...\n        }\n        ```\n\n    Args:\n        index: sample index, or sensor name.\n\n    Returns:\n        Loaded sample if `index` is an integer type, or the appropriate\n        [`Sensor`][abstract_dataloader.spec.] if `index` is a `str`.\n    \"\"\"\n    if isinstance(index, str):\n        return self.sensors[index]\n\n    if self.indices is None:\n        return cast(TSample, {\n            k: v[index] for k, v in self.sensors.items()})\n    else:\n        return cast(TSample, {\n            k: v[self.indices[k][index].item()]\n            for k, v in self.sensors.items()})\n</code></pre>"},{"location":"abstract/#abstract_dataloader.abstract.Trace.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Total number of sensor-tuple samples.</p> Fallback <p>Returns the number of synchronized index tuples.</p> Source code in <code>src/abstract_dataloader/abstract.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Total number of sensor-tuple samples.\n\n    Fallback:\n        Returns the number of synchronized index tuples.\n    \"\"\"\n    if self.indices is None:\n        return len(list(self.sensors.values())[0])\n    else:\n        return list(self.indices.values())[0].shape[0]\n</code></pre>"},{"location":"abstract/#abstract_dataloader.abstract.Trace.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> <p>Friendly representation.</p> Source code in <code>src/abstract_dataloader/abstract.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Friendly representation.\"\"\"\n    sensors = \", \".join(self.sensors.keys())\n    return (\n        f\"{self.__class__.__name__}({self.name}, {len(self)}x[{sensors}])\")\n</code></pre>"},{"location":"abstract/#abstract_dataloader.abstract.Dataset","title":"abstract_dataloader.abstract.Dataset","text":"<p>               Bases: <code>Dataset[TSample]</code></p> <p>A dataset, consisting of multiple traces, nominally concatenated.</p> Type Parameters <p><code>Sample</code>: sample data type which this <code>Sensor</code> returns. As a     convention, we suggest returning \"batched\" data by default, i.e.     with a leading singleton axis.</p> <p>Parameters:</p> Name Type Description Default <code>traces</code> <code>Sequence[Trace[TSample]]</code> <p>traces which make up this dataset.</p> required Source code in <code>src/abstract_dataloader/abstract.py</code> <pre><code>class Dataset(spec.Dataset[TSample]):\n    \"\"\"A dataset, consisting of multiple traces, nominally concatenated.\n\n    Type Parameters:\n        `Sample`: sample data type which this `Sensor` returns. As a\n            convention, we suggest returning \"batched\" data by default, i.e.\n            with a leading singleton axis.\n\n    Args:\n        traces: traces which make up this dataset.\n    \"\"\"\n\n    def __init__(self, traces: Sequence[spec.Trace[TSample]]) -&gt; None:\n        self.traces = traces\n\n    @cached_property\n    def indices(self) -&gt; Int64[np.ndarray, \"N\"]:\n        \"\"\"End indices of each trace, with respect to global indices.\"\"\"\n        lengths = np.array([len(t) for t in self.traces], dtype=np.int64)\n        return np.cumsum(lengths)\n\n    def __getitem__(self, index: int | np.integer) -&gt; TSample:\n        \"\"\"Fetch item from this dataset by global index.\n\n        !!! bug \"Unsigned integer subtraction promotes to `np.float64`\"\n\n            Subtracting unsigned integers may cause numpy to promote the result\n            to a floating point number. Extending implementations should be\n            careful about this behavior!\n\n            In the default implementation here, we make sure that the computed\n            indices are `int64` instead of `uint64`, and always cast the input\n            to an `int64`.\n\n        Fallback:\n            Supports (and assumes) random accesses; maps to datasets using\n            `np.searchsorted` to search against pre-computed trace start\n            indices ([`indices`][^.]), which costs on the order of 10-100us\n            per call @ 100k traces.\n\n        Args:\n            index: sample index.\n\n        Returns:\n            loaded sample.\n\n        Raises:\n            IndexError: provided index is out of bounds.\n        \"\"\"\n        if index &lt; 0 or index &gt;= len(self):\n            raise IndexError(\n                f\"Index {index} is out of bounds for dataset with length \"\n                f\"{len(self)}.\")\n\n        if isinstance(index, np.integer):\n            index = np.int64(index)\n\n        trace = np.searchsorted(self.indices, index, side=\"right\")\n        if trace &gt; 0:\n            remainder = index - self.indices[trace - 1]\n        else:\n            remainder = index\n        # We have to ignore type here since python's Sequence type is not\n        # well defined, i.e., does not allow `np.integer` indexing even though\n        # `np.integer` is interchangeable with `int`.\n        return self.traces[trace][remainder]  # type: ignore\n\n    def __len__(self) -&gt; int:\n        \"\"\"Total number of samples in this dataset.\n\n        Fallback:\n            Fetch the dataset length from the trace start indices (at the cost\n            of triggering index computation).\n        \"\"\"\n        return self.indices[-1].item()\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Friendly representation.\"\"\"\n        return (\n            f\"{self.__class__.__name__}\"\n            f\"({len(self.traces)} traces, n={len(self)})\")\n</code></pre>"},{"location":"abstract/#abstract_dataloader.abstract.Dataset.indices","title":"indices  <code>cached</code> <code>property</code>","text":"<pre><code>indices: Int64[ndarray, N]\n</code></pre> <p>End indices of each trace, with respect to global indices.</p>"},{"location":"abstract/#abstract_dataloader.abstract.Dataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(index: int | integer) -&gt; TSample\n</code></pre> <p>Fetch item from this dataset by global index.</p> <p>Unsigned integer subtraction promotes to <code>np.float64</code></p> <p>Subtracting unsigned integers may cause numpy to promote the result to a floating point number. Extending implementations should be careful about this behavior!</p> <p>In the default implementation here, we make sure that the computed indices are <code>int64</code> instead of <code>uint64</code>, and always cast the input to an <code>int64</code>.</p> Fallback <p>Supports (and assumes) random accesses; maps to datasets using <code>np.searchsorted</code> to search against pre-computed trace start indices (<code>indices</code>), which costs on the order of 10-100us per call @ 100k traces.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int | integer</code> <p>sample index.</p> required <p>Returns:</p> Type Description <code>TSample</code> <p>loaded sample.</p> <p>Raises:</p> Type Description <code>IndexError</code> <p>provided index is out of bounds.</p> Source code in <code>src/abstract_dataloader/abstract.py</code> <pre><code>def __getitem__(self, index: int | np.integer) -&gt; TSample:\n    \"\"\"Fetch item from this dataset by global index.\n\n    !!! bug \"Unsigned integer subtraction promotes to `np.float64`\"\n\n        Subtracting unsigned integers may cause numpy to promote the result\n        to a floating point number. Extending implementations should be\n        careful about this behavior!\n\n        In the default implementation here, we make sure that the computed\n        indices are `int64` instead of `uint64`, and always cast the input\n        to an `int64`.\n\n    Fallback:\n        Supports (and assumes) random accesses; maps to datasets using\n        `np.searchsorted` to search against pre-computed trace start\n        indices ([`indices`][^.]), which costs on the order of 10-100us\n        per call @ 100k traces.\n\n    Args:\n        index: sample index.\n\n    Returns:\n        loaded sample.\n\n    Raises:\n        IndexError: provided index is out of bounds.\n    \"\"\"\n    if index &lt; 0 or index &gt;= len(self):\n        raise IndexError(\n            f\"Index {index} is out of bounds for dataset with length \"\n            f\"{len(self)}.\")\n\n    if isinstance(index, np.integer):\n        index = np.int64(index)\n\n    trace = np.searchsorted(self.indices, index, side=\"right\")\n    if trace &gt; 0:\n        remainder = index - self.indices[trace - 1]\n    else:\n        remainder = index\n    # We have to ignore type here since python's Sequence type is not\n    # well defined, i.e., does not allow `np.integer` indexing even though\n    # `np.integer` is interchangeable with `int`.\n    return self.traces[trace][remainder]  # type: ignore\n</code></pre>"},{"location":"abstract/#abstract_dataloader.abstract.Dataset.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Total number of samples in this dataset.</p> Fallback <p>Fetch the dataset length from the trace start indices (at the cost of triggering index computation).</p> Source code in <code>src/abstract_dataloader/abstract.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Total number of samples in this dataset.\n\n    Fallback:\n        Fetch the dataset length from the trace start indices (at the cost\n        of triggering index computation).\n    \"\"\"\n    return self.indices[-1].item()\n</code></pre>"},{"location":"abstract/#abstract_dataloader.abstract.Dataset.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> <p>Friendly representation.</p> Source code in <code>src/abstract_dataloader/abstract.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Friendly representation.\"\"\"\n    return (\n        f\"{self.__class__.__name__}\"\n        f\"({len(self.traces)} traces, n={len(self)})\")\n</code></pre>"},{"location":"abstract/#abstract_dataloader.abstract.Transform","title":"abstract_dataloader.abstract.Transform","text":"<p>               Bases: <code>Transform[TRaw, TTransformed]</code></p> <p>Sample or batch data transform.</p> <p>Warning</p> <p>Transform types are not verified during initialization, and can only be verified using runtime type checkers when the transforms are applied.</p> Type Parameters <ul> <li><code>TRaw</code>: Input data type.</li> <li><code>TTransformed</code>: Output data type.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>transforms</code> <code>Sequence[Transform]</code> <p>transforms to apply sequentially; each output type must be the input type of the next transform.</p> required Source code in <code>src/abstract_dataloader/abstract.py</code> <pre><code>class Transform(spec.Transform[TRaw, TTransformed]):\n    \"\"\"Sample or batch data transform.\n\n    !!! warning\n\n        Transform types are not verified during initialization, and can only\n        be verified using runtime type checkers when the transforms are\n        applied.\n\n    Type Parameters:\n        - `TRaw`: Input data type.\n        - `TTransformed`: Output data type.\n\n    Args:\n        transforms: transforms to apply sequentially; each output type\n            must be the input type of the next transform.\n    \"\"\"\n\n    def __init__(self, transforms: Sequence[spec.Transform]) -&gt; None:\n        self.transforms = transforms\n\n    def __call__(self, data: TRaw) -&gt; TTransformed:\n        \"\"\"Apply transforms to a batch of samples.\n\n        Args:\n            data: A `TRaw` batch.\n\n        Returns:\n            A `TTransformed` batch.\n        \"\"\"\n        for tf in self.transforms:\n            data = tf(data)\n        return cast(TTransformed, data)\n</code></pre>"},{"location":"abstract/#abstract_dataloader.abstract.Transform.__call__","title":"__call__","text":"<pre><code>__call__(data: TRaw) -&gt; TTransformed\n</code></pre> <p>Apply transforms to a batch of samples.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>TRaw</code> <p>A <code>TRaw</code> batch.</p> required <p>Returns:</p> Type Description <code>TTransformed</code> <p>A <code>TTransformed</code> batch.</p> Source code in <code>src/abstract_dataloader/abstract.py</code> <pre><code>def __call__(self, data: TRaw) -&gt; TTransformed:\n    \"\"\"Apply transforms to a batch of samples.\n\n    Args:\n        data: A `TRaw` batch.\n\n    Returns:\n        A `TTransformed` batch.\n    \"\"\"\n    for tf in self.transforms:\n        data = tf(data)\n    return cast(TTransformed, data)\n</code></pre>"},{"location":"abstract/#abstract_dataloader.abstract.Collate","title":"abstract_dataloader.abstract.Collate","text":"<p>               Bases: <code>Collate[TTransformed, TCollated]</code></p> <p>Data collation.</p> Type Parameters <ul> <li><code>TTransformed</code>: Input data type.</li> <li><code>TCollated</code>: Output data type.</li> </ul> Source code in <code>src/abstract_dataloader/abstract.py</code> <pre><code>class Collate(spec.Collate[TTransformed, TCollated]):\n    \"\"\"Data collation.\n\n    Type Parameters:\n        - `TTransformed`: Input data type.\n        - `TCollated`: Output data type.\n    \"\"\"\n\n    def __call__(self, data: Sequence[TTransformed]) -&gt; TCollated:\n        \"\"\"Collate a set of samples.\n\n        Args:\n            data: A set of `TTransformed` samples.\n\n        Returns:\n            A `TCollated` batch.\n        \"\"\"\n        return cast(TCollated, data)\n</code></pre>"},{"location":"abstract/#abstract_dataloader.abstract.Collate.__call__","title":"__call__","text":"<pre><code>__call__(data: Sequence[TTransformed]) -&gt; TCollated\n</code></pre> <p>Collate a set of samples.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Sequence[TTransformed]</code> <p>A set of <code>TTransformed</code> samples.</p> required <p>Returns:</p> Type Description <code>TCollated</code> <p>A <code>TCollated</code> batch.</p> Source code in <code>src/abstract_dataloader/abstract.py</code> <pre><code>def __call__(self, data: Sequence[TTransformed]) -&gt; TCollated:\n    \"\"\"Collate a set of samples.\n\n    Args:\n        data: A set of `TTransformed` samples.\n\n    Returns:\n        A `TCollated` batch.\n    \"\"\"\n    return cast(TCollated, data)\n</code></pre>"},{"location":"abstract/#abstract_dataloader.abstract.Pipeline","title":"abstract_dataloader.abstract.Pipeline","text":"<p>               Bases: <code>Pipeline[TRaw, TTransformed, TCollated, TProcessed]</code></p> <p>Dataloader transform pipeline.</p> Composition Rules <ul> <li>A full <code>Pipeline</code> can be sequentially pre-composed and/or   post-composed with one or more <code>Transform</code>s; this is   implemented by <code>generic.ComposedPipeline</code>.</li> <li><code>Pipeline</code>s can always be composed in parallel; this is implemented   by <code>generic.ParallelPipelines</code>, with a   pytorch <code>nn.Module</code>-compatible version in   <code>torch.ParallelPipelines</code>.</li> </ul> Type Parameters <ul> <li><code>TRaw</code>: Input data format.</li> <li><code>TTransformed</code>: Data after the first <code>transform</code> step.</li> <li><code>TCollated</code>: Data after the second <code>collate</code> step.</li> <li><code>TProcessed</code>: Output data format.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>Transform[TRaw, TTransformed] | None</code> <p>sample transform; if <code>None</code>, the identity transform is used (or the default transform, if overridden).</p> <code>None</code> <code>collate</code> <code>Collate[TTransformed, TCollated] | None</code> <p>sample collation; if <code>None</code>, the provided default is used. Note that there is no fallback for collation, and <code>NotImplementedError</code> will be raised if none is provided.</p> <code>None</code> <code>batch</code> <code>Transform[TCollated, TProcessed] | None</code> <p>batch collation; if <code>None</code>, the identity transform is used.</p> <code>None</code> Source code in <code>src/abstract_dataloader/abstract.py</code> <pre><code>class Pipeline(\n    spec.Pipeline[TRaw, TTransformed, TCollated, TProcessed]\n):\n    \"\"\"Dataloader transform pipeline.\n\n    Composition Rules:\n        - A full `Pipeline` can be sequentially pre-composed and/or\n          post-composed with one or more [`Transform`][^.]s; this is\n          implemented by [`generic.ComposedPipeline`][abstract_dataloader.].\n        - `Pipeline`s can always be composed in parallel; this is implemented\n          by [`generic.ParallelPipelines`][abstract_dataloader.], with a\n          pytorch [`nn.Module`][torch.]-compatible version in\n          [`torch.ParallelPipelines`][abstract_dataloader.].\n\n    Type Parameters:\n        - `TRaw`: Input data format.\n        - `TTransformed`: Data after the first `transform` step.\n        - `TCollated`: Data after the second `collate` step.\n        - `TProcessed`: Output data format.\n\n    Args:\n        sample: sample transform; if `None`, the identity transform is used\n            (or the default transform, if overridden).\n        collate: sample collation; if `None`, the provided default is used.\n            Note that there is no fallback for collation, and\n            `NotImplementedError` will be raised if none is provided.\n        batch: batch collation; if `None`, the identity transform is used.\n    \"\"\"\n\n    def __init__(\n        self, sample: spec.Transform[TRaw, TTransformed] | None = None,\n        collate: spec.Collate[TTransformed, TCollated] | None = None,\n        batch: spec.Transform[TCollated, TProcessed] | None = None\n    ) -&gt; None:\n        if sample is not None:\n            self.sample = sample\n        if collate is not None:\n            self.collate = collate\n        if batch is not None:\n            self.batch = batch\n\n    def sample(self, data: TRaw) -&gt; TTransformed:\n        \"\"\"Transform single samples.\n\n        - Operates on single samples, nominally on the CPU-side of a\n          dataloader.\n        - This method is both sequentially and parallel composable.\n\n        Fallback:\n            The identity transform is provided by default\n            (`TTransformed = TRaw`).\n\n        Args:\n            data: A single `TRaw` data sample.\n\n        Returns:\n            A single `TTransformed` data sample.\n        \"\"\"\n        return cast(TTransformed, data)\n\n    def collate(self, data: Sequence[TTransformed]) -&gt; TCollated:\n        \"\"\"Collate a list of data samples into a GPU-ready batch.\n\n        - Operates on the CPU-side of the dataloader, and is responsible for\n          aggregating individual samples into a batch (but not transferring to\n          the GPU).\n        - Analogous to the `collate_fn` of a\n          [pytorch dataloader](https://pytorch.org/docs/stable/data.html).\n        - This method is not sequentially composable.\n\n        Args:\n            data: A sequence of `TTransformed` data samples.\n\n        Returns:\n            A `TCollated` collection of the input sequence.\n        \"\"\"\n        return cast(TCollated, data)\n\n    def batch(self, data: TCollated) -&gt; TProcessed:\n        \"\"\"Transform data batch.\n\n        - Operates on a batch of data, nominally on the GPU-side of a\n          dataloader.\n        - This method is both sequentially and parallel composable.\n\n        !!! info \"Implementation as `torch.nn.Module`\"\n\n            If this `Pipeline` requires GPU state, and the GPU components\n            are tied to CPU-side or collation functions (so cannot be\n            separated and implemented separately) it may be helpful to\n            implement the `Pipeline` as a `torch.nn.Module`. In this case,\n            `batch` should redirect to `__call__`, which in turn redirects to\n            [`nn.Module.forward`][torch.] in order to handle any registered\n            pytorch hooks.\n\n        Fallback:\n            The identity transform is provided by default\n            (`TProcessed = TCollated`).\n\n        Args:\n            data: A `TCollated` batch of data, nominally already sent to the\n                GPU.\n\n        Returns:\n            The `TProcessed` output, ready for the downstream model.\n        \"\"\"\n        return cast(TProcessed, data)\n</code></pre>"},{"location":"abstract/#abstract_dataloader.abstract.Pipeline.batch","title":"batch","text":"<pre><code>batch(data: TCollated) -&gt; TProcessed\n</code></pre> <p>Transform data batch.</p> <ul> <li>Operates on a batch of data, nominally on the GPU-side of a   dataloader.</li> <li>This method is both sequentially and parallel composable.</li> </ul> <p>Implementation as <code>torch.nn.Module</code></p> <p>If this <code>Pipeline</code> requires GPU state, and the GPU components are tied to CPU-side or collation functions (so cannot be separated and implemented separately) it may be helpful to implement the <code>Pipeline</code> as a <code>torch.nn.Module</code>. In this case, <code>batch</code> should redirect to <code>__call__</code>, which in turn redirects to <code>nn.Module.forward</code> in order to handle any registered pytorch hooks.</p> Fallback <p>The identity transform is provided by default (<code>TProcessed = TCollated</code>).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>TCollated</code> <p>A <code>TCollated</code> batch of data, nominally already sent to the GPU.</p> required <p>Returns:</p> Type Description <code>TProcessed</code> <p>The <code>TProcessed</code> output, ready for the downstream model.</p> Source code in <code>src/abstract_dataloader/abstract.py</code> <pre><code>def batch(self, data: TCollated) -&gt; TProcessed:\n    \"\"\"Transform data batch.\n\n    - Operates on a batch of data, nominally on the GPU-side of a\n      dataloader.\n    - This method is both sequentially and parallel composable.\n\n    !!! info \"Implementation as `torch.nn.Module`\"\n\n        If this `Pipeline` requires GPU state, and the GPU components\n        are tied to CPU-side or collation functions (so cannot be\n        separated and implemented separately) it may be helpful to\n        implement the `Pipeline` as a `torch.nn.Module`. In this case,\n        `batch` should redirect to `__call__`, which in turn redirects to\n        [`nn.Module.forward`][torch.] in order to handle any registered\n        pytorch hooks.\n\n    Fallback:\n        The identity transform is provided by default\n        (`TProcessed = TCollated`).\n\n    Args:\n        data: A `TCollated` batch of data, nominally already sent to the\n            GPU.\n\n    Returns:\n        The `TProcessed` output, ready for the downstream model.\n    \"\"\"\n    return cast(TProcessed, data)\n</code></pre>"},{"location":"abstract/#abstract_dataloader.abstract.Pipeline.collate","title":"collate","text":"<pre><code>collate(data: Sequence[TTransformed]) -&gt; TCollated\n</code></pre> <p>Collate a list of data samples into a GPU-ready batch.</p> <ul> <li>Operates on the CPU-side of the dataloader, and is responsible for   aggregating individual samples into a batch (but not transferring to   the GPU).</li> <li>Analogous to the <code>collate_fn</code> of a   pytorch dataloader.</li> <li>This method is not sequentially composable.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Sequence[TTransformed]</code> <p>A sequence of <code>TTransformed</code> data samples.</p> required <p>Returns:</p> Type Description <code>TCollated</code> <p>A <code>TCollated</code> collection of the input sequence.</p> Source code in <code>src/abstract_dataloader/abstract.py</code> <pre><code>def collate(self, data: Sequence[TTransformed]) -&gt; TCollated:\n    \"\"\"Collate a list of data samples into a GPU-ready batch.\n\n    - Operates on the CPU-side of the dataloader, and is responsible for\n      aggregating individual samples into a batch (but not transferring to\n      the GPU).\n    - Analogous to the `collate_fn` of a\n      [pytorch dataloader](https://pytorch.org/docs/stable/data.html).\n    - This method is not sequentially composable.\n\n    Args:\n        data: A sequence of `TTransformed` data samples.\n\n    Returns:\n        A `TCollated` collection of the input sequence.\n    \"\"\"\n    return cast(TCollated, data)\n</code></pre>"},{"location":"abstract/#abstract_dataloader.abstract.Pipeline.sample","title":"sample","text":"<pre><code>sample(data: TRaw) -&gt; TTransformed\n</code></pre> <p>Transform single samples.</p> <ul> <li>Operates on single samples, nominally on the CPU-side of a   dataloader.</li> <li>This method is both sequentially and parallel composable.</li> </ul> Fallback <p>The identity transform is provided by default (<code>TTransformed = TRaw</code>).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>TRaw</code> <p>A single <code>TRaw</code> data sample.</p> required <p>Returns:</p> Type Description <code>TTransformed</code> <p>A single <code>TTransformed</code> data sample.</p> Source code in <code>src/abstract_dataloader/abstract.py</code> <pre><code>def sample(self, data: TRaw) -&gt; TTransformed:\n    \"\"\"Transform single samples.\n\n    - Operates on single samples, nominally on the CPU-side of a\n      dataloader.\n    - This method is both sequentially and parallel composable.\n\n    Fallback:\n        The identity transform is provided by default\n        (`TTransformed = TRaw`).\n\n    Args:\n        data: A single `TRaw` data sample.\n\n    Returns:\n        A single `TTransformed` data sample.\n    \"\"\"\n    return cast(TTransformed, data)\n</code></pre>"},{"location":"dataloader/","title":"Data Loading","text":"<p>TL;DR</p> <ul> <li>A <code>Dataset</code> consists of one or more traces.</li> <li>A <code>Trace</code> consists of one or more simultaneously recorded sensors, which are asynchronously recorded and synchronized with a <code>Synchronization</code> protocol.</li> <li>A <code>Sensor</code> is a synchronous time series of sensor data, with associated <code>Metadata</code>.</li> </ul>"},{"location":"dataloader/#dataset-hierarchy","title":"Dataset Hierarchy","text":"<p>Multimodal datasets generally operate using the following pattern:</p> <ul> <li>You construct a data collection system consisting of multiple sensors, which can record data simultaneously.</li> <li>The system has a way of starting a recording, which causes all of the sensors to record data, with some kind of metadata indicating which session the data came from. This session is sometimes referred to as a \"trace\" (also \"sequence\", \"session\", etc).</li> <li>You offload the data from the collection system, and organize them into folders by trace. You then collect many such traces, and organize them into a \"dataset.\"</li> </ul> <p>To implement a dataloader specification based on this common pattern, we start from a dataset, trace, and sensor-based architecture:</p> <p></p> <ul> <li>A <code>Sensor</code> consists of all of the data recorded by a given modality in a single recording, with a modality being defined by one or possibly multiple feature streams which make sense to be loaded together. The <code>Sensor</code> keeps track of its own metadata, and has some way of loading the underlying data; the data also expected to be a single synchronous time series.</li> <li>Multiple <code>Sensor</code> streams are recorded simultaneously to make up a <code>Trace</code>, which in turn has some way of loading data across its constituent sensors.</li> <li>While a <code>Trace</code> can already be useful on its own (e.g. for \"scene\" processing pipelines like 3D reconstruction), many traces are then collected into a <code>Dataset</code>, which in turn is charged with loading data from the correct trace on each load call.</li> </ul>"},{"location":"dataloader/#time-synchronization","title":"Time Synchronization","text":"<p>Next, we address time synchronization between sensor streams which are \"asynchronous\" - that is, potentially have different data collection periods, or even have no regular period at all.</p> <p>At a high level, all approaches for dealing with asynchronous time series data can be abstracted as an algorithm which takes asynchronous time stamps in, and returns instructions for how to load synchronized samples. We define this procedure using a <code>Synchronization</code> spec, which maps global trace indices to sensor indices:</p> <p></p> <p>Trace Indexing</p> <p>We standardize the trace index as an array of integers, i.e. <code>index[i]</code> indicates the map from trace index <code>i</code> to sensor index. This implies that more advanced indexing methods, e.g. returning multiple observations, need to be implemented externally at the trace and/or sensor level. We may revisit this assumption in the future.</p> <p>Timestamps: Why <code>float64</code>?</p> <p>We standardize timestamps as float64 epoch times, which yields a precision of better than 1us. While there are probably some use cases which demand better precision, other solutions such as using a \"relative\" epoch time can also be entertained to increase this precision. In any case, I do not believe that the <code>uint64</code> epoch-in-nanoseconds convention provides any substantial benefits here.</p>"},{"location":"dataloader/#indexing","title":"Indexing","text":"<p>One of the downsides of our <code>dataset =&gt; trace =&gt; sensor</code> hierarchy is the need to implement global (dataset) to local (trace) index translation, which is not needed in monolithic dataloaders.</p> <p></p> <p>Fortunately, aside from a negligible performance overhead, in addition to presenting a cleaner abstraction, this approach provides substantial memory advantages:</p> <ul> <li>Index lookups are cheap. Using np.searchsorted (see <code>abstract.Dataset</code>), the global to local lookup latency is on the order of microseconds even with 100k traces.</li> <li>This approach saves memory. Assuming that metadata for the entire dataset will be loaded into memory, and assuming that the instructions for fetching each sample within each trace can be calculated without storing additional metadata (e.g. <code>trace/data/blob_{index}.npz</code>), no additional memory usage is required, while a global index-based system would likely require at least 4 bytes <code>int32</code> for each sample. In a more likely case, 10s of bytes would be required to store a file path, which can add up.</li> </ul>"},{"location":"generic/","title":"Generic","text":""},{"location":"generic/#abstract_dataloader.generic","title":"Generic Component Implementations","text":"<p>Generic \"ready-to-use\" implementations of common components.</p> <p>Other generic and largely reusable components can be added to this submodule.</p> <p>Note</p> <p>Numpy (and jaxtyping) are the only dependencies; to keep the <code>abstract_dataloader</code>'s dependencies lightweight and flexible, components should only be added here if they do not require any additional dependencies.</p>"},{"location":"generic/#abstract_dataloader.generic.ComposedPipeline","title":"abstract_dataloader.generic.ComposedPipeline","text":"<p>               Bases: <code>Pipeline[TRaw, TTransformed, TCollated, TProcessed]</code>, <code>Generic[TRaw, TRawInner, TTransformed, TCollated, TProcessedInner, TProcessed]</code></p> <p>Compose pipeline sequentially with pre and post transforms.</p> Type Parameters <ul> <li><code>TRaw</code>: initial input type.</li> <li><code>TRawInner</code>: output of the pre-composed transform, and input to the     provided <code>Pipeline</code>.</li> <li><code>TCollated</code>, <code>TProcessed</code>: intermediate values for the provided     <code>Pipeline</code>.</li> <li><code>TProcessedInner</code>: output of the transforms, and input to the     post-composed transform.</li> <li><code>TProcessed</code>: output type.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>Pipeline[TRawInner, TTransformed, TCollated, TProcessedInner]</code> <p>pipeline to compose.</p> required <code>pre</code> <code>Transform[TRaw, TRawInner] | None</code> <p>pre-transform to apply on the CPU side; skipped if <code>None</code>.</p> <code>None</code> <code>post</code> <code>Transform[TProcessedInner, TProcessed] | None</code> <p>post-transform to apply on the GPU side; skipped if <code>None</code>.</p> <code>None</code> Source code in <code>src/abstract_dataloader/generic/composition.py</code> <pre><code>class ComposedPipeline(\n    spec.Pipeline[TRaw, TTransformed, TCollated, TProcessed],\n    Generic[\n        TRaw, TRawInner, TTransformed,\n        TCollated, TProcessedInner, TProcessed]\n):\n    \"\"\"Compose pipeline sequentially with pre and post transforms.\n\n    Type Parameters:\n        - `TRaw`: initial input type.\n        - `TRawInner`: output of the pre-composed transform, and input to the\n            provided [`Pipeline`][abstract_dataloader.spec].\n        - `TCollated`, `TProcessed`: intermediate values for the provided\n            [`Pipeline`][abstract_dataloader.spec].\n        - `TProcessedInner`: output of the transforms, and input to the\n            post-composed transform.\n        - `TProcessed`: output type.\n\n    Args:\n        pipeline: pipeline to compose.\n        pre: pre-transform to apply on the CPU side; skipped if `None`.\n        post: post-transform to apply on the GPU side; skipped if `None`.\n    \"\"\"\n\n    def __init__(\n        self, pipeline: spec.Pipeline[\n            TRawInner, TTransformed, TCollated, TProcessedInner],\n        pre: spec.Transform[TRaw, TRawInner] | None = None,\n        post: spec.Transform[TProcessedInner, TProcessed] | None = None\n    ) -&gt; None:\n        self.pipeline = pipeline\n        self.pre = pre\n        self.post = post\n\n        self.collate = pipeline.collate\n\n    def sample(self, data: TRaw) -&gt; TTransformed:\n        \"\"\"Transform single samples.\n\n        Args:\n            data: A single `TRaw` data sample.\n\n        Returns:\n            A single `TTransformed` data sample.\n        \"\"\"\n        if self.pre is None:\n            transformed = cast(TRawInner, data)\n        else:\n            transformed = self.pre(data)\n        return self.pipeline.sample(transformed)\n\n    def batch(self, data: TCollated) -&gt; TProcessed:\n        \"\"\"Transform data batch.\n\n        Args:\n            data: A `TCollated` batch of data, nominally already sent to the\n                GPU.\n\n        Returns:\n            The `TProcessed` output, ready for the downstream model.\n        \"\"\"\n        transformed = self.pipeline.batch(data)\n        if self.post is None:\n            return cast(TProcessed, transformed)\n        else:\n            return self.post(transformed)\n</code></pre>"},{"location":"generic/#abstract_dataloader.generic.ComposedPipeline.batch","title":"batch","text":"<pre><code>batch(data: TCollated) -&gt; TProcessed\n</code></pre> <p>Transform data batch.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>TCollated</code> <p>A <code>TCollated</code> batch of data, nominally already sent to the GPU.</p> required <p>Returns:</p> Type Description <code>TProcessed</code> <p>The <code>TProcessed</code> output, ready for the downstream model.</p> Source code in <code>src/abstract_dataloader/generic/composition.py</code> <pre><code>def batch(self, data: TCollated) -&gt; TProcessed:\n    \"\"\"Transform data batch.\n\n    Args:\n        data: A `TCollated` batch of data, nominally already sent to the\n            GPU.\n\n    Returns:\n        The `TProcessed` output, ready for the downstream model.\n    \"\"\"\n    transformed = self.pipeline.batch(data)\n    if self.post is None:\n        return cast(TProcessed, transformed)\n    else:\n        return self.post(transformed)\n</code></pre>"},{"location":"generic/#abstract_dataloader.generic.ComposedPipeline.sample","title":"sample","text":"<pre><code>sample(data: TRaw) -&gt; TTransformed\n</code></pre> <p>Transform single samples.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>TRaw</code> <p>A single <code>TRaw</code> data sample.</p> required <p>Returns:</p> Type Description <code>TTransformed</code> <p>A single <code>TTransformed</code> data sample.</p> Source code in <code>src/abstract_dataloader/generic/composition.py</code> <pre><code>def sample(self, data: TRaw) -&gt; TTransformed:\n    \"\"\"Transform single samples.\n\n    Args:\n        data: A single `TRaw` data sample.\n\n    Returns:\n        A single `TTransformed` data sample.\n    \"\"\"\n    if self.pre is None:\n        transformed = cast(TRawInner, data)\n    else:\n        transformed = self.pre(data)\n    return self.pipeline.sample(transformed)\n</code></pre>"},{"location":"generic/#abstract_dataloader.generic.Empty","title":"abstract_dataloader.generic.Empty","text":"<p>               Bases: <code>Synchronization</code></p> <p>Dummy synchronization which does not synchronize sensor pairs.</p> <p>No samples will be registered, and the trace can only be used as a collection of sensors.</p> Source code in <code>src/abstract_dataloader/generic/sync.py</code> <pre><code>class Empty(spec.Synchronization):\n    \"\"\"Dummy synchronization which does not synchronize sensor pairs.\n\n    No samples will be registered, and the trace can only be used as a\n    collection of sensors.\n    \"\"\"\n\n    def __call__(\n        self, timestamps: dict[str, Float64[np.ndarray, \"_N\"]]\n    ) -&gt; dict[str, UInt32[np.ndarray, \"M\"]]:\n        \"\"\"Apply synchronization protocol.\n\n        Args:\n            timestamps: input sensor timestamps.\n\n        Returns:\n            Synchronized index map.\n        \"\"\"\n        return {k: np.array([], dtype=np.uint32) for k in timestamps}\n</code></pre>"},{"location":"generic/#abstract_dataloader.generic.Empty.__call__","title":"__call__","text":"<pre><code>__call__(\n    timestamps: dict[str, Float64[ndarray, _N]],\n) -&gt; dict[str, UInt32[ndarray, M]]\n</code></pre> <p>Apply synchronization protocol.</p> <p>Parameters:</p> Name Type Description Default <code>timestamps</code> <code>dict[str, Float64[ndarray, _N]]</code> <p>input sensor timestamps.</p> required <p>Returns:</p> Type Description <code>dict[str, UInt32[ndarray, M]]</code> <p>Synchronized index map.</p> Source code in <code>src/abstract_dataloader/generic/sync.py</code> <pre><code>def __call__(\n    self, timestamps: dict[str, Float64[np.ndarray, \"_N\"]]\n) -&gt; dict[str, UInt32[np.ndarray, \"M\"]]:\n    \"\"\"Apply synchronization protocol.\n\n    Args:\n        timestamps: input sensor timestamps.\n\n    Returns:\n        Synchronized index map.\n    \"\"\"\n    return {k: np.array([], dtype=np.uint32) for k in timestamps}\n</code></pre>"},{"location":"generic/#abstract_dataloader.generic.Metadata","title":"abstract_dataloader.generic.Metadata  <code>dataclass</code>","text":"<p>               Bases: <code>Metadata</code></p> <p>Generic metadata with timestamps.</p> <p>Attributes:</p> Name Type Description <code>timestamps</code> <code>Float64[ndarray, N]</code> <p>epoch timestamps.</p> Source code in <code>src/abstract_dataloader/generic/sequence.py</code> <pre><code>@dataclass\nclass Metadata(spec.Metadata):\n    \"\"\"Generic metadata with timestamps.\n\n    Attributes:\n        timestamps: epoch timestamps.\n    \"\"\"\n\n    timestamps: Float64[np.ndarray, \"N\"]\n</code></pre>"},{"location":"generic/#abstract_dataloader.generic.Nearest","title":"abstract_dataloader.generic.Nearest","text":"<p>               Bases: <code>Synchronization</code></p> <p>Nearest sample synchronization, with respect to a reference sensor.</p> <p>Applies the following:</p> <ul> <li>Compute the midpoints between observations between each sensor.</li> <li>Find which bin the reference sensor timestamps fall into.</li> <li>Calculate the resulting time delta between timestamps. If this exceeds   <code>tol</code> for any sensor-reference pair, remove this match.</li> </ul> <p>See <code>Synchronization</code> for protocol details.</p> <p>Parameters:</p> Name Type Description Default <code>reference</code> <code>str</code> <p>reference sensor to synchronize to.</p> required <code>tol</code> <code>float</code> <p>synchronization time tolerance, in seconds. Setting <code>tol = np.inf</code> works to disable this check altogether.</p> <code>0.1</code> <code>margin</code> <code>tuple[int | float, int | float]</code> <p>time margin (in seconds; <code>float</code>) or index margin (in samples; <code>int</code>) to apply to the start and end time relative to the reference sensor, excluding samples within this margin.</p> <code>(0, 0)</code> Source code in <code>src/abstract_dataloader/generic/sync.py</code> <pre><code>class Nearest(spec.Synchronization):\n    \"\"\"Nearest sample synchronization, with respect to a reference sensor.\n\n    Applies the following:\n\n    - Compute the midpoints between observations between each sensor.\n    - Find which bin the reference sensor timestamps fall into.\n    - Calculate the resulting time delta between timestamps. If this exceeds\n      `tol` for any sensor-reference pair, remove this match.\n\n    See [`Synchronization`][abstract_dataloader.spec.] for protocol details.\n\n    Args:\n        reference: reference sensor to synchronize to.\n        tol: synchronization time tolerance, in seconds. Setting `tol = np.inf`\n            works to disable this check altogether.\n        margin: time margin (in seconds; `float`) or index margin\n            (in samples; `int`) to apply to the start and end time relative to\n            the reference sensor, excluding samples within this margin.\n    \"\"\"\n\n    def __init__(\n        self, reference: str, tol: float = 0.1,\n        margin: tuple[int | float, int | float] = (0, 0)\n    ) -&gt; None:\n        if tol &lt; 0:\n            raise ValueError(\n                f\"Synchronization tolerance must be positive: {tol} &lt; 0\")\n\n        self.tol = tol\n        self.reference = reference\n        self.margin = margin\n\n    def __call__(\n        self, timestamps: dict[str, Float64[np.ndarray, \"_N\"]]\n    ) -&gt; dict[str, UInt32[np.ndarray, \"M\"]]:\n        \"\"\"Apply synchronization protocol.\n\n        Args:\n            timestamps: input sensor timestamps.\n\n        Returns:\n            Synchronized index map.\n        \"\"\"\n        try:\n            t_ref = timestamps[self.reference]\n        except KeyError:\n            raise KeyError(\n                f\"Reference sensor {self.reference} was not provided in \"\n                f\"timestamps, with keys: {list(timestamps.keys())}\")\n\n        if isinstance(self.margin[0], float):\n            t_ref = t_ref[np.argmax(t_ref &gt; t_ref[0] + self.margin[0]):]\n        elif isinstance(self.margin[0], int) and self.margin[0] &gt; 0:\n            t_ref = t_ref[self.margin[0]:]\n\n        if isinstance(self.margin[1], float):\n            t_ref = t_ref[\n                :-np.argmax((t_ref &lt; t_ref[-1] - self.margin[1])[::-1])]\n        elif isinstance(self.margin[1], int) and self.margin[1] &gt; 0:\n            t_ref = t_ref[:-self.margin[1]]\n\n        indices = {\n            k: np.searchsorted(\n                (t_sensor[:-1] + t_sensor[1:]) / 2, t_ref\n            ).astype(np.uint32)\n            for k, t_sensor in timestamps.items()}\n\n        valid = np.all(np.array([\n           np.abs(timestamps[k][i_nearest] - t_ref) &lt; self.tol\n        for k, i_nearest in indices.items()]), axis=0)\n\n        return {k: v[valid] for k, v in indices.items()}\n</code></pre>"},{"location":"generic/#abstract_dataloader.generic.Nearest.__call__","title":"__call__","text":"<pre><code>__call__(\n    timestamps: dict[str, Float64[ndarray, _N]],\n) -&gt; dict[str, UInt32[ndarray, M]]\n</code></pre> <p>Apply synchronization protocol.</p> <p>Parameters:</p> Name Type Description Default <code>timestamps</code> <code>dict[str, Float64[ndarray, _N]]</code> <p>input sensor timestamps.</p> required <p>Returns:</p> Type Description <code>dict[str, UInt32[ndarray, M]]</code> <p>Synchronized index map.</p> Source code in <code>src/abstract_dataloader/generic/sync.py</code> <pre><code>def __call__(\n    self, timestamps: dict[str, Float64[np.ndarray, \"_N\"]]\n) -&gt; dict[str, UInt32[np.ndarray, \"M\"]]:\n    \"\"\"Apply synchronization protocol.\n\n    Args:\n        timestamps: input sensor timestamps.\n\n    Returns:\n        Synchronized index map.\n    \"\"\"\n    try:\n        t_ref = timestamps[self.reference]\n    except KeyError:\n        raise KeyError(\n            f\"Reference sensor {self.reference} was not provided in \"\n            f\"timestamps, with keys: {list(timestamps.keys())}\")\n\n    if isinstance(self.margin[0], float):\n        t_ref = t_ref[np.argmax(t_ref &gt; t_ref[0] + self.margin[0]):]\n    elif isinstance(self.margin[0], int) and self.margin[0] &gt; 0:\n        t_ref = t_ref[self.margin[0]:]\n\n    if isinstance(self.margin[1], float):\n        t_ref = t_ref[\n            :-np.argmax((t_ref &lt; t_ref[-1] - self.margin[1])[::-1])]\n    elif isinstance(self.margin[1], int) and self.margin[1] &gt; 0:\n        t_ref = t_ref[:-self.margin[1]]\n\n    indices = {\n        k: np.searchsorted(\n            (t_sensor[:-1] + t_sensor[1:]) / 2, t_ref\n        ).astype(np.uint32)\n        for k, t_sensor in timestamps.items()}\n\n    valid = np.all(np.array([\n       np.abs(timestamps[k][i_nearest] - t_ref) &lt; self.tol\n    for k, i_nearest in indices.items()]), axis=0)\n\n    return {k: v[valid] for k, v in indices.items()}\n</code></pre>"},{"location":"generic/#abstract_dataloader.generic.Next","title":"abstract_dataloader.generic.Next","text":"<p>               Bases: <code>Synchronization</code></p> <p>Next sample synchronization, with respect to a reference sensor.</p> <p>Applies the following:</p> <ul> <li>Find the start time, defined by the earliest time which is observed by   all sensors, and the end time, defined by the last time which is observed   by all sensors.</li> <li>Truncate the reference sensor's timestamps to this start and end time,   and use this as the query timestamps.</li> <li>For each time in the query, find the first sample from each sensor which   is after this time.</li> </ul> <p>See <code>Synchronization</code> for protocol details.</p> <p>Parameters:</p> Name Type Description Default <code>reference</code> <code>str</code> <p>reference sensor to synchronize to.</p> required <code>margin</code> <code>tuple[int | float, int | float]</code> <p>time margin (in seconds; <code>float</code>) or index margin (in samples; <code>int</code>) to apply to the start and end, excluding samples within this margin.</p> <code>(0, 0)</code> Source code in <code>src/abstract_dataloader/generic/sync.py</code> <pre><code>class Next(spec.Synchronization):\n    \"\"\"Next sample synchronization, with respect to a reference sensor.\n\n    Applies the following:\n\n    - Find the start time, defined by the earliest time which is observed by\n      all sensors, and the end time, defined by the last time which is observed\n      by all sensors.\n    - Truncate the reference sensor's timestamps to this start and end time,\n      and use this as the query timestamps.\n    - For each time in the query, find the first sample from each sensor which\n      is after this time.\n\n    See [`Synchronization`][abstract_dataloader.spec.] for protocol details.\n\n    Args:\n        reference: reference sensor to synchronize to.\n        margin: time margin (in seconds; `float`) or index margin\n            (in samples; `int`) to apply to the start and end, excluding\n            samples within this margin.\n    \"\"\"\n\n    def __init__(\n        self, reference: str,\n        margin: tuple[int | float, int | float] = (0, 0)\n    ) -&gt; None:\n        self.reference = reference\n        self.margin = margin\n\n    def __call__(\n        self, timestamps: dict[str, Float64[np.ndarray, \"_N\"]]\n    ) -&gt; dict[str, UInt32[np.ndarray, \"M\"]]:\n        \"\"\"Apply synchronization protocol.\n\n        Args:\n            timestamps: input sensor timestamps.\n\n        Returns:\n            Synchronized index map.\n        \"\"\"\n        try:\n            ref_time_all = timestamps[self.reference]\n        except KeyError:\n            raise KeyError(\n                f\"Reference sensor {self.reference} was not provided in \"\n                f\"timestamps, with keys: {list(timestamps.keys())}\")\n\n        start_time = max(t[0] for t in timestamps.values())\n        end_time = min(t[-1] for t in timestamps.values())\n\n        if isinstance(self.margin[0], float):\n            start_time += self.margin[0]\n        if isinstance(self.margin[1], float):\n            end_time -= self.margin[1]\n\n        start_idx = np.searchsorted(ref_time_all, start_time)\n        end_idx = np.searchsorted(ref_time_all, end_time)\n\n        if isinstance(self.margin[0], int):\n            start_idx += self.margin[0]\n        if isinstance(self.margin[1], int):\n            end_idx -= self.margin[1]\n\n        ref_time = ref_time_all[start_idx:end_idx]\n        return {\n            k: np.searchsorted(v, ref_time).astype(np.uint32)\n            for k, v in timestamps.items()}\n</code></pre>"},{"location":"generic/#abstract_dataloader.generic.Next.__call__","title":"__call__","text":"<pre><code>__call__(\n    timestamps: dict[str, Float64[ndarray, _N]],\n) -&gt; dict[str, UInt32[ndarray, M]]\n</code></pre> <p>Apply synchronization protocol.</p> <p>Parameters:</p> Name Type Description Default <code>timestamps</code> <code>dict[str, Float64[ndarray, _N]]</code> <p>input sensor timestamps.</p> required <p>Returns:</p> Type Description <code>dict[str, UInt32[ndarray, M]]</code> <p>Synchronized index map.</p> Source code in <code>src/abstract_dataloader/generic/sync.py</code> <pre><code>def __call__(\n    self, timestamps: dict[str, Float64[np.ndarray, \"_N\"]]\n) -&gt; dict[str, UInt32[np.ndarray, \"M\"]]:\n    \"\"\"Apply synchronization protocol.\n\n    Args:\n        timestamps: input sensor timestamps.\n\n    Returns:\n        Synchronized index map.\n    \"\"\"\n    try:\n        ref_time_all = timestamps[self.reference]\n    except KeyError:\n        raise KeyError(\n            f\"Reference sensor {self.reference} was not provided in \"\n            f\"timestamps, with keys: {list(timestamps.keys())}\")\n\n    start_time = max(t[0] for t in timestamps.values())\n    end_time = min(t[-1] for t in timestamps.values())\n\n    if isinstance(self.margin[0], float):\n        start_time += self.margin[0]\n    if isinstance(self.margin[1], float):\n        end_time -= self.margin[1]\n\n    start_idx = np.searchsorted(ref_time_all, start_time)\n    end_idx = np.searchsorted(ref_time_all, end_time)\n\n    if isinstance(self.margin[0], int):\n        start_idx += self.margin[0]\n    if isinstance(self.margin[1], int):\n        end_idx -= self.margin[1]\n\n    ref_time = ref_time_all[start_idx:end_idx]\n    return {\n        k: np.searchsorted(v, ref_time).astype(np.uint32)\n        for k, v in timestamps.items()}\n</code></pre>"},{"location":"generic/#abstract_dataloader.generic.ParallelPipelines","title":"abstract_dataloader.generic.ParallelPipelines","text":"<p>               Bases: <code>Pipeline[PRaw, PTransformed, PCollated, PProcessed]</code></p> <p>Compose multiple transforms in parallel.</p> <p>For example, with transforms <code>{\"radar\": radar_tf, \"lidar\": lidar_tf, ...}</code>, the composed transform performs:</p> <pre><code>{\n    \"radar\": radar_tf.transform(data[\"radar\"]),\n    \"lidar\": lidar_tf.transform(data[\"lidar\"]),\n    ...\n}\n</code></pre> <p>Note</p> <p>This implies that the type parameters must be <code>dict[str, Any]</code>, so this class is parameterized by a separate set of <code>Composed(Raw|Transformed|Collated|Processed)</code> types with this bound.</p> <p>Tip</p> <p>See <code>torch.ParallelPipelines</code> for an implementation which is compatible with <code>nn.Module</code>-based pipelines.</p> Type Parameters <ul> <li><code>PRaw</code>, <code>PTransformed</code>, <code>PCollated</code>, <code>PProcessed</code>: see   <code>Pipeline</code>.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>transforms</code> <code>Pipeline</code> <p>transforms to compose. The key indicates the subkey to apply each transform to.</p> <code>{}</code> Source code in <code>src/abstract_dataloader/generic/composition.py</code> <pre><code>class ParallelPipelines(\n    spec.Pipeline[PRaw, PTransformed, PCollated, PProcessed],\n):\n    \"\"\"Compose multiple transforms in parallel.\n\n    For example, with transforms `{\"radar\": radar_tf, \"lidar\": lidar_tf, ...}`,\n    the composed transform performs:\n\n    ```python\n    {\n        \"radar\": radar_tf.transform(data[\"radar\"]),\n        \"lidar\": lidar_tf.transform(data[\"lidar\"]),\n        ...\n    }\n    ```\n\n    !!! note\n\n        This implies that the type parameters must be `dict[str, Any]`, so this\n        class is parameterized by a separate set of\n        `Composed(Raw|Transformed|Collated|Processed)` types with this bound.\n\n    !!! tip\n\n        See [`torch.ParallelPipelines`][abstract_dataloader.] for an\n        implementation which is compatible with [`nn.Module`][torch.]-based\n        pipelines.\n\n    Type Parameters:\n        - `PRaw`, `PTransformed`, `PCollated`, `PProcessed`: see\n          [`Pipeline`][abstract_dataloader.spec.].\n\n    Args:\n        transforms: transforms to compose. The key indicates the subkey to\n            apply each transform to.\n    \"\"\"\n\n    def __init__(self, **transforms: spec.Pipeline) -&gt; None:\n        self.transforms = transforms\n\n    def sample(self, data: PRaw) -&gt; PTransformed:\n        return cast(\n            PTransformed,\n            {k: v.sample(data[k]) for k, v in self.transforms.items()})\n\n    def collate(self, data: Sequence[PTransformed]) -&gt; PCollated:\n        return cast(PCollated, {\n            k: v.collate([x[k] for x in data])\n            for k, v in self.transforms.items()\n        })\n\n    def batch(self, data: PCollated) -&gt; PProcessed:\n        return cast(\n            PProcessed,\n            {k: v.batch(data[k]) for k, v in self.transforms.items()})\n</code></pre>"},{"location":"generic/#abstract_dataloader.generic.ParallelTransforms","title":"abstract_dataloader.generic.ParallelTransforms","text":"<p>               Bases: <code>Transform[PRaw, PTransformed]</code></p> <p>Compose multiple transforms, similar to <code>ParallelPipelines</code>.</p> Type Parameters <ul> <li><code>PRaw</code>, <code>PTransformed</code>, <code>Transform</code>.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>transforms</code> <code>Transform</code> <p>transforms to compose. The key indicates the subkey to apply each transform to.</p> <code>{}</code> Source code in <code>src/abstract_dataloader/generic/composition.py</code> <pre><code>class ParallelTransforms(spec.Transform[PRaw, PTransformed]):\n    \"\"\"Compose multiple transforms, similar to [`ParallelPipelines`][^.].\n\n    Type Parameters:\n        - `PRaw`, `PTransformed`, [`Transform`][abstract_dataloader.spec.].\n\n    Args:\n        transforms: transforms to compose. The key indicates the subkey to\n            apply each transform to.\n    \"\"\"\n\n    def __init__(self, **transforms: spec.Transform) -&gt; None:\n        self.transforms = transforms\n\n    def __call__(self, data: PRaw) -&gt; PTransformed:\n        return cast(\n            PTransformed,\n            {k: v(data[k]) for k, v in self.transforms.items()})\n</code></pre>"},{"location":"generic/#abstract_dataloader.generic.SequencePipeline","title":"abstract_dataloader.generic.SequencePipeline","text":"<p>               Bases: <code>Pipeline[Sequence[TRaw], Sequence[TTransformed], Sequence[TCollated], Sequence[TProcessed]]</code>, <code>Generic[TRaw, TTransformed, TCollated, TProcessed]</code></p> <p>Transform which passes an additional sequence axis through.</p> <p>The given <code>Pipeline</code> is modified to accept <code>Sequence[...]</code> for each data type in its pipeline, and return a <code>list[...]</code> across the additional axis, thus \"passing through\" the axis.</p> <p>For example, suppose a sequence dataloader reads</p> <pre><code>[\n    [Raw[s=0, t=0], Raw[s=0, t=1], ... Raw[s=0, t=n]]\n    [Raw[s=1, t=0], Raw[s=1, t=1], ... Raw[s=1, t=n]]\n    ...\n    [Raw[s=b, t=0], Raw[s=b, t=1], ... Raw[s=b, t=n]\n]\n</code></pre> <p>for sequence length <code>t = 0...n</code> and batch sample <code>s = 0...b</code>. For sequence length <code>t</code>, the output of the transforms will be batched with the sequence on the outside:</p> <pre><code>[\n    Processed[s=0...b] [t=0],\n    Processed[s=0...b] [t=1],\n    ...\n    Processed[s=0...b] [t=n]\n]\n</code></pre> Type Parameters <ul> <li><code>TRaw</code>, <code>TTransformed</code>, <code>TCollated</code>, <code>TProcessed</code>: see   <code>Pipeline</code>.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>Pipeline[TRaw, TTransformed, TCollated, TProcessed]</code> <p>input pipeline.</p> required Source code in <code>src/abstract_dataloader/generic/sequence.py</code> <pre><code>class SequencePipeline(\n    spec.Pipeline[\n        Sequence[TRaw], Sequence[TTransformed],\n        Sequence[TCollated], Sequence[TProcessed]],\n    Generic[TRaw, TTransformed, TCollated, TProcessed]\n):\n    \"\"\"Transform which passes an additional sequence axis through.\n\n    The given `Pipeline` is modified to accept `Sequence[...]` for each\n    data type in its pipeline, and return a `list[...]` across the additional\n    axis, thus \"passing through\" the axis.\n\n    For example, suppose a sequence dataloader reads\n\n    ```\n    [\n        [Raw[s=0, t=0], Raw[s=0, t=1], ... Raw[s=0, t=n]]\n        [Raw[s=1, t=0], Raw[s=1, t=1], ... Raw[s=1, t=n]]\n        ...\n        [Raw[s=b, t=0], Raw[s=b, t=1], ... Raw[s=b, t=n]\n    ]\n    ```\n\n    for sequence length `t = 0...n` and batch sample `s = 0...b`. For sequence\n    length `t`, the output of the transforms will be batched with the sequence\n    on the outside:\n\n    ```\n    [\n        Processed[s=0...b] [t=0],\n        Processed[s=0...b] [t=1],\n        ...\n        Processed[s=0...b] [t=n]\n    ]\n    ```\n\n    Type Parameters:\n        - `TRaw`, `TTransformed`, `TCollated`, `TProcessed`: see\n          [`Pipeline`][abstract_dataloader.spec.].\n\n    Args:\n        pipeline: input pipeline.\n    \"\"\"\n\n    def __init__(\n        self, pipeline: spec.Pipeline[\n            TRaw, TTransformed, TCollated, TProcessed]\n    ) -&gt; None:\n        self.pipeline = pipeline\n\n    def sample(self, data: Sequence[TRaw]) -&gt; list[TTransformed]:\n        return [self.pipeline.sample(x) for x in data]\n\n    def collate(\n        self, data: Sequence[Sequence[TTransformed]]\n    ) -&gt; list[TCollated]:\n        return [self.pipeline.collate(x) for x in zip(*data)]\n\n    def batch(self, data: Sequence[TCollated]) -&gt; list[TProcessed]:\n        return [self.pipeline.batch(x) for x in data]\n</code></pre>"},{"location":"generic/#abstract_dataloader.generic.Window","title":"abstract_dataloader.generic.Window","text":"<p>               Bases: <code>Sensor[SampleStack, Metadata]</code>, <code>Generic[SampleStack, Sample, TMetadata]</code></p> <p>Load sensor data across a time window using a sensor transform.</p> <p>Use this class as a generic transform to give time history to any sensor:</p> <pre><code>sensor =  ... # implements spec.Sensor\nwith_history = generic.Window(sensor, past=5, future=1, parallel=7)\n</code></pre> <p>In this example, 5 past samples, the current sample, and 1 future sample are loaded on every index:</p> <pre><code>with_history[i] = [\n    sensor[i], sensor[i + 1], ... sensor[i + 5], sensor[i + 6]]\n                                        ^\n                            # timestamp for synchronization\n</code></pre> Type Parameters <ul> <li><code>SampleStack</code>: a collated series of consecutive samples. Can simply be     <code>list[Sample]</code>.</li> <li><code>Sample</code>: single observation sample type.</li> <li><code>TMetadata</code>: metadata type for the underlying sensor. Note that the     <code>Window</code> wrapper doesn't actually have metadata type <code>TMetadata</code>;     this type is just passed through from the sensor which is wrapped.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>sensor</code> <code>Sensor[Sample, TMetadata]</code> <p>sensor to wrap.</p> required <code>collate_fn</code> <code>Callable[[list[Sample]], SampleStack] | None</code> <p>collate function for aggregating a list of samples; if not specified, the samples are simply returned as a list.</p> <code>None</code> <code>past</code> <code>int</code> <p>number of past samples, in addition to the current sample. Set to <code>0</code> to disable.</p> <code>0</code> <code>future</code> <code>int</code> <p>number of future samples, in addition to the current sample. Set to <code>0</code> to disable.</p> <code>0</code> <code>crop</code> <code>bool</code> <p>if <code>True</code>, crop the first <code>past</code> and last <code>future</code> samples in the reported metadata to ensure that all samples are fully valid.</p> <code>True</code> <code>parallel</code> <code>int | None</code> <p>maximum number of samples to load in parallel; if <code>None</code>, all samples are loaded sequentially.</p> <code>None</code> Source code in <code>src/abstract_dataloader/generic/sequence.py</code> <pre><code>class Window(\n    abstract.Sensor[SampleStack, Metadata],\n    Generic[SampleStack, Sample, TMetadata]\n):\n    \"\"\"Load sensor data across a time window using a sensor transform.\n\n    Use this class as a generic transform to give time history to any sensor:\n\n    ```python\n    sensor =  ... # implements spec.Sensor\n    with_history = generic.Window(sensor, past=5, future=1, parallel=7)\n    ```\n\n    In this example, 5 past samples, the current sample, and 1 future sample\n    are loaded on every index:\n\n    ```python\n    with_history[i] = [\n        sensor[i], sensor[i + 1], ... sensor[i + 5], sensor[i + 6]]\n                                            ^\n                                # timestamp for synchronization\n    ```\n\n    Type Parameters:\n        - `SampleStack`: a collated series of consecutive samples. Can simply be\n            `list[Sample]`.\n        - `Sample`: single observation sample type.\n        - `TMetadata`: metadata type for the underlying sensor. Note that the\n            `Window` wrapper doesn't actually have metadata type `TMetadata`;\n            this type is just passed through from the sensor which is wrapped.\n\n    Args:\n        sensor: sensor to wrap.\n        collate_fn: collate function for aggregating a list of samples; if not\n            specified, the samples are simply returned as a list.\n        past: number of past samples, in addition to the current sample. Set\n            to `0` to disable.\n        future: number of future samples, in addition to the current sample.\n            Set to `0` to disable.\n        crop: if `True`, crop the first `past` and last `future` samples in\n            the reported metadata to ensure that all samples are fully valid.\n        parallel: maximum number of samples to load in parallel; if `None`, all\n            samples are loaded sequentially.\n    \"\"\"\n\n    def __init__(\n        self, sensor: spec.Sensor[Sample, TMetadata],\n        collate_fn: Callable[[list[Sample]], SampleStack] | None = None,\n        past: int = 0, future: int = 0, crop: bool = True,\n        parallel: int | None = None\n    ) -&gt; None:\n        self.sensor = sensor\n        self.past = past\n        self.future = future\n        self.parallel = parallel\n\n        if collate_fn is None:\n            collate_fn = cast(\n                Callable[[list[Sample]], SampleStack], lambda x: x)\n        self.collate_fn = collate_fn\n\n        self.cropped = crop\n        if crop:\n            # hack for negative indexing\n            _future = None if future == 0 else -future\n            self.metadata = Metadata(\n                timestamps=sensor.metadata.timestamps[past:_future])\n        else:\n            self.metadata = Metadata(timestamps=sensor.metadata.timestamps)\n\n    @classmethod\n    def from_partial_sensor(\n        cls, sensor: Callable[[str], spec.Sensor[Sample, TMetadata]],\n        collate_fn: Callable[[list[Sample]], SampleStack] | None = None,\n        past: int = 0, future: int = 0, crop: bool = True,\n        parallel: int | None = None\n    ) -&gt; Callable[[str], \"Window[SampleStack, Sample, TMetadata]\"]:\n        \"\"\"Partially initialize from partially initialized sensor.\n\n        Use this to create windowed sensor constructors which can be\n        applied to different traces to construct a dataset. For example,\n        if you have a `sensor_constructor`:\n\n        ```python\n        sensor_constructor = ...\n        windowed_sensor_constructor = Window.from_partial_sensor(\n            sensor_constructor, ...)\n\n        # ... somewhere inside the dataset constructor\n        sensor_instance = windowed_sensor_constructor(path_to_trace)\n        ```\n\n        Args:\n            sensor: sensor *constructor* to wrap.\n            collate_fn: collate function for aggregating a list of samples; if\n                not specified, the samples are simply returned as a list.\n            past: number of past samples, in addition to the current sample.\n                Set to `0` to disable.\n            future: number of future samples, in addition to the current\n                sample. Set to `0` to disable.\n            crop: if `True`, crop the first `past` and last `future` samples in\n                the reported metadata to ensure that all samples are full\n                valid.\n            parallel: maximum number of samples to load in parallel; if `None`,\n                all samples are loaded sequentially.\n        \"\"\"\n        def create_wrapped_sensor(\n            path: str\n        ) -&gt; Window[SampleStack, Sample, TMetadata]:\n            return cls(\n                sensor(path), collate_fn=collate_fn, past=past,\n                future=future, crop=crop, parallel=parallel)\n\n        return create_wrapped_sensor\n\n    def __getitem__(self, index: int | np.integer) -&gt; SampleStack:\n        \"\"\"Fetch measurements from this sensor, by index.\n\n        !!! warning\n\n            Note that `past` samples are lost at the beginning, and `future`\n            samples at the end to account for the window size!\n\n            If `crop=True`, these lost samples are taken into account by the\n            `Window` wrapper; if `crop=False`, the caller must handle this.\n\n        Args:\n            index: sample index.\n\n        Returns:\n            A set of `past + 1 + future` consecutives samples. Note that there\n                is a `past` offset of indices between the wrapped `Window` and\n                the underlying sensor!\n\n        Raises:\n            IndexError: if `crop=False`, and the requested index is out of\n                bounds (i.e., in the first `past` or last `future` samples).\n        \"\"\"\n        if self.cropped:\n            window = list(range(index, index + self.past + self.future + 1))\n        else:\n            window = list(range(index - self.past, index + self.future + 1))\n\n        if window[0] &lt; 0 or window[-1] &gt;= len(self.sensor):\n            raise IndexError(\n                f\"Requested invalid index {index} for uncropped \"\n                f\"Window(past={self.past}, future={self.future}).\")\n\n        if self.parallel is not None:\n            with ThreadPool(min(len(window), self.parallel)) as p:\n                return self.collate_fn(p.map(self.sensor.__getitem__, window))\n        else:\n            return self.collate_fn(list(map(self.sensor.__getitem__, window)))\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Get friendly name (passing through to the underlying sensor).\"\"\"\n        return f\"{repr(self.sensor)} x [-{self.past}:+{self.future}]\"\n</code></pre>"},{"location":"generic/#abstract_dataloader.generic.Window.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(index: int | integer) -&gt; SampleStack\n</code></pre> <p>Fetch measurements from this sensor, by index.</p> <p>Warning</p> <p>Note that <code>past</code> samples are lost at the beginning, and <code>future</code> samples at the end to account for the window size!</p> <p>If <code>crop=True</code>, these lost samples are taken into account by the <code>Window</code> wrapper; if <code>crop=False</code>, the caller must handle this.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int | integer</code> <p>sample index.</p> required <p>Returns:</p> Type Description <code>SampleStack</code> <p>A set of <code>past + 1 + future</code> consecutives samples. Note that there is a <code>past</code> offset of indices between the wrapped <code>Window</code> and the underlying sensor!</p> <p>Raises:</p> Type Description <code>IndexError</code> <p>if <code>crop=False</code>, and the requested index is out of bounds (i.e., in the first <code>past</code> or last <code>future</code> samples).</p> Source code in <code>src/abstract_dataloader/generic/sequence.py</code> <pre><code>def __getitem__(self, index: int | np.integer) -&gt; SampleStack:\n    \"\"\"Fetch measurements from this sensor, by index.\n\n    !!! warning\n\n        Note that `past` samples are lost at the beginning, and `future`\n        samples at the end to account for the window size!\n\n        If `crop=True`, these lost samples are taken into account by the\n        `Window` wrapper; if `crop=False`, the caller must handle this.\n\n    Args:\n        index: sample index.\n\n    Returns:\n        A set of `past + 1 + future` consecutives samples. Note that there\n            is a `past` offset of indices between the wrapped `Window` and\n            the underlying sensor!\n\n    Raises:\n        IndexError: if `crop=False`, and the requested index is out of\n            bounds (i.e., in the first `past` or last `future` samples).\n    \"\"\"\n    if self.cropped:\n        window = list(range(index, index + self.past + self.future + 1))\n    else:\n        window = list(range(index - self.past, index + self.future + 1))\n\n    if window[0] &lt; 0 or window[-1] &gt;= len(self.sensor):\n        raise IndexError(\n            f\"Requested invalid index {index} for uncropped \"\n            f\"Window(past={self.past}, future={self.future}).\")\n\n    if self.parallel is not None:\n        with ThreadPool(min(len(window), self.parallel)) as p:\n            return self.collate_fn(p.map(self.sensor.__getitem__, window))\n    else:\n        return self.collate_fn(list(map(self.sensor.__getitem__, window)))\n</code></pre>"},{"location":"generic/#abstract_dataloader.generic.Window.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> <p>Get friendly name (passing through to the underlying sensor).</p> Source code in <code>src/abstract_dataloader/generic/sequence.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Get friendly name (passing through to the underlying sensor).\"\"\"\n    return f\"{repr(self.sensor)} x [-{self.past}:+{self.future}]\"\n</code></pre>"},{"location":"generic/#abstract_dataloader.generic.Window.from_partial_sensor","title":"from_partial_sensor  <code>classmethod</code>","text":"<pre><code>from_partial_sensor(\n    sensor: Callable[[str], Sensor[Sample, TMetadata]],\n    collate_fn: Callable[[list[Sample]], SampleStack] | None = None,\n    past: int = 0,\n    future: int = 0,\n    crop: bool = True,\n    parallel: int | None = None,\n) -&gt; Callable[[str], Window[SampleStack, Sample, TMetadata]]\n</code></pre> <p>Partially initialize from partially initialized sensor.</p> <p>Use this to create windowed sensor constructors which can be applied to different traces to construct a dataset. For example, if you have a <code>sensor_constructor</code>:</p> <pre><code>sensor_constructor = ...\nwindowed_sensor_constructor = Window.from_partial_sensor(\n    sensor_constructor, ...)\n\n# ... somewhere inside the dataset constructor\nsensor_instance = windowed_sensor_constructor(path_to_trace)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>sensor</code> <code>Callable[[str], Sensor[Sample, TMetadata]]</code> <p>sensor constructor to wrap.</p> required <code>collate_fn</code> <code>Callable[[list[Sample]], SampleStack] | None</code> <p>collate function for aggregating a list of samples; if not specified, the samples are simply returned as a list.</p> <code>None</code> <code>past</code> <code>int</code> <p>number of past samples, in addition to the current sample. Set to <code>0</code> to disable.</p> <code>0</code> <code>future</code> <code>int</code> <p>number of future samples, in addition to the current sample. Set to <code>0</code> to disable.</p> <code>0</code> <code>crop</code> <code>bool</code> <p>if <code>True</code>, crop the first <code>past</code> and last <code>future</code> samples in the reported metadata to ensure that all samples are full valid.</p> <code>True</code> <code>parallel</code> <code>int | None</code> <p>maximum number of samples to load in parallel; if <code>None</code>, all samples are loaded sequentially.</p> <code>None</code> Source code in <code>src/abstract_dataloader/generic/sequence.py</code> <pre><code>@classmethod\ndef from_partial_sensor(\n    cls, sensor: Callable[[str], spec.Sensor[Sample, TMetadata]],\n    collate_fn: Callable[[list[Sample]], SampleStack] | None = None,\n    past: int = 0, future: int = 0, crop: bool = True,\n    parallel: int | None = None\n) -&gt; Callable[[str], \"Window[SampleStack, Sample, TMetadata]\"]:\n    \"\"\"Partially initialize from partially initialized sensor.\n\n    Use this to create windowed sensor constructors which can be\n    applied to different traces to construct a dataset. For example,\n    if you have a `sensor_constructor`:\n\n    ```python\n    sensor_constructor = ...\n    windowed_sensor_constructor = Window.from_partial_sensor(\n        sensor_constructor, ...)\n\n    # ... somewhere inside the dataset constructor\n    sensor_instance = windowed_sensor_constructor(path_to_trace)\n    ```\n\n    Args:\n        sensor: sensor *constructor* to wrap.\n        collate_fn: collate function for aggregating a list of samples; if\n            not specified, the samples are simply returned as a list.\n        past: number of past samples, in addition to the current sample.\n            Set to `0` to disable.\n        future: number of future samples, in addition to the current\n            sample. Set to `0` to disable.\n        crop: if `True`, crop the first `past` and last `future` samples in\n            the reported metadata to ensure that all samples are full\n            valid.\n        parallel: maximum number of samples to load in parallel; if `None`,\n            all samples are loaded sequentially.\n    \"\"\"\n    def create_wrapped_sensor(\n        path: str\n    ) -&gt; Window[SampleStack, Sample, TMetadata]:\n        return cls(\n            sensor(path), collate_fn=collate_fn, past=past,\n            future=future, crop=crop, parallel=parallel)\n\n    return create_wrapped_sensor\n</code></pre>"},{"location":"guide/","title":"Using the Abstract Dataloader","text":"<p>Recommendations</p> <ul> <li>Define your components using <code>abstract</code> base classes, and specify input components using <code>spec</code> interfaces.</li> <li>Set up a type system, and provide your types to the ADL <code>spec</code> and <code>generic</code> components as type parameters.</li> <li>Use a static type checker and a runtime type checker.</li> </ul>"},{"location":"guide/#using-adl-components","title":"Using ADL Components","text":"<p>Code using ADL-compliant components (which may itself be another ADL-compliant component) should use the exports provided in the <code>spec</code> as type annotations. When combined with static type checkers such as mypy or pyright and runtime (dynamic) type checkers such as beartype and typeguard, this allows code to be verified for some degree of type safety and spec compliance.</p> <pre><code>from dataclasses import dataclass\nfrom abstract_dataloader.spec import Sensor\n\n@dataclass\nclass Foo:\n    x: float\n\ndef example(adl_sensor: Sensor[Foo, Any], idx: int) -&gt; float:\n    x1 = adl_sensor[idx].x\n    # pyright: x: float\n    x2 = adl_sensor[x1]\n    # pyright: argument of type \"float\" cannot be assigned to parameter\n    # \"index\" of type \"int | Integer[Any]\"...\n    return x2\n</code></pre> <p>Spec Verification via Type Checking</p> <p>The <code>abstract_dataloader</code> module does not include any runtime type checking; downstream modules should enable runtime type checking if desired, for example with install hooks like <code>beartype_this_package()</code> or jaxtyping's <code>install_import_hook(...)</code>.</p>"},{"location":"guide/#implementing-adl-components","title":"Implementing ADL Components","text":"<p>There are three ways to implement ADL-compliant components, in order of preference:</p> <ol> <li>Use the abstract base classes provided in <code>abstract_dataloader.abstract</code>.</li> <li>Explicitly implement the protocols described in <code>abstract_dataloader.spec</code>.</li> <li>Implement the protocols using the specifications as guidance with neither <code>abstract_dataloader.abstract</code> nor <code>abstract_dataloader.spec</code> classes as base classes.</li> </ol>"},{"location":"guide/#using-the-abstract-base-classes","title":"Using the Abstract Base Classes","text":"<p>The <code>abstract_dataloader.abstract</code> submodule provides Abstract Base Class implementations of applicable specifications, with required methods annotated, and methods which can be written in terms of others \"polyfilled\" by default. This is the fastest way to get started:</p> ADL-compliant SensorSetup &amp; Types <pre><code>class Lidar(abstract.Sensor[LidarData, LidarMetadata]):\n\n    def __init__(\n        self, metadata: LidarMetadata, path: str, name: str = \"sensor\"\n    ) -&gt; None:\n        self.path = path\n        super().__init__(metadata=metadata, name=name)\n\n    def __getitem__(self, batch: int | np.integer) -&gt; LidarData:\n        blob_path = os.path.join(self.path, self.name, f\"{batch:06}.npz\")\n        with open(blob_path) as f:\n            return dict(np.load(f))\n</code></pre> <pre><code>from dataclasses import dataclass\nimport numpy as np\nfrom abstract_dataloader import spec, abstract\n\n@dataclass\nclass LidarMetadata(abstract.Metadata):\n    timestamps: Float64[np.ndarray, \"N\"]\n    # ... other data fields ...\n\n@dataclass\nclass LidarData:\n    # ... definition of data fields ...\n</code></pre> <p>Prefer <code>spec</code> to <code>abstract</code> as type bounds</p> <p>While this example does not take any ADL components as inputs, ADL-compliant implementations which take components as inputs should use <code>spec</code> types as annotations instead of <code>abstract</code> base classes as annotations. This preserves modularity; if an <code>abstract</code> base class is used as an input type annotation, this prevents the use of other compatible implementations which do not use the specified base class.</p>"},{"location":"guide/#explicitly-implementing-an-adl-component","title":"Explicitly Implementing an ADL Component","text":"<p>Implementations do not necessarily need to use the abstract base classes provided, and can instead use the provided specifications as base classes. This explicitly implements the defined specifications, which allows static type checkers to provide some degree of verification that you have implemented the specification.</p> <p>Example</p> <p>The provided <code>generic.Next</code> explicitly implements <code>spec.Synchronization</code>:</p> <pre><code>import numpy as np\nfrom jaxtyping import Float64, UInt32\nfrom abstract_dataloader import spec\n\nclass Next(spec.Synchronization):\n\n    def __init__(self, reference: str) -&gt; None:\n        self.reference = reference\n\n    def __call__(\n        self, timestamps: dict[str, Float64[np.ndarray, \"_N\"]]\n    ) -&gt; dict[str, UInt32[np.ndarray, \"M\"]]:\n        ref_time_all = timestamps[self.reference]\n        start_time = max(t[0] for t in timestamps.values())\n        end_time = min(t[-1] for t in timestamps.values())\n\n        start_idx = np.searchsorted(ref_time_all, start_time)\n        end_idx = np.searchsorted(ref_time_all, end_time)\n        ref_time = ref_time_all[start_idx:end_idx]\n        return {\n            k: np.searchsorted(v, ref_time).astype(np.uint32)\n            for k, v in timestamps.items()}\n</code></pre>"},{"location":"guide/#implicitly-implementing-an-adl-component","title":"Implicitly Implementing an ADL Component","text":"<p>Since the abstract dataloader is based entirely on structural subtyping - \"static duck typing\", implementing an ADL-compliant component does not actually require the abstract dataloader!</p> <ul> <li>Simply implementing the protocols described by the ADL, using the same types (or with more general inputs and more specific outputs), automatically allows for ADL compatibility with downstream components which use only the ADL spec and correctly apply ADL spec type-checking.</li> <li>Since the types only describe behavior, parts or all of the <code>spec</code> can be copied into other modules; these copies then become totally interchangeable with the originals!</li> </ul>"},{"location":"guide/#extending-protocols","title":"Extending Protocols","text":"<p>When extending the abstract dataloader specifications with additional domain-specific features, keep in mind that your implementation must be usable wherever an ADL-compliant dataloader is used. This is known as the Liskov Substitution Principle (and is the error you will get from static type checkers if you break that compatibility). The key implications are as follows:</p> <p>Preconditions cannot be strengthened in the subtype.</p> <ul> <li>Implementations may not impose stricter requirements on the input types; for example, where <code>__getitem__</code> is expected to allow <code>index: int | np.integer</code>, implementations may not require more specific types, e.g. <code>np.int32</code>.</li> <li>This also implies that additional arguments must be optional, and have defaults, so that they are not required to be set when called.</li> </ul> <p>Postconditions cannot be weakened in the subtype.</p> <ul> <li>Return types cannot be more general than in the specifications. In general, this should never be an issue, since the return types provided in the abstract dataloader specifications are extremely general.</li> <li>However, the generic types provided should be followed; for example, when <code>Sensor.__getitem__</code> returns a certain <code>Sample</code>, <code>Sensor.stream</code> should also return that same type.</li> </ul> <p>Example</p> <p>The <code>abstract.Sensor</code> provides a <code>stream</code> implementation with an additional <code>batch</code> argument that modifies the return type, while still remaining fully compliant with the <code>Sensor</code> specification.</p> <pre><code>@overload\ndef stream(self, batch: None = None) -&gt; Iterator[TSample]: ...\n\n@overload\ndef stream(self, batch: int) -&gt; Iterator[list[TSample]]: ...\n\ndef stream(\n    self, batch: int | None = None\n) -&gt; Iterator[TSample | list[TSample]]:\n    if batch is None:\n        for i in range(len(self)):\n            yield self[i]\n    else:\n        for i in range(len(self) // batch):\n            yield [self[j] for j in range(i * batch, (i + 1) * batch)]\n</code></pre> <p>This is accomplished using an <code>@overload</code> with a default that falls back to the specification:</p> <ul> <li>If <code>.stream()</code> is used only as described by the spec, <code>batch=None</code> matches the first overload, which returns an <code>Iterator[TSample]</code>, just as in the spec.</li> <li>If <code>.stream()</code> is passed a <code>batch=...</code> argument, the second overload now matches, instead returning an <code>Iterator[list[TSample]]</code>.</li> </ul>"},{"location":"spec/","title":"Spec","text":""},{"location":"spec/#abstract_dataloader.spec","title":"Abstract Dataloader Specifications","text":"<p>Abstract Dataloader Specifications.</p> <p>The implementations here provide \"duck type\" protocol definitions of key data loading primitives. In order to implement the specification, users simply need to \"fill in\" the methods described here for the types which they wish to implement.</p> <p>Type Parameters</p> <p>ADL specification protocol types are defined as generics, which are parameterized by other types. These type parameters are documented by a <code>Type Parameters</code> section where applicable.</p> <p>Composition Rules</p> <p>ADL protocols which can be composed together are documented by a <code>Composition Rules</code> section.</p>"},{"location":"spec/#abstract_dataloader.spec.Metadata","title":"abstract_dataloader.spec.Metadata","text":"<p>               Bases: <code>Protocol</code></p> <p>Sensor metadata.</p> <p>All sensor metadata is expected to be held in memory during training, so great effort should be taken to minimize its memory usage. Any additional information which is not strictly necessary for book-keeping, or which takes more than negligible space, should be loaded as data instead.</p> <p>Note</p> <p>This can be a <code>@dataclass</code>, <code>typing.NamedTuple</code>, or a fully custom type - it just has to expose a <code>timestamps</code> attribute.</p> <p>Attributes:</p> Name Type Description <code>timestamps</code> <code>Float[ndarray, N]</code> <p>measurement timestamps, in seconds. Nominally in epoch time; must be consistent within each trace (but not necessarily across traces). Suggested type: <code>float64,</code> which gives precision of &lt;1us.</p> Source code in <code>src/abstract_dataloader/spec.py</code> <pre><code>@runtime_checkable\nclass Metadata(Protocol):\n    \"\"\"Sensor metadata.\n\n    All sensor metadata is expected to be held in memory during training, so\n    great effort should be taken to minimize its memory usage. Any additional\n    information which is not strictly necessary for book-keeping, or which\n    takes more than negligible space, should be loaded as data instead.\n\n    !!! note\n\n        This can be a `@dataclass`, [`typing.NamedTuple`][typing.NamedTuple],\n        or a fully custom type - it just has to expose a `timestamps`\n        attribute.\n\n    Attributes:\n        timestamps: measurement timestamps, in seconds. Nominally in epoch\n            time; must be consistent within each trace (but not necessarily\n            across traces). Suggested type: `float64,` which gives precision of\n            &lt;1us.\n    \"\"\"\n\n    timestamps: Float[np.ndarray, \"N\"]\n</code></pre>"},{"location":"spec/#abstract_dataloader.spec.Sensor","title":"abstract_dataloader.spec.Sensor","text":"<p>               Bases: <code>Protocol</code>, <code>Generic[TSample, TMetadata]</code></p> <p>A sensor, consisting of a synchronous time-series of measurements.</p> <p>This protocol is parameterized by generic <code>TSample</code> and <code>TMetadata</code> types, which can encode the expected data type of this sensor. For example:</p> <pre><code>class Point2D(TypedDict):\n    x: float\n    y: float\n\ndef point_transform(point_sensor: Sensor[Point2D, Any]) -&gt; T:\n    ...\n</code></pre> <p>This encodes an argument, <code>point_sensor</code>, which expected to be a sensor that reads data with type <code>Point2D</code>, but does not specify a metadata type.</p> Type Parameters <ul> <li><code>TSample</code>: sample data type which this <code>Sensor</code> returns. As a     convention, we suggest returning \"batched\" data by default, i.e.     with a leading singleton axis.</li> <li><code>TMetadata</code>: metadata type associated with this sensor; must     implement <code>Metadata</code>.</li> </ul> <p>Attributes:</p> Name Type Description <code>metadata</code> <code>TMetadata</code> <p>sensor metadata, including timestamp information.</p> Source code in <code>src/abstract_dataloader/spec.py</code> <pre><code>@runtime_checkable\nclass Sensor(Protocol, Generic[TSample, TMetadata]):\n    \"\"\"A sensor, consisting of a synchronous time-series of measurements.\n\n    This protocol is parameterized by generic `TSample` and `TMetadata` types,\n    which can encode the expected data type of this sensor. For example:\n\n    ```python\n    class Point2D(TypedDict):\n        x: float\n        y: float\n\n    def point_transform(point_sensor: Sensor[Point2D, Any]) -&gt; T:\n        ...\n    ```\n\n    This encodes an argument, `point_sensor`, which expected to be a sensor\n    that reads data with type `Point2D`, but does not specify a metadata type.\n\n    Type Parameters:\n        - `TSample`: sample data type which this `Sensor` returns. As a\n            convention, we suggest returning \"batched\" data by default, i.e.\n            with a leading singleton axis.\n        - `TMetadata`: metadata type associated with this sensor; must\n            implement [`Metadata`][^.].\n\n    Attributes:\n        metadata: sensor metadata, including timestamp information.\n    \"\"\"\n\n    metadata: TMetadata\n\n    def stream(self) -&gt; Iterator[TSample]:\n        \"\"\"Stream values recorded by this sensor.\n\n        Returns:\n            An iterator yielding successive samples.\n        \"\"\"\n        ...\n\n    def __getitem__(self, index: int | np.integer) -&gt; TSample:\n        \"\"\"Fetch measurements from this sensor, by index.\n\n        Args:\n            index: sample index, in the sensor scope.\n\n        Returns:\n            Loaded sample.\n        \"\"\"\n        ...\n\n    def __len__(self) -&gt; int:\n        \"\"\"Total number of measurements.\"\"\"\n        ...\n</code></pre>"},{"location":"spec/#abstract_dataloader.spec.Sensor.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(index: int | integer) -&gt; TSample\n</code></pre> <p>Fetch measurements from this sensor, by index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int | integer</code> <p>sample index, in the sensor scope.</p> required <p>Returns:</p> Type Description <code>TSample</code> <p>Loaded sample.</p> Source code in <code>src/abstract_dataloader/spec.py</code> <pre><code>def __getitem__(self, index: int | np.integer) -&gt; TSample:\n    \"\"\"Fetch measurements from this sensor, by index.\n\n    Args:\n        index: sample index, in the sensor scope.\n\n    Returns:\n        Loaded sample.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"spec/#abstract_dataloader.spec.Sensor.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Total number of measurements.</p> Source code in <code>src/abstract_dataloader/spec.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Total number of measurements.\"\"\"\n    ...\n</code></pre>"},{"location":"spec/#abstract_dataloader.spec.Sensor.stream","title":"stream","text":"<pre><code>stream() -&gt; Iterator[TSample]\n</code></pre> <p>Stream values recorded by this sensor.</p> <p>Returns:</p> Type Description <code>Iterator[TSample]</code> <p>An iterator yielding successive samples.</p> Source code in <code>src/abstract_dataloader/spec.py</code> <pre><code>def stream(self) -&gt; Iterator[TSample]:\n    \"\"\"Stream values recorded by this sensor.\n\n    Returns:\n        An iterator yielding successive samples.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"spec/#abstract_dataloader.spec.Synchronization","title":"abstract_dataloader.spec.Synchronization","text":"<p>               Bases: <code>Protocol</code></p> <p>Synchronization protocol for asynchronous time-series.</p> <p>Defines a rule for creating matching sensor index tuples which correspond to some kind of global index.</p> <p>Generic Implementations</p> <p>The following generic implementations are included with <code>abstract_dataloader.generic</code>:</p> <ul> <li><code>Empty</code>: a no-op for     intializing a trace without any synchronization (i.e., just as a     container of sensors).</li> <li><code>Nearest</code>: find the nearest     measurement for each sensor relative to the reference sensor's     measurements.</li> <li><code>Next</code>: find the next     measurement for each sensor relative to the reference sensor's     measurements.</li> </ul> Source code in <code>src/abstract_dataloader/spec.py</code> <pre><code>@runtime_checkable\nclass Synchronization(Protocol):\n    \"\"\"Synchronization protocol for asynchronous time-series.\n\n    Defines a rule for creating matching sensor index tuples which correspond\n    to some kind of global index.\n\n    !!! info \"Generic Implementations\"\n\n        The following generic implementations are included with\n        [`abstract_dataloader.generic`][abstract_dataloader.generic]:\n\n        - [`Empty`][abstract_dataloader.generic.Empty]: a no-op for\n            intializing a trace without any synchronization (i.e., just as a\n            container of sensors).\n        - [`Nearest`][abstract_dataloader.generic.Nearest]: find the nearest\n            measurement for each sensor relative to the reference sensor's\n            measurements.\n        - [`Next`][abstract_dataloader.generic.Next]: find the next\n            measurement for each sensor relative to the reference sensor's\n            measurements.\n    \"\"\"\n\n    def __call__(\n        self, timestamps: dict[str, Float[np.ndarray, \"_N\"]]\n    ) -&gt; dict[str, Integer[np.ndarray, \"M\"]]:\n        \"\"\"Apply synchronization protocol.\n\n        Args:\n            timestamps: sensor timestamps. Each key denotes a different sensor\n                name, and the value denotes the timestamps for that sensor.\n\n        Returns:\n            A dictionary, where keys correspond to each sensor, and values\n                correspond to the indices which map global indices to sensor\n                indices, i.e. `global[sensor, i] = sensor[sync[sensor] [i]]`.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"spec/#abstract_dataloader.spec.Synchronization.__call__","title":"__call__","text":"<pre><code>__call__(\n    timestamps: dict[str, Float[ndarray, _N]],\n) -&gt; dict[str, Integer[ndarray, M]]\n</code></pre> <p>Apply synchronization protocol.</p> <p>Parameters:</p> Name Type Description Default <code>timestamps</code> <code>dict[str, Float[ndarray, _N]]</code> <p>sensor timestamps. Each key denotes a different sensor name, and the value denotes the timestamps for that sensor.</p> required <p>Returns:</p> Type Description <code>dict[str, Integer[ndarray, M]]</code> <p>A dictionary, where keys correspond to each sensor, and values correspond to the indices which map global indices to sensor indices, i.e. <code>global[sensor, i] = sensor[sync[sensor] [i]]</code>.</p> Source code in <code>src/abstract_dataloader/spec.py</code> <pre><code>def __call__(\n    self, timestamps: dict[str, Float[np.ndarray, \"_N\"]]\n) -&gt; dict[str, Integer[np.ndarray, \"M\"]]:\n    \"\"\"Apply synchronization protocol.\n\n    Args:\n        timestamps: sensor timestamps. Each key denotes a different sensor\n            name, and the value denotes the timestamps for that sensor.\n\n    Returns:\n        A dictionary, where keys correspond to each sensor, and values\n            correspond to the indices which map global indices to sensor\n            indices, i.e. `global[sensor, i] = sensor[sync[sensor] [i]]`.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"spec/#abstract_dataloader.spec.Trace","title":"abstract_dataloader.spec.Trace","text":"<p>               Bases: <code>Protocol</code>, <code>Generic[TSample]</code></p> <p>A sensor trace, consisting of multiple simultaneously-recording sensors.</p> <p>This protocol is parameterized by a generic <code>Sample</code> type, which can encode the expected data type of this trace.</p> <p>Why not <code>Sequence</code>?</p> <p>While collections of simultaneously-recorded sensor data are commonly referred to as \"sequences,\" the <code>Sequence</code> name conflicts with the python standard-library <code>Sequence</code> type. Since the abstract dataloader heavily references these abstract types (and you should too!), we use \"Trace\" to avoid conflicts.</p> Type Parameters <ul> <li><code>Sample</code>: sample data type which this <code>Trace</code> returns. As a     convention, we suggest returning \"batched\" data by default, i.e.     with a leading singleton axis.</li> </ul> Source code in <code>src/abstract_dataloader/spec.py</code> <pre><code>@runtime_checkable\nclass Trace(Protocol, Generic[TSample]):\n    \"\"\"A sensor trace, consisting of multiple simultaneously-recording sensors.\n\n    This protocol is parameterized by a generic `Sample` type, which can encode\n    the expected data type of this trace.\n\n    !!! question \"Why not `Sequence`?\"\n\n        While collections of simultaneously-recorded sensor data are commonly\n        referred to as \"sequences,\" the `Sequence` name conflicts with the\n        python standard-library\n        [`Sequence`](https://docs.python.org/3/library/collections.abc.html#collections.abc.Sequence)\n        type. Since the abstract dataloader heavily references these\n        [abstract types][collections.abc] (and you should too!), we use \"Trace\"\n        to avoid conflicts.\n\n    Type Parameters:\n        - `Sample`: sample data type which this `Trace` returns. As a\n            convention, we suggest returning \"batched\" data by default, i.e.\n            with a leading singleton axis.\n    \"\"\"\n\n    @overload\n    def __getitem__(self, index: str) -&gt; Sensor: ...\n\n    @overload\n    def __getitem__(self, index: int | np.integer) -&gt; TSample: ...\n\n    def __getitem__(self, index: int | np.integer | str) -&gt; TSample | Sensor:\n        \"\"\"Get item from global index (or fetch a sensor by name).\n\n        !!! info\n\n            For the user's convenience, traces can be indexed by a `str` sensor\n            name, returning that [`Sensor`][^^.]. While we are generally wary\n            of requiring \"quality of life\" features, we include this since a\n            simple `isinstance(index, str)` check suffices to implement this\n            feature.\n\n        Args:\n            index: sample index, or sensor name.\n\n        Returns:\n            Loaded sample if `index` is an integer type, or the appropriate\n            [`Sensor`][^^.] if `index` is a `str`.\n        \"\"\"\n        ...\n\n    def __len__(self) -&gt; int:\n        \"\"\"Total number of sensor-tuple samples.\"\"\"\n        ...\n</code></pre>"},{"location":"spec/#abstract_dataloader.spec.Trace.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(index: str) -&gt; Sensor\n</code></pre><pre><code>__getitem__(index: int | integer) -&gt; TSample\n</code></pre> <pre><code>__getitem__(index: int | integer | str) -&gt; TSample | Sensor\n</code></pre> <p>Get item from global index (or fetch a sensor by name).</p> <p>Info</p> <p>For the user's convenience, traces can be indexed by a <code>str</code> sensor name, returning that <code>Sensor</code>. While we are generally wary of requiring \"quality of life\" features, we include this since a simple <code>isinstance(index, str)</code> check suffices to implement this feature.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int | integer | str</code> <p>sample index, or sensor name.</p> required <p>Returns:</p> Type Description <code>TSample | Sensor</code> <p>Loaded sample if <code>index</code> is an integer type, or the appropriate</p> <code>TSample | Sensor</code> <p><code>Sensor</code> if <code>index</code> is a <code>str</code>.</p> Source code in <code>src/abstract_dataloader/spec.py</code> <pre><code>def __getitem__(self, index: int | np.integer | str) -&gt; TSample | Sensor:\n    \"\"\"Get item from global index (or fetch a sensor by name).\n\n    !!! info\n\n        For the user's convenience, traces can be indexed by a `str` sensor\n        name, returning that [`Sensor`][^^.]. While we are generally wary\n        of requiring \"quality of life\" features, we include this since a\n        simple `isinstance(index, str)` check suffices to implement this\n        feature.\n\n    Args:\n        index: sample index, or sensor name.\n\n    Returns:\n        Loaded sample if `index` is an integer type, or the appropriate\n        [`Sensor`][^^.] if `index` is a `str`.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"spec/#abstract_dataloader.spec.Trace.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Total number of sensor-tuple samples.</p> Source code in <code>src/abstract_dataloader/spec.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Total number of sensor-tuple samples.\"\"\"\n    ...\n</code></pre>"},{"location":"spec/#abstract_dataloader.spec.Dataset","title":"abstract_dataloader.spec.Dataset","text":"<p>               Bases: <code>Protocol</code>, <code>Generic[TSample]</code></p> <p>A dataset, consisting of multiple traces concatenated together.</p> <p><code>Trace</code> subtypes <code>Dataset</code></p> <p>Due to the type signatures, a <code>Trace</code> is actually a subtype of <code>Dataset</code>. This means that a dataset which implements a collection of traces can also take a collection of datasets!</p> Type Parameters <ul> <li><code>TSample</code>: sample data type which this <code>Dataset</code> returns. As a     convention, we suggest returning \"batched\" data by default, i.e.     with a leading singleton axis.</li> </ul> Source code in <code>src/abstract_dataloader/spec.py</code> <pre><code>@runtime_checkable\nclass Dataset(Protocol, Generic[TSample]):\n    \"\"\"A dataset, consisting of multiple traces concatenated together.\n\n    !!! note \"[`Trace`][^.] subtypes [`Dataset`][^.]\"\n\n        Due to the type signatures, a [`Trace`][^.] is actually a subtype of\n        [`Dataset`][^.]. This means that a dataset which implements a\n        collection of traces can also take a collection of datasets!\n\n    Type Parameters:\n        - `TSample`: sample data type which this `Dataset` returns. As a\n            convention, we suggest returning \"batched\" data by default, i.e.\n            with a leading singleton axis.\n    \"\"\"\n\n    def __getitem__(self, index: int | np.integer) -&gt; TSample:\n        \"\"\"Fetch item from this dataset by global index.\n\n        Args:\n            index: sample index.\n\n        Returns:\n            Loaded sample.\n        \"\"\"\n        ...\n\n    def __len__(self) -&gt; int:\n        \"\"\"Total number of samples in this dataset.\"\"\"\n        ...\n</code></pre>"},{"location":"spec/#abstract_dataloader.spec.Dataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(index: int | integer) -&gt; TSample\n</code></pre> <p>Fetch item from this dataset by global index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int | integer</code> <p>sample index.</p> required <p>Returns:</p> Type Description <code>TSample</code> <p>Loaded sample.</p> Source code in <code>src/abstract_dataloader/spec.py</code> <pre><code>def __getitem__(self, index: int | np.integer) -&gt; TSample:\n    \"\"\"Fetch item from this dataset by global index.\n\n    Args:\n        index: sample index.\n\n    Returns:\n        Loaded sample.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"spec/#abstract_dataloader.spec.Dataset.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Total number of samples in this dataset.</p> Source code in <code>src/abstract_dataloader/spec.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Total number of samples in this dataset.\"\"\"\n    ...\n</code></pre>"},{"location":"spec/#abstract_dataloader.spec.Transform","title":"abstract_dataloader.spec.Transform","text":"<p>               Bases: <code>Protocol</code>, <code>Generic[TRaw, TTransformed]</code></p> <p>Sample or batch-wise transform.</p> <p>Note</p> <p>This protocol is a suggestively-named equivalent to <code>Callable[[TRaw], TTransformed]</code> or <code>Callable[[Any], Any]</code>.</p> Composition Rules <ul> <li><code>Transform</code> can be freely composed, as long as each transform's   <code>TTransformed</code> matches the next transform's <code>TRaw</code>; this composition   is implemented by <code>abstract.Transform</code>.</li> <li>Composed <code>Transform</code>s result in another <code>Transform</code>:   <pre><code>Transform[T2, T3] (.) Transform[T1, T2] = Transform[T1, T3].\n</code></pre></li> </ul> Type Parameters <ul> <li><code>TRaw</code>: Input data type.</li> <li><code>TTransformed</code>: Output data type.</li> </ul> Source code in <code>src/abstract_dataloader/spec.py</code> <pre><code>@runtime_checkable\nclass Transform(Protocol, Generic[TRaw, TTransformed]):\n    \"\"\"Sample or batch-wise transform.\n\n    !!! note\n\n        This protocol is a suggestively-named equivalent to\n        `Callable[[TRaw], TTransformed]` or `Callable[[Any], Any]`.\n\n    Composition Rules:\n        - `Transform` can be freely composed, as long as each transform's\n          `TTransformed` matches the next transform's `TRaw`; this composition\n          is implemented by [`abstract.Transform`][abstract_dataloader.].\n        - Composed `Transform`s result in another `Transform`:\n          ```\n          Transform[T2, T3] (.) Transform[T1, T2] = Transform[T1, T3].\n          ```\n\n    Type Parameters:\n        - `TRaw`: Input data type.\n        - `TTransformed`: Output data type.\n    \"\"\"\n\n    def __call__(self, data: TRaw) -&gt; TTransformed:\n        \"\"\"Apply transform to a single sample.\n\n        Args:\n            data: A single `TRaw` data sample.\n\n        Returns:\n            A single `TTransformed` data sample.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"spec/#abstract_dataloader.spec.Transform.__call__","title":"__call__","text":"<pre><code>__call__(data: TRaw) -&gt; TTransformed\n</code></pre> <p>Apply transform to a single sample.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>TRaw</code> <p>A single <code>TRaw</code> data sample.</p> required <p>Returns:</p> Type Description <code>TTransformed</code> <p>A single <code>TTransformed</code> data sample.</p> Source code in <code>src/abstract_dataloader/spec.py</code> <pre><code>def __call__(self, data: TRaw) -&gt; TTransformed:\n    \"\"\"Apply transform to a single sample.\n\n    Args:\n        data: A single `TRaw` data sample.\n\n    Returns:\n        A single `TTransformed` data sample.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"spec/#abstract_dataloader.spec.Collate","title":"abstract_dataloader.spec.Collate","text":"<p>               Bases: <code>Protocol</code>, <code>Generic[TTransformed, TCollated]</code></p> <p>Data collation.</p> <p>Note</p> <p>This protocol is a equivalent to <code>Callable[[Sequence[TTransformed]], TCollated]</code>. <code>Collate</code> can also be viewed as a special case of <code>Transform</code>, where the input type <code>TRaw</code> must be a <code>Sequence[...]</code>.</p> Composition Rules <ul> <li><code>Collate</code> can only be composed in parallel, and can never be   sequentially composed.</li> </ul> Type Parameters <ul> <li><code>TTransformed</code>: Input data type.</li> <li><code>TCollated</code>: Output data type.</li> </ul> Source code in <code>src/abstract_dataloader/spec.py</code> <pre><code>@runtime_checkable\nclass Collate(Protocol, Generic[TTransformed, TCollated]):\n    \"\"\"Data collation.\n\n    !!! note\n\n        This protocol is a equivalent to\n        `Callable[[Sequence[TTransformed]], TCollated]`. `Collate` can also\n        be viewed as a special case of `Transform`, where the input type\n        `TRaw` must be a `Sequence[...]`.\n\n    Composition Rules:\n        - `Collate` can only be composed in parallel, and can never be\n          sequentially composed.\n\n    Type Parameters:\n        - `TTransformed`: Input data type.\n        - `TCollated`: Output data type.\n    \"\"\"\n\n    def __call__(self, data: Sequence[TTransformed]) -&gt; TCollated:\n        \"\"\"Collate a set of samples.\n\n        Args:\n            data: A set of `TTransformed` samples.\n\n        Returns:\n            A `TCollated` batch.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"spec/#abstract_dataloader.spec.Collate.__call__","title":"__call__","text":"<pre><code>__call__(data: Sequence[TTransformed]) -&gt; TCollated\n</code></pre> <p>Collate a set of samples.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Sequence[TTransformed]</code> <p>A set of <code>TTransformed</code> samples.</p> required <p>Returns:</p> Type Description <code>TCollated</code> <p>A <code>TCollated</code> batch.</p> Source code in <code>src/abstract_dataloader/spec.py</code> <pre><code>def __call__(self, data: Sequence[TTransformed]) -&gt; TCollated:\n    \"\"\"Collate a set of samples.\n\n    Args:\n        data: A set of `TTransformed` samples.\n\n    Returns:\n        A `TCollated` batch.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"spec/#abstract_dataloader.spec.Pipeline","title":"abstract_dataloader.spec.Pipeline","text":"<p>               Bases: <code>Protocol</code>, <code>Generic[TRaw, TTransformed, TCollated, TProcessed]</code></p> <p>Dataloader transform pipeline.</p> <p>This protocol is parameterized by four type variables which encode the different data formats at each stage in the pipeline. This forms a <code>Raw -&gt; Transformed -&gt; Collated -&gt; Processed</code> pipeline with three transforms:</p> <ul> <li><code>sample</code>: a sample to sample transform; can be sequentially   assembled from one or more <code>Transform</code>s.</li> <li><code>collate</code>: a list-of-samples to batch transform. Can use exactly one   <code>Collate</code>.</li> <li><code>batch</code>: a batch to batch transform; can be sequentially assembled   from one or more <code>Transform</code>s.</li> </ul> Composition Rules <ul> <li>A full <code>Pipeline</code> can be sequentially pre-composed and/or   post-composed with one or more <code>Transform</code>s; this is   implemented by <code>generic.ComposedPipeline</code>.</li> <li><code>Pipeline</code>s can always be composed in parallel; this is implemented   by <code>generic.ParallelPipelines</code>, with a   pytorch <code>nn.Module</code>-compatible version in   <code>torch.ParallelPipelines</code>.</li> </ul> Type Parameters <ul> <li><code>TRaw</code>: Input data format.</li> <li><code>TTransformed</code>: Data after the first <code>transform</code> step.</li> <li><code>TCollated</code>: Data after the second <code>collate</code> step.</li> <li><code>TProcessed</code>: Output data format.</li> </ul> Source code in <code>src/abstract_dataloader/spec.py</code> <pre><code>@runtime_checkable\nclass Pipeline(\n    Protocol, Generic[TRaw, TTransformed, TCollated, TProcessed]\n):\n    \"\"\"Dataloader transform pipeline.\n\n    This protocol is parameterized by four type variables which encode the\n    different data formats at each stage in the pipeline. This forms a\n    `Raw -&gt; Transformed -&gt; Collated -&gt; Processed` pipeline with three\n    transforms:\n\n    - [`sample`][.]: a sample to sample transform; can be sequentially\n      assembled from one or more [`Transform`][^.]s.\n    - [`collate`][.]: a list-of-samples to batch transform. Can use exactly one\n      [`Collate`][^.].\n    - [`batch`][.]: a batch to batch transform; can be sequentially assembled\n      from one or more [`Transform`][^.]s.\n\n    Composition Rules:\n        - A full `Pipeline` can be sequentially pre-composed and/or\n          post-composed with one or more [`Transform`][^.]s; this is\n          implemented by [`generic.ComposedPipeline`][abstract_dataloader.].\n        - `Pipeline`s can always be composed in parallel; this is implemented\n          by [`generic.ParallelPipelines`][abstract_dataloader.], with a\n          pytorch [`nn.Module`][torch.]-compatible version in\n          [`torch.ParallelPipelines`][abstract_dataloader.].\n\n    Type Parameters:\n        - `TRaw`: Input data format.\n        - `TTransformed`: Data after the first `transform` step.\n        - `TCollated`: Data after the second `collate` step.\n        - `TProcessed`: Output data format.\n    \"\"\"\n\n    def sample(self, data: TRaw) -&gt; TTransformed:\n        \"\"\"Transform single samples.\n\n        - Operates on single samples, nominally on the CPU-side of a\n          dataloader.\n        - This method is both sequentially and parallel composable.\n\n        Args:\n            data: A single `TRaw` data sample.\n\n        Returns:\n            A single `TTransformed` data sample.\n        \"\"\"\n        ...\n\n    def collate(self, data: Sequence[TTransformed]) -&gt; TCollated:\n        \"\"\"Collate a list of data samples into a GPU-ready batch.\n\n        - Operates on the CPU-side of the dataloader, and is responsible for\n          aggregating individual samples into a batch (but not transferring to\n          the GPU).\n        - Analogous to the `collate_fn` of a\n          [pytorch dataloader](https://pytorch.org/docs/stable/data.html).\n        - This method is not sequentially composable.\n\n        Args:\n            data: A sequence of `TTransformed` data samples.\n\n        Returns:\n            A `TCollated` collection of the input sequence.\n        \"\"\"\n        ...\n\n    def batch(self, data: TCollated) -&gt; TProcessed:\n        \"\"\"Transform data batch.\n\n        - Operates on a batch of data, nominally on the GPU-side of a\n          dataloader.\n        - This method is both sequentially and parallel composable.\n\n        !!! info \"Implementation as `torch.nn.Module`\"\n\n            If these `Transforms` require GPU state, it may be helpful to\n            implement it as a `torch.nn.Module`. In this case, `batch`\n            should redirect to `__call__`, which in turn redirects to\n            [`nn.Module.forward`][torch.] in order to handle any registered\n            pytorch hooks.\n\n        Args:\n            data: A `TCollated` batch of data, nominally already sent to the\n                GPU.\n\n        Returns:\n            The `TProcessed` output, ready for the downstream model.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"spec/#abstract_dataloader.spec.Pipeline.batch","title":"batch","text":"<pre><code>batch(data: TCollated) -&gt; TProcessed\n</code></pre> <p>Transform data batch.</p> <ul> <li>Operates on a batch of data, nominally on the GPU-side of a   dataloader.</li> <li>This method is both sequentially and parallel composable.</li> </ul> <p>Implementation as <code>torch.nn.Module</code></p> <p>If these <code>Transforms</code> require GPU state, it may be helpful to implement it as a <code>torch.nn.Module</code>. In this case, <code>batch</code> should redirect to <code>__call__</code>, which in turn redirects to <code>nn.Module.forward</code> in order to handle any registered pytorch hooks.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>TCollated</code> <p>A <code>TCollated</code> batch of data, nominally already sent to the GPU.</p> required <p>Returns:</p> Type Description <code>TProcessed</code> <p>The <code>TProcessed</code> output, ready for the downstream model.</p> Source code in <code>src/abstract_dataloader/spec.py</code> <pre><code>def batch(self, data: TCollated) -&gt; TProcessed:\n    \"\"\"Transform data batch.\n\n    - Operates on a batch of data, nominally on the GPU-side of a\n      dataloader.\n    - This method is both sequentially and parallel composable.\n\n    !!! info \"Implementation as `torch.nn.Module`\"\n\n        If these `Transforms` require GPU state, it may be helpful to\n        implement it as a `torch.nn.Module`. In this case, `batch`\n        should redirect to `__call__`, which in turn redirects to\n        [`nn.Module.forward`][torch.] in order to handle any registered\n        pytorch hooks.\n\n    Args:\n        data: A `TCollated` batch of data, nominally already sent to the\n            GPU.\n\n    Returns:\n        The `TProcessed` output, ready for the downstream model.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"spec/#abstract_dataloader.spec.Pipeline.collate","title":"collate","text":"<pre><code>collate(data: Sequence[TTransformed]) -&gt; TCollated\n</code></pre> <p>Collate a list of data samples into a GPU-ready batch.</p> <ul> <li>Operates on the CPU-side of the dataloader, and is responsible for   aggregating individual samples into a batch (but not transferring to   the GPU).</li> <li>Analogous to the <code>collate_fn</code> of a   pytorch dataloader.</li> <li>This method is not sequentially composable.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Sequence[TTransformed]</code> <p>A sequence of <code>TTransformed</code> data samples.</p> required <p>Returns:</p> Type Description <code>TCollated</code> <p>A <code>TCollated</code> collection of the input sequence.</p> Source code in <code>src/abstract_dataloader/spec.py</code> <pre><code>def collate(self, data: Sequence[TTransformed]) -&gt; TCollated:\n    \"\"\"Collate a list of data samples into a GPU-ready batch.\n\n    - Operates on the CPU-side of the dataloader, and is responsible for\n      aggregating individual samples into a batch (but not transferring to\n      the GPU).\n    - Analogous to the `collate_fn` of a\n      [pytorch dataloader](https://pytorch.org/docs/stable/data.html).\n    - This method is not sequentially composable.\n\n    Args:\n        data: A sequence of `TTransformed` data samples.\n\n    Returns:\n        A `TCollated` collection of the input sequence.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"spec/#abstract_dataloader.spec.Pipeline.sample","title":"sample","text":"<pre><code>sample(data: TRaw) -&gt; TTransformed\n</code></pre> <p>Transform single samples.</p> <ul> <li>Operates on single samples, nominally on the CPU-side of a   dataloader.</li> <li>This method is both sequentially and parallel composable.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>TRaw</code> <p>A single <code>TRaw</code> data sample.</p> required <p>Returns:</p> Type Description <code>TTransformed</code> <p>A single <code>TTransformed</code> data sample.</p> Source code in <code>src/abstract_dataloader/spec.py</code> <pre><code>def sample(self, data: TRaw) -&gt; TTransformed:\n    \"\"\"Transform single samples.\n\n    - Operates on single samples, nominally on the CPU-side of a\n      dataloader.\n    - This method is both sequentially and parallel composable.\n\n    Args:\n        data: A single `TRaw` data sample.\n\n    Returns:\n        A single `TTransformed` data sample.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"torch/","title":"Torch","text":""},{"location":"torch/#abstract_dataloader.torch","title":"Pytorch Interoperability","text":"<p>Pytorch-ADL wrappers.</p> <p>These implementations are a superset of the <code>generic</code> components, and provide interoperability with pytorch dataloaders, modules, etc. For example, any <code>Pipeline</code>-related components which could contain pytorch <code>nn.Module</code>s are modified to subclass <code>nn.Module</code> in order to properly register them.</p> <p>Warning</p> <p>This module is not automatically imported; you will need to explicitly import it:</p> <pre><code>from abstract_dataloader import torch as adl_torch\n</code></pre> <p>Since pytorch is not declared as a required dependency, you will also need to install <code>torch</code> (or install the <code>torch</code> extra with <code>pip install abstract_dataloader[torch]</code>).</p> <p>Note</p> <p>Recursive tree operations such as reshaping and stacking are performed using the <code>optree</code> library, or, if that is not present, <code>torch.utils._pytree</code>, which implements equivalent functionality. If <code>torch.utils._pytree</code> is removed in a later version, the constructor will raise <code>NotImplementedError</code>, and this fallback will need to be replaced.</p> <p>Warning</p> <p>Custom data container classes such as <code>@dataclass</code> are only supported if <code>optree</code> is installed, and they are registered with optree. However, <code>dict</code>, <code>list</code>, <code>tuple</code>, and equivalent types such as <code>TypedDict</code> and <code>NamedTuple</code> will work out of the box.</p>"},{"location":"torch/#abstract_dataloader.torch.Collate","title":"abstract_dataloader.torch.Collate","text":"<p>               Bases: <code>Collate[TTransformed, TCollated]</code></p> <p>Generic numpy to pytorch collation.</p> <p>Converts numpy arrays to pytorch tensors, and either stacks or concatenates each value.</p> Type Parameters <ul> <li><code>TTransformed</code>: input sample type.</li> <li><code>TCollated</code>: output collated type.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Literal['stack', 'concat']</code> <p>whether to <code>stack</code> or <code>concat</code> during collation.</p> <code>'concat'</code> Source code in <code>src/abstract_dataloader/torch/torch.py</code> <pre><code>class Collate(spec.Collate[TTransformed, TCollated]):\n    \"\"\"Generic numpy to pytorch collation.\n\n    Converts numpy arrays to pytorch tensors, and either stacks or concatenates\n    each value.\n\n    Type Parameters:\n        - `TTransformed`: input sample type.\n        - `TCollated`: output collated type.\n\n    Args:\n        mode: whether to `stack` or `concat` during collation.\n    \"\"\"\n\n    def __init__(self, mode: Literal[\"stack\", \"concat\"] = \"concat\") -&gt; None:\n        self.mode = mode\n        self.treelib = _get_treelib()\n\n    def __call__(self, data: Sequence[TTransformed]) -&gt; TCollated:\n        if self.mode == \"concat\":\n            return self.treelib.tree_map(\n                lambda *x: torch.concat([torch.from_numpy(s) for s in x]),\n                *data)  # type: ignore\n        else:\n            return self.treelib.tree_map(\n                lambda *x: torch.stack([torch.from_numpy(s) for s in x]),\n                *data)  # type: ignore\n</code></pre>"},{"location":"torch/#abstract_dataloader.torch.ComposedPipeline","title":"abstract_dataloader.torch.ComposedPipeline","text":"<p>               Bases: <code>Module</code>, <code>ComposedPipeline[TRaw, TRawInner, TTransformed, TCollated, TProcessedInner, TProcessed]</code></p> <p>Compose pipeline sequentially with pre and post transforms.</p> Type Parameters <ul> <li><code>TRaw</code>: initial input type.</li> <li><code>TRawInner</code>: output of the pre-composed transform, and input to the     provided <code>Pipeline</code>.</li> <li><code>TCollated</code>, <code>TProcessed</code>: intermediate values for the provided     <code>Pipeline</code>.</li> <li><code>TProcessedInner</code>: output of the transforms, and input to the     post-composed transform.</li> <li><code>TProcessed</code>: output type.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>Pipeline[TRawInner, TTransformed, TCollated, TProcessedInner]</code> <p>pipeline to compose.</p> required <code>pre</code> <code>Transform[TRaw, TRawInner] | None</code> <p>pre-transform to apply on the CPU side; skipped if <code>None</code>.</p> <code>None</code> <code>post</code> <code>Transform[TProcessedInner, TProcessed] | None</code> <p>post-transform to apply on the GPU side; skipped if <code>None</code>.</p> <code>None</code> Source code in <code>src/abstract_dataloader/torch/generic.py</code> <pre><code>class ComposedPipeline(\n    torch.nn.Module,\n    generic.ComposedPipeline[\n        TRaw, TRawInner, TTransformed, TCollated, TProcessedInner, TProcessed]\n):\n    \"\"\"Compose pipeline sequentially with pre and post transforms.\n\n    Type Parameters:\n        - `TRaw`: initial input type.\n        - `TRawInner`: output of the pre-composed transform, and input to the\n            provided [`Pipeline`][abstract_dataloader.spec].\n        - `TCollated`, `TProcessed`: intermediate values for the provided\n            [`Pipeline`][abstract_dataloader.spec].\n        - `TProcessedInner`: output of the transforms, and input to the\n            post-composed transform.\n        - `TProcessed`: output type.\n\n    Args:\n        pipeline: pipeline to compose.\n        pre: pre-transform to apply on the CPU side; skipped if `None`.\n        post: post-transform to apply on the GPU side; skipped if `None`.\n    \"\"\"\n\n    def __init__(\n        self, pipeline: spec.Pipeline[\n            TRawInner, TTransformed, TCollated, TProcessedInner],\n        pre: spec.Transform[TRaw, TRawInner] | None = None,\n        post: spec.Transform[TProcessedInner, TProcessed] | None = None\n    ) -&gt; None:\n        super().__init__()\n        self.pipeline = pipeline\n        self.pre = pre\n        self.post = post\n\n        self.collate = pipeline.collate\n</code></pre>"},{"location":"torch/#abstract_dataloader.torch.Empty","title":"abstract_dataloader.torch.Empty","text":"<p>               Bases: <code>Synchronization</code></p> <p>Dummy synchronization which does not synchronize sensor pairs.</p> <p>No samples will be registered, and the trace can only be used as a collection of sensors.</p> Source code in <code>src/abstract_dataloader/generic/sync.py</code> <pre><code>class Empty(spec.Synchronization):\n    \"\"\"Dummy synchronization which does not synchronize sensor pairs.\n\n    No samples will be registered, and the trace can only be used as a\n    collection of sensors.\n    \"\"\"\n\n    def __call__(\n        self, timestamps: dict[str, Float64[np.ndarray, \"_N\"]]\n    ) -&gt; dict[str, UInt32[np.ndarray, \"M\"]]:\n        \"\"\"Apply synchronization protocol.\n\n        Args:\n            timestamps: input sensor timestamps.\n\n        Returns:\n            Synchronized index map.\n        \"\"\"\n        return {k: np.array([], dtype=np.uint32) for k in timestamps}\n</code></pre>"},{"location":"torch/#abstract_dataloader.torch.Empty.__call__","title":"__call__","text":"<pre><code>__call__(\n    timestamps: dict[str, Float64[ndarray, _N]],\n) -&gt; dict[str, UInt32[ndarray, M]]\n</code></pre> <p>Apply synchronization protocol.</p> <p>Parameters:</p> Name Type Description Default <code>timestamps</code> <code>dict[str, Float64[ndarray, _N]]</code> <p>input sensor timestamps.</p> required <p>Returns:</p> Type Description <code>dict[str, UInt32[ndarray, M]]</code> <p>Synchronized index map.</p> Source code in <code>src/abstract_dataloader/generic/sync.py</code> <pre><code>def __call__(\n    self, timestamps: dict[str, Float64[np.ndarray, \"_N\"]]\n) -&gt; dict[str, UInt32[np.ndarray, \"M\"]]:\n    \"\"\"Apply synchronization protocol.\n\n    Args:\n        timestamps: input sensor timestamps.\n\n    Returns:\n        Synchronized index map.\n    \"\"\"\n    return {k: np.array([], dtype=np.uint32) for k in timestamps}\n</code></pre>"},{"location":"torch/#abstract_dataloader.torch.Metadata","title":"abstract_dataloader.torch.Metadata  <code>dataclass</code>","text":"<p>               Bases: <code>Metadata</code></p> <p>Generic metadata with timestamps.</p> <p>Attributes:</p> Name Type Description <code>timestamps</code> <code>Float64[ndarray, N]</code> <p>epoch timestamps.</p> Source code in <code>src/abstract_dataloader/generic/sequence.py</code> <pre><code>@dataclass\nclass Metadata(spec.Metadata):\n    \"\"\"Generic metadata with timestamps.\n\n    Attributes:\n        timestamps: epoch timestamps.\n    \"\"\"\n\n    timestamps: Float64[np.ndarray, \"N\"]\n</code></pre>"},{"location":"torch/#abstract_dataloader.torch.Nearest","title":"abstract_dataloader.torch.Nearest","text":"<p>               Bases: <code>Synchronization</code></p> <p>Nearest sample synchronization, with respect to a reference sensor.</p> <p>Applies the following:</p> <ul> <li>Compute the midpoints between observations between each sensor.</li> <li>Find which bin the reference sensor timestamps fall into.</li> <li>Calculate the resulting time delta between timestamps. If this exceeds   <code>tol</code> for any sensor-reference pair, remove this match.</li> </ul> <p>See <code>Synchronization</code> for protocol details.</p> <p>Parameters:</p> Name Type Description Default <code>reference</code> <code>str</code> <p>reference sensor to synchronize to.</p> required <code>tol</code> <code>float</code> <p>synchronization time tolerance, in seconds. Setting <code>tol = np.inf</code> works to disable this check altogether.</p> <code>0.1</code> <code>margin</code> <code>tuple[int | float, int | float]</code> <p>time margin (in seconds; <code>float</code>) or index margin (in samples; <code>int</code>) to apply to the start and end time relative to the reference sensor, excluding samples within this margin.</p> <code>(0, 0)</code> Source code in <code>src/abstract_dataloader/generic/sync.py</code> <pre><code>class Nearest(spec.Synchronization):\n    \"\"\"Nearest sample synchronization, with respect to a reference sensor.\n\n    Applies the following:\n\n    - Compute the midpoints between observations between each sensor.\n    - Find which bin the reference sensor timestamps fall into.\n    - Calculate the resulting time delta between timestamps. If this exceeds\n      `tol` for any sensor-reference pair, remove this match.\n\n    See [`Synchronization`][abstract_dataloader.spec.] for protocol details.\n\n    Args:\n        reference: reference sensor to synchronize to.\n        tol: synchronization time tolerance, in seconds. Setting `tol = np.inf`\n            works to disable this check altogether.\n        margin: time margin (in seconds; `float`) or index margin\n            (in samples; `int`) to apply to the start and end time relative to\n            the reference sensor, excluding samples within this margin.\n    \"\"\"\n\n    def __init__(\n        self, reference: str, tol: float = 0.1,\n        margin: tuple[int | float, int | float] = (0, 0)\n    ) -&gt; None:\n        if tol &lt; 0:\n            raise ValueError(\n                f\"Synchronization tolerance must be positive: {tol} &lt; 0\")\n\n        self.tol = tol\n        self.reference = reference\n        self.margin = margin\n\n    def __call__(\n        self, timestamps: dict[str, Float64[np.ndarray, \"_N\"]]\n    ) -&gt; dict[str, UInt32[np.ndarray, \"M\"]]:\n        \"\"\"Apply synchronization protocol.\n\n        Args:\n            timestamps: input sensor timestamps.\n\n        Returns:\n            Synchronized index map.\n        \"\"\"\n        try:\n            t_ref = timestamps[self.reference]\n        except KeyError:\n            raise KeyError(\n                f\"Reference sensor {self.reference} was not provided in \"\n                f\"timestamps, with keys: {list(timestamps.keys())}\")\n\n        if isinstance(self.margin[0], float):\n            t_ref = t_ref[np.argmax(t_ref &gt; t_ref[0] + self.margin[0]):]\n        elif isinstance(self.margin[0], int) and self.margin[0] &gt; 0:\n            t_ref = t_ref[self.margin[0]:]\n\n        if isinstance(self.margin[1], float):\n            t_ref = t_ref[\n                :-np.argmax((t_ref &lt; t_ref[-1] - self.margin[1])[::-1])]\n        elif isinstance(self.margin[1], int) and self.margin[1] &gt; 0:\n            t_ref = t_ref[:-self.margin[1]]\n\n        indices = {\n            k: np.searchsorted(\n                (t_sensor[:-1] + t_sensor[1:]) / 2, t_ref\n            ).astype(np.uint32)\n            for k, t_sensor in timestamps.items()}\n\n        valid = np.all(np.array([\n           np.abs(timestamps[k][i_nearest] - t_ref) &lt; self.tol\n        for k, i_nearest in indices.items()]), axis=0)\n\n        return {k: v[valid] for k, v in indices.items()}\n</code></pre>"},{"location":"torch/#abstract_dataloader.torch.Nearest.__call__","title":"__call__","text":"<pre><code>__call__(\n    timestamps: dict[str, Float64[ndarray, _N]],\n) -&gt; dict[str, UInt32[ndarray, M]]\n</code></pre> <p>Apply synchronization protocol.</p> <p>Parameters:</p> Name Type Description Default <code>timestamps</code> <code>dict[str, Float64[ndarray, _N]]</code> <p>input sensor timestamps.</p> required <p>Returns:</p> Type Description <code>dict[str, UInt32[ndarray, M]]</code> <p>Synchronized index map.</p> Source code in <code>src/abstract_dataloader/generic/sync.py</code> <pre><code>def __call__(\n    self, timestamps: dict[str, Float64[np.ndarray, \"_N\"]]\n) -&gt; dict[str, UInt32[np.ndarray, \"M\"]]:\n    \"\"\"Apply synchronization protocol.\n\n    Args:\n        timestamps: input sensor timestamps.\n\n    Returns:\n        Synchronized index map.\n    \"\"\"\n    try:\n        t_ref = timestamps[self.reference]\n    except KeyError:\n        raise KeyError(\n            f\"Reference sensor {self.reference} was not provided in \"\n            f\"timestamps, with keys: {list(timestamps.keys())}\")\n\n    if isinstance(self.margin[0], float):\n        t_ref = t_ref[np.argmax(t_ref &gt; t_ref[0] + self.margin[0]):]\n    elif isinstance(self.margin[0], int) and self.margin[0] &gt; 0:\n        t_ref = t_ref[self.margin[0]:]\n\n    if isinstance(self.margin[1], float):\n        t_ref = t_ref[\n            :-np.argmax((t_ref &lt; t_ref[-1] - self.margin[1])[::-1])]\n    elif isinstance(self.margin[1], int) and self.margin[1] &gt; 0:\n        t_ref = t_ref[:-self.margin[1]]\n\n    indices = {\n        k: np.searchsorted(\n            (t_sensor[:-1] + t_sensor[1:]) / 2, t_ref\n        ).astype(np.uint32)\n        for k, t_sensor in timestamps.items()}\n\n    valid = np.all(np.array([\n       np.abs(timestamps[k][i_nearest] - t_ref) &lt; self.tol\n    for k, i_nearest in indices.items()]), axis=0)\n\n    return {k: v[valid] for k, v in indices.items()}\n</code></pre>"},{"location":"torch/#abstract_dataloader.torch.Next","title":"abstract_dataloader.torch.Next","text":"<p>               Bases: <code>Synchronization</code></p> <p>Next sample synchronization, with respect to a reference sensor.</p> <p>Applies the following:</p> <ul> <li>Find the start time, defined by the earliest time which is observed by   all sensors, and the end time, defined by the last time which is observed   by all sensors.</li> <li>Truncate the reference sensor's timestamps to this start and end time,   and use this as the query timestamps.</li> <li>For each time in the query, find the first sample from each sensor which   is after this time.</li> </ul> <p>See <code>Synchronization</code> for protocol details.</p> <p>Parameters:</p> Name Type Description Default <code>reference</code> <code>str</code> <p>reference sensor to synchronize to.</p> required <code>margin</code> <code>tuple[int | float, int | float]</code> <p>time margin (in seconds; <code>float</code>) or index margin (in samples; <code>int</code>) to apply to the start and end, excluding samples within this margin.</p> <code>(0, 0)</code> Source code in <code>src/abstract_dataloader/generic/sync.py</code> <pre><code>class Next(spec.Synchronization):\n    \"\"\"Next sample synchronization, with respect to a reference sensor.\n\n    Applies the following:\n\n    - Find the start time, defined by the earliest time which is observed by\n      all sensors, and the end time, defined by the last time which is observed\n      by all sensors.\n    - Truncate the reference sensor's timestamps to this start and end time,\n      and use this as the query timestamps.\n    - For each time in the query, find the first sample from each sensor which\n      is after this time.\n\n    See [`Synchronization`][abstract_dataloader.spec.] for protocol details.\n\n    Args:\n        reference: reference sensor to synchronize to.\n        margin: time margin (in seconds; `float`) or index margin\n            (in samples; `int`) to apply to the start and end, excluding\n            samples within this margin.\n    \"\"\"\n\n    def __init__(\n        self, reference: str,\n        margin: tuple[int | float, int | float] = (0, 0)\n    ) -&gt; None:\n        self.reference = reference\n        self.margin = margin\n\n    def __call__(\n        self, timestamps: dict[str, Float64[np.ndarray, \"_N\"]]\n    ) -&gt; dict[str, UInt32[np.ndarray, \"M\"]]:\n        \"\"\"Apply synchronization protocol.\n\n        Args:\n            timestamps: input sensor timestamps.\n\n        Returns:\n            Synchronized index map.\n        \"\"\"\n        try:\n            ref_time_all = timestamps[self.reference]\n        except KeyError:\n            raise KeyError(\n                f\"Reference sensor {self.reference} was not provided in \"\n                f\"timestamps, with keys: {list(timestamps.keys())}\")\n\n        start_time = max(t[0] for t in timestamps.values())\n        end_time = min(t[-1] for t in timestamps.values())\n\n        if isinstance(self.margin[0], float):\n            start_time += self.margin[0]\n        if isinstance(self.margin[1], float):\n            end_time -= self.margin[1]\n\n        start_idx = np.searchsorted(ref_time_all, start_time)\n        end_idx = np.searchsorted(ref_time_all, end_time)\n\n        if isinstance(self.margin[0], int):\n            start_idx += self.margin[0]\n        if isinstance(self.margin[1], int):\n            end_idx -= self.margin[1]\n\n        ref_time = ref_time_all[start_idx:end_idx]\n        return {\n            k: np.searchsorted(v, ref_time).astype(np.uint32)\n            for k, v in timestamps.items()}\n</code></pre>"},{"location":"torch/#abstract_dataloader.torch.Next.__call__","title":"__call__","text":"<pre><code>__call__(\n    timestamps: dict[str, Float64[ndarray, _N]],\n) -&gt; dict[str, UInt32[ndarray, M]]\n</code></pre> <p>Apply synchronization protocol.</p> <p>Parameters:</p> Name Type Description Default <code>timestamps</code> <code>dict[str, Float64[ndarray, _N]]</code> <p>input sensor timestamps.</p> required <p>Returns:</p> Type Description <code>dict[str, UInt32[ndarray, M]]</code> <p>Synchronized index map.</p> Source code in <code>src/abstract_dataloader/generic/sync.py</code> <pre><code>def __call__(\n    self, timestamps: dict[str, Float64[np.ndarray, \"_N\"]]\n) -&gt; dict[str, UInt32[np.ndarray, \"M\"]]:\n    \"\"\"Apply synchronization protocol.\n\n    Args:\n        timestamps: input sensor timestamps.\n\n    Returns:\n        Synchronized index map.\n    \"\"\"\n    try:\n        ref_time_all = timestamps[self.reference]\n    except KeyError:\n        raise KeyError(\n            f\"Reference sensor {self.reference} was not provided in \"\n            f\"timestamps, with keys: {list(timestamps.keys())}\")\n\n    start_time = max(t[0] for t in timestamps.values())\n    end_time = min(t[-1] for t in timestamps.values())\n\n    if isinstance(self.margin[0], float):\n        start_time += self.margin[0]\n    if isinstance(self.margin[1], float):\n        end_time -= self.margin[1]\n\n    start_idx = np.searchsorted(ref_time_all, start_time)\n    end_idx = np.searchsorted(ref_time_all, end_time)\n\n    if isinstance(self.margin[0], int):\n        start_idx += self.margin[0]\n    if isinstance(self.margin[1], int):\n        end_idx -= self.margin[1]\n\n    ref_time = ref_time_all[start_idx:end_idx]\n    return {\n        k: np.searchsorted(v, ref_time).astype(np.uint32)\n        for k, v in timestamps.items()}\n</code></pre>"},{"location":"torch/#abstract_dataloader.torch.ParallelPipelines","title":"abstract_dataloader.torch.ParallelPipelines","text":"<p>               Bases: <code>Module</code>, <code>ParallelPipelines[PRaw, PTransformed, PCollated, PProcessed]</code></p> <p>Transform Compositions, modified for Pytorch compatibility.</p> <p>Any <code>nn.Module</code> transforms are registered to a separate <code>nn.ModuleDict</code>; the original <code>.transforms</code> attribute is maintained with references to the full pipeline.</p> <p>See <code>generic.ParallelPipelines</code> for more details about this implementation. <code>.forward</code> and <code>.__call__</code> should work as expected within pytorch.</p> Type Parameters <ul> <li><code>PRaw</code>, <code>PTransformed</code>, <code>PCollated</code>, <code>PProcessed</code>: see   <code>Pipeline</code>.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>transforms</code> <code>Pipeline</code> <p>pipelines to compose. The key indicates the subkey to apply each transform to.</p> <code>{}</code> Source code in <code>src/abstract_dataloader/torch/generic.py</code> <pre><code>class ParallelPipelines(\n    torch.nn.Module,\n    generic.ParallelPipelines[PRaw, PTransformed, PCollated, PProcessed]\n):\n    \"\"\"Transform Compositions, modified for Pytorch compatibility.\n\n    Any [`nn.Module`][torch.] transforms are registered to a separate\n    [`nn.ModuleDict`][torch.]; the original `.transforms` attribute is\n    maintained with references to the full pipeline.\n\n    See [`generic.ParallelPipelines`][abstract_dataloader.]\n    for more details about this implementation. `.forward` and `.__call__`\n    should work as expected within pytorch.\n\n    Type Parameters:\n        - `PRaw`, `PTransformed`, `PCollated`, `PProcessed`: see\n          [`Pipeline`][abstract_dataloader.spec.].\n\n    Args:\n        transforms: pipelines to compose. The key indicates the subkey to\n            apply each transform to.\n    \"\"\"\n\n    def __init__(self, **transforms: spec.Pipeline) -&gt; None:\n        super().__init__()\n        self.transforms = transforms\n        self._transforms = torch.nn.ModuleDict({\n            k: v for k, v in transforms.items()\n            if isinstance(v, torch.nn.Module)})\n\n    def forward(self, data: PCollated) -&gt; PProcessed:\n        # We have to redefine this for some reason to make torch happy.\n        # I think `nn.Module` has a generic `forward` implementation which\n        # is clobbering `ComposeTransform`.\n        return cast(\n            PProcessed,\n            {k: v.batch(data[k]) for k, v in self.transforms.items()})\n\n    def batch(self, data: PCollated) -&gt; PProcessed:\n        \"\"\"Alias `batch` to `__call__` to `forward` via `nn.Module`.\"\"\"\n        return self(data)\n</code></pre>"},{"location":"torch/#abstract_dataloader.torch.ParallelPipelines.batch","title":"batch","text":"<pre><code>batch(data: PCollated) -&gt; PProcessed\n</code></pre> <p>Alias <code>batch</code> to <code>__call__</code> to <code>forward</code> via <code>nn.Module</code>.</p> Source code in <code>src/abstract_dataloader/torch/generic.py</code> <pre><code>def batch(self, data: PCollated) -&gt; PProcessed:\n    \"\"\"Alias `batch` to `__call__` to `forward` via `nn.Module`.\"\"\"\n    return self(data)\n</code></pre>"},{"location":"torch/#abstract_dataloader.torch.ParallelTransforms","title":"abstract_dataloader.torch.ParallelTransforms","text":"<p>               Bases: <code>Module</code>, <code>Transform[PRaw, PTransformed]</code></p> <p>Compose multiple transforms, similar to <code>ParallelPipelines</code>.</p> Type Parameters <ul> <li><code>PRaw</code>, <code>PTransformed</code>, <code>Transform</code>.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>transforms</code> <code>Transform</code> <p>transforms to compose. The key indicates the subkey to apply each transform to.</p> <code>{}</code> Source code in <code>src/abstract_dataloader/torch/generic.py</code> <pre><code>class ParallelTransforms(torch.nn.Module, spec.Transform[PRaw, PTransformed]):\n    \"\"\"Compose multiple transforms, similar to [`ParallelPipelines`][^.].\n\n    Type Parameters:\n        - `PRaw`, `PTransformed`, [`Transform`][abstract_dataloader.spec.].\n\n    Args:\n        transforms: transforms to compose. The key indicates the subkey to\n            apply each transform to.\n    \"\"\"\n\n    def __init__(self, **transforms: spec.Transform) -&gt; None:\n        super().__init__()\n        self.transforms = transforms\n        self._transforms = torch.nn.ModuleDict({\n            k: v for k, v in transforms.items()\n            if isinstance(v, torch.nn.Module)})\n\n    def __call__(self, data: PRaw) -&gt; PTransformed:\n        return cast(\n            PTransformed,\n            {k: v(data[k]) for k, v in self.transforms.items()})\n</code></pre>"},{"location":"torch/#abstract_dataloader.torch.SequencePipeline","title":"abstract_dataloader.torch.SequencePipeline","text":"<p>               Bases: <code>Module</code>, <code>SequencePipeline[TRaw, TTransformed, TCollated, TProcessed]</code></p> <p>Transform which passes an additional sequence axis through.</p> <p>The given <code>Pipeline</code> is modified to accept <code>Sequence[...]</code> for each data type in its pipeline, and return a <code>list[...]</code> across the additional axis, thus \"passing through\" the axis.</p> <p>For example, suppose a sequence dataloader reads</p> <pre><code>[\n    [Raw[s=0, t=0], Raw[s=0, t=1], ... Raw[s=0, t=n]]\n    [Raw[s=1, t=0], Raw[s=1, t=1], ... Raw[s=1, t=n]]\n    ...\n    [Raw[s=b, t=0], Raw[s=b, t=1], ... Raw[s=b, t=n]\n]\n</code></pre> <p>for sequence length <code>t = 0...n</code> and batch sample <code>s = 0...b</code>. For sequence length <code>t</code>, the output of the transforms will be batched with the sequence on the outside:</p> <pre><code>[\n    Processed[s=0...b] [t=0],\n    Processed[s=0...b] [t=1],\n    ...\n    Processed[s=0...b] [t=n]\n]\n</code></pre> Type Parameters <ul> <li><code>TRaw</code>, <code>TTransformed</code>, <code>TCollated</code>, <code>TProcessed</code>: see   <code>Pipeline</code>.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>Pipeline[TRaw, TTransformed, TCollated, TProcessed]</code> <p>input pipeline.</p> required Source code in <code>src/abstract_dataloader/torch/generic.py</code> <pre><code>class SequencePipeline(\n    torch.nn.Module,\n    generic.SequencePipeline[TRaw, TTransformed, TCollated, TProcessed]\n):\n    \"\"\"Transform which passes an additional sequence axis through.\n\n    The given `Pipeline` is modified to accept `Sequence[...]` for each\n    data type in its pipeline, and return a `list[...]` across the additional\n    axis, thus \"passing through\" the axis.\n\n    For example, suppose a sequence dataloader reads\n\n    ```\n    [\n        [Raw[s=0, t=0], Raw[s=0, t=1], ... Raw[s=0, t=n]]\n        [Raw[s=1, t=0], Raw[s=1, t=1], ... Raw[s=1, t=n]]\n        ...\n        [Raw[s=b, t=0], Raw[s=b, t=1], ... Raw[s=b, t=n]\n    ]\n    ```\n\n    for sequence length `t = 0...n` and batch sample `s = 0...b`. For sequence\n    length `t`, the output of the transforms will be batched with the sequence\n    on the outside:\n\n    ```\n    [\n        Processed[s=0...b] [t=0],\n        Processed[s=0...b] [t=1],\n        ...\n        Processed[s=0...b] [t=n]\n    ]\n    ```\n\n    Type Parameters:\n        - `TRaw`, `TTransformed`, `TCollated`, `TProcessed`: see\n          [`Pipeline`][abstract_dataloader.spec.].\n\n    Args:\n        pipeline: input pipeline.\n    \"\"\"\n\n    def __init__(\n        self, pipeline: spec.Pipeline[\n            TRaw, TTransformed, TCollated, TProcessed]\n    ) -&gt; None:\n        super().__init__()\n        self.pipeline = pipeline\n</code></pre>"},{"location":"torch/#abstract_dataloader.torch.StackedSequencePipeline","title":"abstract_dataloader.torch.StackedSequencePipeline","text":"<p>               Bases: <code>Module</code>, <code>Pipeline[Sequence[TRaw], Sequence[TTransformed], TCollated, TProcessed]</code></p> <p>Modify a pipeline to act on sequences.</p> <p>Unlike the generic <code>generic.SequencePipeline</code> implementation, this class places the sequence axis directly inside each tensor, so that each data type has axes <code>(batch, sequence, ...)</code>. For the same input,</p> <pre><code>[\n    [Raw[s=0, t=0], Raw[s=0, t=1], ... Raw[s=0, t=n]]\n    [Raw[s=1, t=0], Raw[s=1, t=1], ... Raw[s=1, t=n]]\n    ...\n    [Raw[s=b, t=0], Raw[s=b, t=1], ... Raw[s=b, t=n]\n]\n</code></pre> <p>this pipeline instead yields</p> <pre><code>Processed[s=0...b] [t=0...n].\n</code></pre> <p>Info</p> <p>This class requires that all outputs of <code>.collate()</code> are pytorch tensors. Furthermore, batches must be treated as an additional leading axis by both <code>.collate</code> and <code>.forward</code>.</p> <p>Warning</p> <p>Since the output has an additional axis, it does not necessarily have the same type as the underlying transform!</p> <p>This is accomplished by appropriately reshaping the data to use the batch-vectorized underlying implementation:</p> <ul> <li><code>.sample</code>: apply the pipeline to each sample across the additional   sequence axis.</li> <li><code>.collate</code>: concatenate all sequences into a single <code>list[Raw]</code>, instead   of a <code>list[list[Raw]]</code>. Then, collate the list, and reshape back into   <code>batch sequence ...</code> order.</li> <li><code>.batch</code>: flatten the collated data back to a <code>(batch sequence) ...</code>   single leading batch axis, apply the pipeline, and reshape back.</li> </ul> Type Parameters <ul> <li><code>PRaw</code>, <code>PTransformed</code>, <code>PCollated</code>, <code>PProcessed</code>: see   <code>Pipeline</code>.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>Pipeline[TRaw, TTransformed, TCollated, TProcessed]</code> <p>pipeline to transform to accept sequences.</p> required Source code in <code>src/abstract_dataloader/torch/torch.py</code> <pre><code>class StackedSequencePipeline(\n    torch.nn.Module,\n    spec.Pipeline[\n        Sequence[TRaw], Sequence[TTransformed], TCollated, TProcessed]\n):\n    \"\"\"Modify a pipeline to act on sequences.\n\n    Unlike the generic [`generic.SequencePipeline`][abstract_dataloader.]\n    implementation, this class places the sequence axis directly inside each\n    tensor, so that each data type has axes `(batch, sequence, ...)`. For the\n    same input,\n\n    ```\n    [\n        [Raw[s=0, t=0], Raw[s=0, t=1], ... Raw[s=0, t=n]]\n        [Raw[s=1, t=0], Raw[s=1, t=1], ... Raw[s=1, t=n]]\n        ...\n        [Raw[s=b, t=0], Raw[s=b, t=1], ... Raw[s=b, t=n]\n    ]\n    ```\n\n    this pipeline instead yields\n\n    ```python\n    Processed[s=0...b] [t=0...n].\n    ```\n\n    !!! info\n\n        This class requires that all outputs of `.collate()` are pytorch\n        tensors. Furthermore, batches must be treated as an additional leading\n        axis by both `.collate` and `.forward`.\n\n    !!! warning\n\n        Since the output has an additional axis, it does not necessarily have\n        the same type as the underlying transform!\n\n    This is accomplished by appropriately reshaping the data to use the\n    batch-vectorized underlying implementation:\n\n    - `.sample`: apply the pipeline to each sample across the additional\n      sequence axis.\n    - `.collate`: concatenate all sequences into a single `list[Raw]`, instead\n      of a `list[list[Raw]]`. Then, collate the list, and reshape back into\n      `batch sequence ...` order.\n    - `.batch`: flatten the collated data back to a `(batch sequence) ...`\n      single leading batch axis, apply the pipeline, and reshape back.\n\n    Type Parameters:\n        - `PRaw`, `PTransformed`, `PCollated`, `PProcessed`: see\n          [`Pipeline`][abstract_dataloader.spec.].\n\n    Args:\n        pipeline: pipeline to transform to accept sequences.\n    \"\"\"\n\n    def __init__(\n        self, pipeline: spec.Pipeline[\n            TRaw, TTransformed, TCollated, TProcessed]\n    ) -&gt; None:\n        super().__init__()\n        self.pipeline = pipeline\n        self.treelib = _get_treelib()\n\n    def sample(self, data: Sequence[TRaw]) -&gt; list[TTransformed]:\n        return [self.pipeline.sample(x) for x in data]\n\n    def collate(self, data: Sequence[Sequence[TTransformed]]) -&gt; Any:\n        data_flat = sum((list(x) for x in data), start=[])\n        collated_flat = self.pipeline.collate(data_flat)\n        unflattened = self.treelib.tree_map(\n            lambda x: x.reshape(len(data), -1, *x.shape[1:]),\n            collated_flat)   # type: ignore\n        return unflattened\n\n    def batch(self, data: Any) -&gt; Any:\n        batch = self.treelib.tree_leaves(data)[0].shape[0]  # type: ignore\n        flattened = self.treelib.tree_map(\n            lambda x: x.reshape(-1, *x.shape[2:]), data)\n        transformed = self.pipeline.batch(cast(TCollated, flattened))\n        unflattened = self.treelib.tree_map(\n            lambda x: x.reshape(batch, -1, *x.shape[1:]),\n            transformed)  # type: ignore\n        return unflattened\n</code></pre>"},{"location":"torch/#abstract_dataloader.torch.TransformedDataset","title":"abstract_dataloader.torch.TransformedDataset","text":"<p>               Bases: <code>Dataset[TTransformed]</code>, <code>Generic[TRaw, TTransformed]</code></p> <p>Pytorch-compatible dataset with transformation applied.</p> <p>Extends <code>torch.utils.data.Dataset</code>, implementing a torch \"map-style\" dataset.</p> Type Parameters <ul> <li><code>TRaw</code>: raw data type from the dataloader.</li> <li><code>TTransformed</code>: output data type from the provided transform function.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset[TRaw]</code> <p>source dataset.</p> required <code>transform</code> <code>Transform[TRaw, TTransformed]</code> <p>transformation to apply to each sample when loading (note that <code>Transform[TRaw, TTransformed]</code> is equivalent to <code>Callable[[TRaw], TTransformed]</code>).</p> required Source code in <code>src/abstract_dataloader/torch/torch.py</code> <pre><code>class TransformedDataset(Dataset[TTransformed], Generic[TRaw, TTransformed]):\n    \"\"\"Pytorch-compatible dataset with transformation applied.\n\n    Extends [`torch.utils.data.Dataset`][?torch.utils.data.Dataset],\n    implementing a torch \"map-style\" dataset.\n\n    Type Parameters:\n        - `TRaw`: raw data type from the dataloader.\n        - `TTransformed`: output data type from the provided transform function.\n\n    Args:\n        dataset: source dataset.\n        transform: transformation to apply to each sample when loading (note\n            that `Transform[TRaw, TTransformed]` is equivalent to\n            `Callable[[TRaw], TTransformed]`).\n    \"\"\"\n\n    def __init__(\n        self, dataset: spec.Dataset[TRaw],\n        transform: spec.Transform[TRaw, TTransformed]\n    ) -&gt; None:\n        self.dataset = dataset\n        self.transform = transform\n\n    def __getitem__(self, index: int | np.integer) -&gt; TTransformed:\n        \"\"\"Map-style dataset indexing.\n\n        Args:\n            index: dataset index; passthrough to the underlying `Dataset`.\n\n        Returns:\n            Transformed sample.\n        \"\"\"\n        return self.transform(self.dataset[index])\n\n    def __len__(self) -&gt; int:\n        \"\"\"Dataset length; passthrough to the underlying `Dataset`.\"\"\"\n        return len(self.dataset)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Friendly name.\"\"\"\n        return f\"Transformed({repr(self.dataset)})\"\n</code></pre>"},{"location":"torch/#abstract_dataloader.torch.TransformedDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(index: int | integer) -&gt; TTransformed\n</code></pre> <p>Map-style dataset indexing.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int | integer</code> <p>dataset index; passthrough to the underlying <code>Dataset</code>.</p> required <p>Returns:</p> Type Description <code>TTransformed</code> <p>Transformed sample.</p> Source code in <code>src/abstract_dataloader/torch/torch.py</code> <pre><code>def __getitem__(self, index: int | np.integer) -&gt; TTransformed:\n    \"\"\"Map-style dataset indexing.\n\n    Args:\n        index: dataset index; passthrough to the underlying `Dataset`.\n\n    Returns:\n        Transformed sample.\n    \"\"\"\n    return self.transform(self.dataset[index])\n</code></pre>"},{"location":"torch/#abstract_dataloader.torch.TransformedDataset.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Dataset length; passthrough to the underlying <code>Dataset</code>.</p> Source code in <code>src/abstract_dataloader/torch/torch.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Dataset length; passthrough to the underlying `Dataset`.\"\"\"\n    return len(self.dataset)\n</code></pre>"},{"location":"torch/#abstract_dataloader.torch.TransformedDataset.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> <p>Friendly name.</p> Source code in <code>src/abstract_dataloader/torch/torch.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Friendly name.\"\"\"\n    return f\"Transformed({repr(self.dataset)})\"\n</code></pre>"},{"location":"torch/#abstract_dataloader.torch.Window","title":"abstract_dataloader.torch.Window","text":"<p>               Bases: <code>Sensor[SampleStack, Metadata]</code>, <code>Generic[SampleStack, Sample, TMetadata]</code></p> <p>Load sensor data across a time window using a sensor transform.</p> <p>Use this class as a generic transform to give time history to any sensor:</p> <pre><code>sensor =  ... # implements spec.Sensor\nwith_history = generic.Window(sensor, past=5, future=1, parallel=7)\n</code></pre> <p>In this example, 5 past samples, the current sample, and 1 future sample are loaded on every index:</p> <pre><code>with_history[i] = [\n    sensor[i], sensor[i + 1], ... sensor[i + 5], sensor[i + 6]]\n                                        ^\n                            # timestamp for synchronization\n</code></pre> Type Parameters <ul> <li><code>SampleStack</code>: a collated series of consecutive samples. Can simply be     <code>list[Sample]</code>.</li> <li><code>Sample</code>: single observation sample type.</li> <li><code>TMetadata</code>: metadata type for the underlying sensor. Note that the     <code>Window</code> wrapper doesn't actually have metadata type <code>TMetadata</code>;     this type is just passed through from the sensor which is wrapped.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>sensor</code> <code>Sensor[Sample, TMetadata]</code> <p>sensor to wrap.</p> required <code>collate_fn</code> <code>Callable[[list[Sample]], SampleStack] | None</code> <p>collate function for aggregating a list of samples; if not specified, the samples are simply returned as a list.</p> <code>None</code> <code>past</code> <code>int</code> <p>number of past samples, in addition to the current sample. Set to <code>0</code> to disable.</p> <code>0</code> <code>future</code> <code>int</code> <p>number of future samples, in addition to the current sample. Set to <code>0</code> to disable.</p> <code>0</code> <code>crop</code> <code>bool</code> <p>if <code>True</code>, crop the first <code>past</code> and last <code>future</code> samples in the reported metadata to ensure that all samples are fully valid.</p> <code>True</code> <code>parallel</code> <code>int | None</code> <p>maximum number of samples to load in parallel; if <code>None</code>, all samples are loaded sequentially.</p> <code>None</code> Source code in <code>src/abstract_dataloader/generic/sequence.py</code> <pre><code>class Window(\n    abstract.Sensor[SampleStack, Metadata],\n    Generic[SampleStack, Sample, TMetadata]\n):\n    \"\"\"Load sensor data across a time window using a sensor transform.\n\n    Use this class as a generic transform to give time history to any sensor:\n\n    ```python\n    sensor =  ... # implements spec.Sensor\n    with_history = generic.Window(sensor, past=5, future=1, parallel=7)\n    ```\n\n    In this example, 5 past samples, the current sample, and 1 future sample\n    are loaded on every index:\n\n    ```python\n    with_history[i] = [\n        sensor[i], sensor[i + 1], ... sensor[i + 5], sensor[i + 6]]\n                                            ^\n                                # timestamp for synchronization\n    ```\n\n    Type Parameters:\n        - `SampleStack`: a collated series of consecutive samples. Can simply be\n            `list[Sample]`.\n        - `Sample`: single observation sample type.\n        - `TMetadata`: metadata type for the underlying sensor. Note that the\n            `Window` wrapper doesn't actually have metadata type `TMetadata`;\n            this type is just passed through from the sensor which is wrapped.\n\n    Args:\n        sensor: sensor to wrap.\n        collate_fn: collate function for aggregating a list of samples; if not\n            specified, the samples are simply returned as a list.\n        past: number of past samples, in addition to the current sample. Set\n            to `0` to disable.\n        future: number of future samples, in addition to the current sample.\n            Set to `0` to disable.\n        crop: if `True`, crop the first `past` and last `future` samples in\n            the reported metadata to ensure that all samples are fully valid.\n        parallel: maximum number of samples to load in parallel; if `None`, all\n            samples are loaded sequentially.\n    \"\"\"\n\n    def __init__(\n        self, sensor: spec.Sensor[Sample, TMetadata],\n        collate_fn: Callable[[list[Sample]], SampleStack] | None = None,\n        past: int = 0, future: int = 0, crop: bool = True,\n        parallel: int | None = None\n    ) -&gt; None:\n        self.sensor = sensor\n        self.past = past\n        self.future = future\n        self.parallel = parallel\n\n        if collate_fn is None:\n            collate_fn = cast(\n                Callable[[list[Sample]], SampleStack], lambda x: x)\n        self.collate_fn = collate_fn\n\n        self.cropped = crop\n        if crop:\n            # hack for negative indexing\n            _future = None if future == 0 else -future\n            self.metadata = Metadata(\n                timestamps=sensor.metadata.timestamps[past:_future])\n        else:\n            self.metadata = Metadata(timestamps=sensor.metadata.timestamps)\n\n    @classmethod\n    def from_partial_sensor(\n        cls, sensor: Callable[[str], spec.Sensor[Sample, TMetadata]],\n        collate_fn: Callable[[list[Sample]], SampleStack] | None = None,\n        past: int = 0, future: int = 0, crop: bool = True,\n        parallel: int | None = None\n    ) -&gt; Callable[[str], \"Window[SampleStack, Sample, TMetadata]\"]:\n        \"\"\"Partially initialize from partially initialized sensor.\n\n        Use this to create windowed sensor constructors which can be\n        applied to different traces to construct a dataset. For example,\n        if you have a `sensor_constructor`:\n\n        ```python\n        sensor_constructor = ...\n        windowed_sensor_constructor = Window.from_partial_sensor(\n            sensor_constructor, ...)\n\n        # ... somewhere inside the dataset constructor\n        sensor_instance = windowed_sensor_constructor(path_to_trace)\n        ```\n\n        Args:\n            sensor: sensor *constructor* to wrap.\n            collate_fn: collate function for aggregating a list of samples; if\n                not specified, the samples are simply returned as a list.\n            past: number of past samples, in addition to the current sample.\n                Set to `0` to disable.\n            future: number of future samples, in addition to the current\n                sample. Set to `0` to disable.\n            crop: if `True`, crop the first `past` and last `future` samples in\n                the reported metadata to ensure that all samples are full\n                valid.\n            parallel: maximum number of samples to load in parallel; if `None`,\n                all samples are loaded sequentially.\n        \"\"\"\n        def create_wrapped_sensor(\n            path: str\n        ) -&gt; Window[SampleStack, Sample, TMetadata]:\n            return cls(\n                sensor(path), collate_fn=collate_fn, past=past,\n                future=future, crop=crop, parallel=parallel)\n\n        return create_wrapped_sensor\n\n    def __getitem__(self, index: int | np.integer) -&gt; SampleStack:\n        \"\"\"Fetch measurements from this sensor, by index.\n\n        !!! warning\n\n            Note that `past` samples are lost at the beginning, and `future`\n            samples at the end to account for the window size!\n\n            If `crop=True`, these lost samples are taken into account by the\n            `Window` wrapper; if `crop=False`, the caller must handle this.\n\n        Args:\n            index: sample index.\n\n        Returns:\n            A set of `past + 1 + future` consecutives samples. Note that there\n                is a `past` offset of indices between the wrapped `Window` and\n                the underlying sensor!\n\n        Raises:\n            IndexError: if `crop=False`, and the requested index is out of\n                bounds (i.e., in the first `past` or last `future` samples).\n        \"\"\"\n        if self.cropped:\n            window = list(range(index, index + self.past + self.future + 1))\n        else:\n            window = list(range(index - self.past, index + self.future + 1))\n\n        if window[0] &lt; 0 or window[-1] &gt;= len(self.sensor):\n            raise IndexError(\n                f\"Requested invalid index {index} for uncropped \"\n                f\"Window(past={self.past}, future={self.future}).\")\n\n        if self.parallel is not None:\n            with ThreadPool(min(len(window), self.parallel)) as p:\n                return self.collate_fn(p.map(self.sensor.__getitem__, window))\n        else:\n            return self.collate_fn(list(map(self.sensor.__getitem__, window)))\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Get friendly name (passing through to the underlying sensor).\"\"\"\n        return f\"{repr(self.sensor)} x [-{self.past}:+{self.future}]\"\n</code></pre>"},{"location":"torch/#abstract_dataloader.torch.Window.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(index: int | integer) -&gt; SampleStack\n</code></pre> <p>Fetch measurements from this sensor, by index.</p> <p>Warning</p> <p>Note that <code>past</code> samples are lost at the beginning, and <code>future</code> samples at the end to account for the window size!</p> <p>If <code>crop=True</code>, these lost samples are taken into account by the <code>Window</code> wrapper; if <code>crop=False</code>, the caller must handle this.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int | integer</code> <p>sample index.</p> required <p>Returns:</p> Type Description <code>SampleStack</code> <p>A set of <code>past + 1 + future</code> consecutives samples. Note that there is a <code>past</code> offset of indices between the wrapped <code>Window</code> and the underlying sensor!</p> <p>Raises:</p> Type Description <code>IndexError</code> <p>if <code>crop=False</code>, and the requested index is out of bounds (i.e., in the first <code>past</code> or last <code>future</code> samples).</p> Source code in <code>src/abstract_dataloader/generic/sequence.py</code> <pre><code>def __getitem__(self, index: int | np.integer) -&gt; SampleStack:\n    \"\"\"Fetch measurements from this sensor, by index.\n\n    !!! warning\n\n        Note that `past` samples are lost at the beginning, and `future`\n        samples at the end to account for the window size!\n\n        If `crop=True`, these lost samples are taken into account by the\n        `Window` wrapper; if `crop=False`, the caller must handle this.\n\n    Args:\n        index: sample index.\n\n    Returns:\n        A set of `past + 1 + future` consecutives samples. Note that there\n            is a `past` offset of indices between the wrapped `Window` and\n            the underlying sensor!\n\n    Raises:\n        IndexError: if `crop=False`, and the requested index is out of\n            bounds (i.e., in the first `past` or last `future` samples).\n    \"\"\"\n    if self.cropped:\n        window = list(range(index, index + self.past + self.future + 1))\n    else:\n        window = list(range(index - self.past, index + self.future + 1))\n\n    if window[0] &lt; 0 or window[-1] &gt;= len(self.sensor):\n        raise IndexError(\n            f\"Requested invalid index {index} for uncropped \"\n            f\"Window(past={self.past}, future={self.future}).\")\n\n    if self.parallel is not None:\n        with ThreadPool(min(len(window), self.parallel)) as p:\n            return self.collate_fn(p.map(self.sensor.__getitem__, window))\n    else:\n        return self.collate_fn(list(map(self.sensor.__getitem__, window)))\n</code></pre>"},{"location":"torch/#abstract_dataloader.torch.Window.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> <p>Get friendly name (passing through to the underlying sensor).</p> Source code in <code>src/abstract_dataloader/generic/sequence.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Get friendly name (passing through to the underlying sensor).\"\"\"\n    return f\"{repr(self.sensor)} x [-{self.past}:+{self.future}]\"\n</code></pre>"},{"location":"torch/#abstract_dataloader.torch.Window.from_partial_sensor","title":"from_partial_sensor  <code>classmethod</code>","text":"<pre><code>from_partial_sensor(\n    sensor: Callable[[str], Sensor[Sample, TMetadata]],\n    collate_fn: Callable[[list[Sample]], SampleStack] | None = None,\n    past: int = 0,\n    future: int = 0,\n    crop: bool = True,\n    parallel: int | None = None,\n) -&gt; Callable[[str], Window[SampleStack, Sample, TMetadata]]\n</code></pre> <p>Partially initialize from partially initialized sensor.</p> <p>Use this to create windowed sensor constructors which can be applied to different traces to construct a dataset. For example, if you have a <code>sensor_constructor</code>:</p> <pre><code>sensor_constructor = ...\nwindowed_sensor_constructor = Window.from_partial_sensor(\n    sensor_constructor, ...)\n\n# ... somewhere inside the dataset constructor\nsensor_instance = windowed_sensor_constructor(path_to_trace)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>sensor</code> <code>Callable[[str], Sensor[Sample, TMetadata]]</code> <p>sensor constructor to wrap.</p> required <code>collate_fn</code> <code>Callable[[list[Sample]], SampleStack] | None</code> <p>collate function for aggregating a list of samples; if not specified, the samples are simply returned as a list.</p> <code>None</code> <code>past</code> <code>int</code> <p>number of past samples, in addition to the current sample. Set to <code>0</code> to disable.</p> <code>0</code> <code>future</code> <code>int</code> <p>number of future samples, in addition to the current sample. Set to <code>0</code> to disable.</p> <code>0</code> <code>crop</code> <code>bool</code> <p>if <code>True</code>, crop the first <code>past</code> and last <code>future</code> samples in the reported metadata to ensure that all samples are full valid.</p> <code>True</code> <code>parallel</code> <code>int | None</code> <p>maximum number of samples to load in parallel; if <code>None</code>, all samples are loaded sequentially.</p> <code>None</code> Source code in <code>src/abstract_dataloader/generic/sequence.py</code> <pre><code>@classmethod\ndef from_partial_sensor(\n    cls, sensor: Callable[[str], spec.Sensor[Sample, TMetadata]],\n    collate_fn: Callable[[list[Sample]], SampleStack] | None = None,\n    past: int = 0, future: int = 0, crop: bool = True,\n    parallel: int | None = None\n) -&gt; Callable[[str], \"Window[SampleStack, Sample, TMetadata]\"]:\n    \"\"\"Partially initialize from partially initialized sensor.\n\n    Use this to create windowed sensor constructors which can be\n    applied to different traces to construct a dataset. For example,\n    if you have a `sensor_constructor`:\n\n    ```python\n    sensor_constructor = ...\n    windowed_sensor_constructor = Window.from_partial_sensor(\n        sensor_constructor, ...)\n\n    # ... somewhere inside the dataset constructor\n    sensor_instance = windowed_sensor_constructor(path_to_trace)\n    ```\n\n    Args:\n        sensor: sensor *constructor* to wrap.\n        collate_fn: collate function for aggregating a list of samples; if\n            not specified, the samples are simply returned as a list.\n        past: number of past samples, in addition to the current sample.\n            Set to `0` to disable.\n        future: number of future samples, in addition to the current\n            sample. Set to `0` to disable.\n        crop: if `True`, crop the first `past` and last `future` samples in\n            the reported metadata to ensure that all samples are full\n            valid.\n        parallel: maximum number of samples to load in parallel; if `None`,\n            all samples are loaded sequentially.\n    \"\"\"\n    def create_wrapped_sensor(\n        path: str\n    ) -&gt; Window[SampleStack, Sample, TMetadata]:\n        return cls(\n            sensor(path), collate_fn=collate_fn, past=past,\n            future=future, crop=crop, parallel=parallel)\n\n    return create_wrapped_sensor\n</code></pre>"},{"location":"transforms/","title":"Data Transforms","text":"<p>TL;DR</p> <ul> <li>A <code>Transform</code> is any data transformation.</li> <li><code>Collate</code> is a special transform which aggregates a <code>Sequence</code> of data into a batch.</li> <li>Combining a CPU-side <code>Transform</code>, a <code>Collate</code> function, and a GPU-side <code>Transform</code> yields a data <code>Pipeline</code>.</li> </ul>"},{"location":"transforms/#transforms","title":"Transforms","text":"<p>Since data processing steps can vary wildly between domains, the lowest common denominator to describe a data transformation is simply a <code>Callable[[TRaw], TTransformed]</code>: a callable which takes an input data type, and converts it to some other data type. We provide this as the suggestively-named <code>Transform</code> protocol:</p> <pre><code>class Transform(Protocol, Generic[TRaw, TTransformed]):\n    \"\"\"Sample or batch-wise transform.\"\"\"\n\n    def __call__(self, data: TRaw) -&gt; TTransformed:\n        \"\"\"Apply transform to a single sample.\"\"\"\n        ...\n</code></pre> <p>Composition Rules</p> <p><code>Transform</code>s can be sequentially composed, as long as the output type of the first is a subtype of the input of the second, e.g. <pre><code>Transform[T2, T3] (.) Transform[T1, T2] = Transform[T1, T3].\n</code></pre> This simple rule is implemented in <code>abstract.Transform</code>.</p>"},{"location":"transforms/#batching-and-collation","title":"Batching and Collation","text":"<p>While clearly universal, the \"all data processing is composed callables\" is too vague, and not really helpful for organizing data transforms. To better categorize data transforms, we turn to analyzing batched operation.</p> <p>From a code portability standpoint, we can break down all transforms based on whether they support batched operation in the inputs and/or outputs. This implies that there are four possible types of transforms: single-sample to single-sample, single-samples to batch, batch to batch, and batch to single-sample</p> <p>Question</p> <p>Are there any use cases for batch to single-sample transforms? I'm not aware of any, though perhaps there are some edge cases out there.</p> <p>Within the first three (which are commonly used) the single-sample to batch transform stands out. We define this as a narrower type compared to a generic transform, which we refer to as <code>Collate</code>, which is analogous to the <code>collate_fn</code> of a pytorch dataloader:</p> <pre><code>class Collate(Protocol, Generic[TTransformed, TCollated]):\n    \"\"\"Data collation.\"\"\"\n\n    def __call__(self, data: Sequence[TTransformed]) -&gt; TCollated:\n        \"\"\"Collate a set of samples.\"\"\"\n        ...\n</code></pre> <p>Composition Rules</p> <p><code>Collate</code> cannot be composed, since samples can only be aggregated into     a batch once. However, like all transforms, they can be composed \"in parallel\" by routing different parts of a data structure to different <code>Collate</code> implementations.</p>"},{"location":"transforms/#pipelines","title":"Pipelines","text":"<p>A typical data processing pipeline consists of a CPU-side transform, a batching function, and a GPU-side transform. We formalize this using a <code>Pipeline</code>, which collects these three components together into a generically typed container:</p> <p></p> <ul> <li><code>Pipeline.sample</code>: apply some transform to a single sample, returning another single sample. This represents most common dataloader operations, e.g. data augmentations, point cloud processing, and nominally occurs on the CPU.</li> <li><code>Pipeline.collate</code>: combine multiple samples into a \"batch\" which facilitates vectorized processing.</li> <li><code>Pipeline.batch</code>: this step operates solely on batched data; there is no sharp line where GPU preprocessing ends, and a GPU-based model begins. This captures expensive, GPU-accelerated preprocessing steps.</li> </ul> <p>Note</p> <p>Since the only distinction between sample-to-sample and batch-to-batch transforms is the definition of a sample and a batch, which are inherently domain and implementation specific, we use the same generic <code>Transform</code> type for both <code>Pipeline.sample</code> and <code>Pipeline.batch</code>, unlike <code>Pipeline.collate</code>, which accepts the distinct <code>Collate</code> type.</p> <p>Implementations may also be <code>.sample</code>/<code>.batch</code>-generic, for example using overloaded operators only, operating on pytorch <code>cpu</code> and <code>cuda</code> tensors, or having a <code>numpy</code>/<code>pytorch</code> switch. As such, we leave distinguishing <code>.sample</code> and <code>.batch</code> transforms up to the user.</p> <p>Composition-rules</p> <p>Since they contain a <code>Collate</code> step, <code>Pipeline</code>s may not be sequentially composed. Again, like all transforms, they can be composed \"in parallel\" by routing different parts of a data structure to different <code>Collate</code> implementations.</p> <p>A <code>Pipeline</code> may also be composed with additional <code>Transform</code>s before and after its <code>.sample</code> and <code>.batch</code>.</p>"},{"location":"tutorial/","title":"Using an Abstract Dataloader","text":"<p>A fully implemented Abstract Dataloader (ADL) compliant system should consist of a collection of modular components implementing sensor data reading, time synchronization, trace &amp; dataset handling, followed by data preprocessing.</p> <p></p> <p>In this tutorial, we cover how to use these components if you are given implementations for some or all of them; we split this into data loading and processing, which are connected only by a trivial interface \u2014 the output of the <code>Dataset</code> should be the input of the <code>Pipeline</code>.</p>"},{"location":"tutorial/#dataset","title":"Dataset","text":"<p>At a minimum, any ADL-compliant dataloader should include one or more <code>Sensor</code> implementations. These may be accompanied by custom <code>Synchronization</code>, <code>Trace</code>, and <code>Dataset</code> implementations.</p>"},{"location":"tutorial/#sensor","title":"<code>Sensor</code>","text":"<p>Sensors must implementing the <code>Sensor</code> specification:</p> <ul> <li>Sensors have a <code>metadata: spec.Metadata</code> attribute, which contains a <code>timestamps: Float64[np.ndarray, \"N\"]</code> attribute.</li> <li>Each sensor has a <code>__getitem__</code> which can be used to read data by index, and a <code>__len__</code>.</li> </ul> <p>Warning</p> <p>The ADL does not prescribe a standardized API for initializing a <code>Sensor</code> since this is highly implementation-specific.</p>"},{"location":"tutorial/#trace","title":"<code>Trace</code>","text":"<p>Once you have initialized a collection of sensors which all correspond to simultaneously-recorded sensor data, assemble them into a <code>Trace</code>:</p> <ul> <li><code>Trace</code> implementations supplied by an ADL-compliant dataloader may have their own initialization methods.</li> <li>A trace has a <code>__getitem__</code> which reads data from each sensor corresponding to some global index assigned by a <code>Synchronization</code> policy, and a <code>__len__</code>.</li> </ul> <p>In the case that a data loading library does not provide a custom <code>Trace</code> implementation, <code>abstract_dataloader.abstract.Trace</code> can be used instead as a simple, no-frills baseline implementation.</p> <p>Info</p> <p>A <code>Trace</code> implementation should take (and <code>abstract.Trace</code> does take) a <code>Synchronization</code> policy as an argument. While a particular ADL-compliant dataloader implementation may provide custom <code>Synchronization</code> policies, a few generic implementations are included with <code>abstract_dataloader.generic</code>:</p> Class Description <code>Empty</code> a no-op for intializing a trace without any synchronization (i.e., just as a container of sensors). <code>Nearest</code> find the nearest measurement for each sensor relative to the reference sensor's measurements. <code>Next</code> find the next measurement for each sensor relative to the reference sensor's measurements."},{"location":"tutorial/#dataset_1","title":"<code>Dataset</code>","text":"<p>Finally, once a collection of <code>Trace</code> objects are initialized, combine them into a <code>Dataset</code>.</p> <ul> <li>As with <code>Trace</code>, <code>Dataset</code> implementations supplied by an ADL-compliant dataloader may have their own initialization methods.</li> <li>Similarly, datasets have a  <code>__getitem__</code> which reads data from each sensor and a <code>__len__</code>.</li> </ul> <p>In the case that a data loading library does not provide a custom <code>Dataset</code> implementation, <code>abstract_dataloader.abstract.Dataset</code> can also be used instead.</p> <p>Tip</p> <p>If your use case does not require combining multiple traces into a dataset, you can directly use a <code>Trace</code> as a <code>Dataset</code>: the <code>Trace</code> protocol subclasses the <code>Dataset</code> protocol, in that all required interfaces are a subset of the <code>Dataset</code> interfaces.</p>"},{"location":"tutorial/#pipeline","title":"Pipeline","text":"<p>Dataloaders may or may not come with data a preprocessing <code>Pipeline</code> which you are expected to use. Since data loaders (<code>Dataset</code>) and processing pipelines (<code>Pipeline</code>) are modular and freely composable assuming they share the same data types, it's also possible that a pipeline is distributed separately from the data loader(s) which it is compatible with.</p>"},{"location":"tutorial/#use-out-of-the-box","title":"Use Out-of-the Box","text":"<p>If a library comes with a complete, ready-to-use <code>Pipeline</code>, then all that remains is to apply the pipeline:</p> <pre><code>def apply_pipeline(indices: Sequence[int], dataset: Dataset, pipeline: Pipeline):\n    raw = [dataset[i] for i in indices]\n    transformed = [pipeline.sample(x) for x in raw]\n    collated = pipeline.collate(transformed)\n    processed = pipeline.batch(collated)\n    return processed\n</code></pre> <p>Tip</p> <p>In practice, the pipeline should be integrated with the data loading and training pipeline to ensure that it is properly pipelined and parallelized!</p> <p>Assuming you are using pytorch, the following building blocks may be helpful:</p> <ul> <li>Use <code>TransformedDataset</code>, which provides a pytorch map-style dataset with a pipeline's <code>.transform</code> applied.</li> <li>Pass <code>Pipeline.collate</code> as the <code>collate_fn</code> for a pytorch <code>DataLoader</code>.</li> <li>If you are using pytorch lightning, <code>ext.lightning.ADLDataModule</code> can also handle <code>.transform</code>, <code>.collate</code>, and a number of other common data marshalling steps for you.</li> </ul>"},{"location":"tutorial/#assemble-from-components","title":"Assemble from Components","text":"<p>If you don't have a complete <code>Pipeline</code> implementation but instead have separate components, you can use <code>abstract.Pipeline</code> to assemble a <code>transform</code>: <code>spec.Transform</code>, <code>collate</code>: <code>spec.Collate</code>, and <code>batch</code>: <code>spec.Transform</code> into a single pipeline.</p> <p>Tip</p> <p>Both <code>transform</code> and <code>pipeline</code> are optional and will fall back to the identity function if not provided. Only <code>collate</code> is required.</p> <p>Info</p> <p>For pytorch users, a reference collate function is provided as <code>abstract_dataloader.torch.Collate</code>, which can handle common pytorch tensor operations and data structures.</p>"},{"location":"types/","title":"Creating a Type System","text":"<p>Recommendations</p> <ul> <li>Set up a type system with <code>@optree.dataclass.dataclass</code>es of jaxtyping-annotated arrays</li> <li>Create separate protocols if cross-compatibility is required</li> <li>Use a <code>optree.tree_map</code>-based <code>Collate</code> function</li> </ul> <p>The Abstract Dataloader is built to facilitate the usage of type systems which describe the types (container type, numerical type, array shape, ...) that are loaded and transformed by dataloaders. While jaxtyping is the de-facto standard for declaring array shapes and numerical types, multiple standard library alternatives exist for the container type, each with their own confusing bugs and strange limitations.</p> <p>Why Type Systems?</p> <p>Type errors - incorrect shapes, data types, etc - are the segmentation faults of scientific computing in python. Like returning invalid pointers, returning incorrect types usually does not cause an immediate error; instead, the error only manifests when the value is used by an incompatible function such as a matrix multiplication which expects a specific shape or a function where incorrect shapes can cause memory usage to explode due to array shape broadcasting.</p> <p>Type checking helps catch and debug these errors: by checking types at logical boundaries such as function/method calls and container initialization, these errors are contained, and can be caught close to their source.</p>"},{"location":"types/#summary","title":"Summary","text":"<code>@dataclass</code> <code>TypedDict</code> <code>NamedTuple</code> Static type checking Runtime type checking Support for generics Approximately a primitive type Protocol-like <p>Other, non-standard-library alternatives:</p> <ul> <li>Pydantic is optimized for complex structures of simple primitives (e.g. parsing complicated JSON configurations or APIs), rather than simple structures of complex primitives (e.g. collections of numpy arrays / pytorch tensors).</li> <li>Tensordict appears to be utterly broken with regards to type checking. It's not clear whether this ever will (or can) be fixed.</li> </ul> Static type checking: it just works <p>Python type checking has come a long way, and all of these containers work out-of-the box with static type checking workflows. This includes tools like VSCode's pylance extension (which uses pyright under the hood), which provides type inferences when hovering over objects in the editor.</p> Runtime type checking: still flaky after all these years <p>Runtime type checking in python still has some way to go, and the flaws of the language and its historical baggage really show here. In particular, I'm not aware of any containers which can reliably be deeply type-checked: that is, for an arbitrary nested structure of python primitives, containers, and numpy/pytorch/etc arrays, verify the type (and shape) of each leaf value.</p> Support for generics: very helpful for numpy-pytorch cross-compatibility <p>Cross-compatibility of container types between numpy and pytorch is a huge plus, since loading data as numpy arrays and converting them to pytorch tensors is such a common design pattern. <code>Generic</code>s are the gold standard for expressing this pattern.</p> Approximately a primitive type: needed to work with tree libraries <p>Tree manipulation libraries such as optree, <code>torch.utils._pytree</code>, <code>jax.tree_utils</code>, and the internal implementation used by <code>lightning.LightningModule</code> allow for operations to be recursively applied across \"PyTrees\" of various built-in container types. Notably, since a <code>NamedTuple</code> is just a fancy tuple, and a <code>TypedDict</code> is just a dict, these tree libraries all work out-of-the-box with these containers, while other containers require library-specific registration to work.</p> Protocol-like: in keeping with the spirit of the ADL <p>Just as the abstract data loader's specifications can be implemented without actually having to import the ADL, a data container can ideally be used as an interface between modules without having to agree on a common dependency. Fortunately, while container types generally don't support this, declaring a <code>Protocol</code> is an easy workaround, so this is not critical.</p>"},{"location":"types/#dataclass","title":"<code>@dataclass</code>","text":"<p>Allows deep type checking</p> <p>The type of each attribute can be checked by runtime type checkers when the <code>@dataclass</code> is instantiated.</p> <p>Fully supports generics</p> <p>Dataclasses can use generic types. For example:</p> <pre><code>@dataclass\nclass Image(Generic[TArray]):\n    image: UInt8[Batch, \"B H W 3\"]\n    t: Float[TArray, \"B\"]\n\n# Image[np.ndarray], Image[torch.Tensor], etc.\n</code></pre> <p>Requires special handling by (some) tree methods</p> <p>Dataclasses don't work with tree manipulation routines (which recursively crawl data structures) out of the box. In addition to libraries like <code>optree</code> or <code>torch.utils._pytree</code>, this includes <code>default_collate</code> in pytorch.</p> <p>Interestingly, pytorch lightning has silently patched this on their end - probably due to the increasing popularity of dataclasses.</p> <p>Requires a separate protocol class</p> <p>Dataclasses are not \"protocol-like\", and two separately defined (but equivalent) dataclasses are not interchangeable. To support this, a separate protocol class needs to be defined.</p> <p>To define a generic dataclass container, which can contain pytorch or numpy types:</p> CodeImports <pre><code>TArray = TypeVar(\"TArray\", bound=torch.Tensor | np.ndarray)\n\n@dataclass\nclass Image(Generic[TArray]):\n    image: UInt8[TArray, \"B H W 3\"]\n    t: Float[TArray, \"B\"]\n</code></pre> <pre><code># For optree compatibility, use `from optree.dataclasses import dataclass`\nfrom dataclasses import dataclass\nfrom typing import TypeVar\nimport torch\nimport numpy as np\nfrom jaxtyping import UInt8, Float\n</code></pre> <p>We can now declare loaders and transforms that interact with this type:</p> LoadersTransforms <pre><code>class ImageSensor(spec.Sensor[Image[np.ndarray]], Metadata):\n    def __getitem__(idx: int | np.integer) -&gt; Image[np.ndarray]:\n        ...\n</code></pre> <pre><code>takes_any_image(data: Image[TArray]) -&gt; Image[TArray]: ...\ntakes_numpy_image(data: Image[np.ndarray]) -&gt; Image[np.ndarray]: ...\nconverts_numpy_to_torch(data: Image[np.ndarray]) -&gt; Image[torch.Tensor]: ...\n</code></pre> <p>We can also extend the type to add additional attributes:</p> <pre><code>class DepthImage(Image[TArray]):\n    depth: Float[TArray, \"B H W\"]\n\ntakes_any_image(DepthImage(image=..., t=..., depth=...))  # Works\n</code></pre> <p>Finally, in order to allow cross-compatibility between projects without having to share a common root class, we can instead declare a common protocol:</p> ProtocolRedefined Dataclass <pre><code>from typing import Protocol\n\n@runtime_checkable\nclass IsImage(Protocol, Generic[TArray]):\n    image: UInt8[TArray, \"B H W 3\"]\n    t: Float[TArray, \"B\"]\n</code></pre> <pre><code>class ImageRedefined(Generic[TArray]):\n    image: UInt8[TArray, \"B H W 3\"]\n    t: Float[TArray, \"B\"]\n\nisinstance(ImageRedefined(image=..., t=...), IsImage)  # True\nisinstance(Image(image=..., t=...), IsImage)  # True\n</code></pre> <p>Runtime checking of protocol types may yield false positives</p> <p>Runtime <code>isinstance</code> checks on <code>runtime_checkable</code> protocols only check that the object has all of the properties that are specified by the protocol; however, it does not verify the types of these properties. This is termed an \"unsafe overlap,\" and the python Protocol specification states that <code>isinstance</code> checks in type checkers should always fail in this case.</p> <p>Since the built-in <code>isinstance</code> does not follow this behavior, runtime type checkers (which all rely on <code>isinstance</code>) all appear to systematically ignore this.</p> <p>Patching <code>default_collate</code> in pytorch dataloaders</p> <p>There is currently no way to add custom node support to <code>torch.utils._pytree</code>, which is used by <code>default_collate</code>. Instead, we must provide a custom collate function with a supported pytree library, such as optree.</p> <ul> <li> <p>If dataclasses are registered with the optree root namespace, then the <code>torch.Collate</code> implementation which we provide is sufficient, as long as <code>optree</code> is installed.     <pre><code>from optree.dataclasses import dataclass as optree_dataclass\nfrom optree.registry import __GLOBAL_NAMESPACE\n\ndataclass = partial(optree_dataclass, namespace=__GLOBAL_NAMESPACE)\n</code></pre></p> </li> <li> <p>If dataclasses are registered with a named optree namespace, then a custom collate function should be provided which uses that namespace:     <pre><code>def collate_fn(data: Sequence[TRaw]) -&gt; TCollated:\n    # Use the namespace provided to `optree.dataclasses.dataclass`\n    return optree.tree_map(\n        lambda *x: torch.stack(x), *data, namespace=\"data\")\n</code></pre></p> </li> </ul> Full example code Without Tree SupportOptree (Root Namespace)Optree (User Namespace) <pre><code>from dataclasses import dataclass\nfrom typing import Protocol, TypeVar\n\nimport torch\nimport numpy as np\nfrom jaxtyping import UInt8, Float\n\nTArray = TypeVar(\"TArray\", bound=torch.Tensor | np.ndarray)\n\n@dataclass\nclass Image(Generic[TArray]):\n    image: UInt8[TArray, \"B H W 3\"]\n    t: Float[TArray, \"B\"]\n\n@dataclass\nclass DepthImage(Image[TArray]):\n    depth: Float[TArray, \"B H W\"]\n\n@runtime_checkable\nclass IsImage(Protocol, Generic[TArray]):\n    image: UInt8[TArray, \"B H W 3\"]\n    t: Float[TArray, \"B\"]\n</code></pre> <pre><code>from functools import partial\nfrom typing import Protocol, TypeVar\n\nfrom optree.dataclasses import dataclass as optree_dataclass\nfrom optree.registry import __GLOBAL_NAMESPACE\nimport torch\nimport numpy as np\nfrom jaxtyping import UInt8, Float\n\ndataclass = partial(optree_dataclass, namespace=__GLOBAL_NAMESPACE)\n\nTArray = TypeVar(\"TArray\", bound=torch.Tensor | np.ndarray)\n\n@dataclass\nclass Image(Generic[TArray]):\n    image: UInt8[TArray, \"B H W 3\"]\n    t: Float[TArray, \"B\"]\n\n@dataclass\nclass DepthImage(Image[TArray]):\n    depth: Float[TArray, \"B H W\"]\n\n@runtime_checkable\nclass IsImage(Protocol, Generic[TArray]):\n    image: UInt8[TArray, \"B H W 3\"]\n    t: Float[TArray, \"B\"]\n</code></pre> <pre><code>from optree.dataclasses import dataclass\nfrom typing import Protocol, TypeVar\n\nimport torch\nimport numpy as np\nfrom jaxtyping import UInt8, Float\n\nTArray = TypeVar(\"TArray\", bound=torch.Tensor | np.ndarray)\n\n@dataclass(namespace=\"data\")\nclass Image(Generic[TArray]):\n    image: UInt8[TArray, \"B H W 3\"]\n    t: Float[TArray, \"B\"]\n\n@dataclass(namespace=\"data\")\nclass DepthImage(Image[TArray]):\n    depth: Float[TArray, \"B H W\"]\n\n@runtime_checkable\nclass IsImage(Protocol, Generic[TArray]):\n    image: UInt8[TArray, \"B H W 3\"]\n    t: Float[TArray, \"B\"]\n</code></pre>"},{"location":"types/#typeddict","title":"<code>TypedDict</code>","text":"<p>Works with tree libraries out of the box</p> <p>Since <code>TypedDict</code> are just dictionaries, they work with tree manipulation routines out of the box.</p> <p>Natively Protocol-like</p> <p><code>TypedDict</code> are just annotations, and behave like protocols: separately defined <code>TypedDict</code> with identical specifications can be used interchangeably. This removes the need to define a separate container type and protocol type.</p> <p> Fundamentally broken and not runtime type checkable </p> <p>While the <code>TypedDict</code> spec provides type checking on paper, <code>isinstance</code> checks are forbidden, which in practice makes runtime type checking of <code>TypedDict</code>s impossible, since all runtime type-checkers rely on <code>isinstance</code>. This is a problem, since the entire point of typed data containers is to facilitate runtime type checking of array shapes!</p> <p>Generic <code>TypedDict</code>s are also supported in the spec; however, due to forbidding <code>isinstance</code> checks, they cause even more problems for runtime type checkers.</p> <p>In practice, this means runtime type checkers like beartype fall back to using <code>Mapping[str, Any]</code> or even just <code>dict</code> when they encounter a <code>TypedDict</code> (and completely explode when they encounter a generic <code>TypedDict</code>). Unfortunately, since the problems with <code>TypedDict</code> originate from fundamental design choices in python's type system, it's unclear if this will ever be fixed -- or if this even can be fixed.</p> <p>Defining a <code>TypedDict</code>-based container which supports both numpy arrays and pytorch tensors requires defining separate classes:</p> <pre><code>from typing import TypedDict\n\nimport torch\nimport numpy as np\nfrom jaxtyping import UInt8, Float\n\nclass ImageTorch(TypedDict):\n    image: UInt8[torch.Tensor, \"B H W 3\"]\n    t: Float[torch.Tensor, \"B\"]\n\nclass ImageNP(TypedDict):\n    image: UInt8[np.ndarray, \"B H W 3\"]\n    t: Float[np.ndarray, \"B\"]\n</code></pre> <p>... and that's it. Everything that can work will work out of the box, but no amount of workarounds will ever make what doesn't work, work.</p>"},{"location":"types/#namedtuple","title":"<code>NamedTuple</code>","text":"<p>Allows deep type checking</p> <p>The type of each attribute can be checked by runtime type checkers when the <code>NamedTuple</code> is instantiated.</p> <p>Works with tree libraries out of the box</p> <p>Since <code>TypedDict</code> are just dictionaries, they work with tree manipulation routines out of the box.</p> <p>Buggy Generics</p> <p>While not as much of a disaster as <code>TypedDict</code>, the inheritance rules for <code>NamedTuple</code> also make it tricky for runtime type checkers to properly support generics.</p> <p>Requires a separate protocol class</p> <p>Dataclasses are not \"protocol-like\", and two separately defined (but equivalent) dataclasses are not interchangeable. To support this, a separate protocol class needs to be defined.</p> <p>Like <code>TypedDict</code>, we need to define separate containers which supports both numpy arrays and pytorch tensors:</p> <pre><code>from typing import NamedTuple\n\nimport torch\nimport numpy as np\nfrom jaxtyping import UInt8, Float\n\nclass ImageTorch(NamedTuple):\n    image: UInt8[torch.Tensor, \"B H W 3\"]\n    t: Float[torch.Tensor, \"B\"]\n\nclass ImageNP(NamedTuple):\n    image: UInt8[np.ndarray, \"B H W 3\"]\n    t: Float[np.ndarray, \"B\"]\n</code></pre> <p>Annoyingly, we now have to also define a matching set of <code>Protocol</code>s for both versions:</p> <pre><code>from typing import Protocol, runtime_checkable\n\n@runtime_checkable\nclass IsImageTorch(Protocol):\n    image: UInt8[torch.Tensor, \"B H W 3\"]\n    t: Float[torch.Tensor, \"B\"]\n\n@runtime_checkable\nclass IsImageNP(Protocol):\n    image: UInt8[np.ndarray, \"B H W 3\"]\n    t: Float[np.ndarray, \"B\"]\n</code></pre>"},{"location":"verification/","title":"Verification using Type Checking","text":"<p>Recommendations</p> <ul> <li>Set up a type system with jaxtyping array annotations, and provide type parameters accordingly.</li> <li>Use pyright for static type checking (just install the vscode Python extension).</li> <li>Use beartype for runtime checking - don't forget to use the jaxtyping hooks!</li> </ul>"},{"location":"verification/#type-annotations-in-the-adl","title":"Type Annotations in the ADL","text":"<p>The abstract dataloader specification makes heavy use of generic types to allow type checkers to work even when indirected by various transformations. For example, consider the definition of <code>Sensor</code>:</p> <pre><code># Definition simplified for readability\nclass Sensor(Protocol, Generic[TSample, TMetadata]):\n\n    metadata: TMetadata\n\n    def __getitem__(self, index: int | np.integer) -&gt; TSample:\n        ...\n</code></pre> <ul> <li><code>TSample</code> and <code>TMetadata</code> declare the types of the metadata and data sample type.</li> <li>Implementations which provide a <code>TSample</code> and/or <code>TMetadata</code> allow type checkers to infer that <code>.metadata</code> should refer to a <code>TMetadata</code> object, and indexing via <code>__getitem__</code> should return a <code>TSample</code> type.</li> </ul> <p>Note</p> <p>In this particular example, <code>TMetadata</code> is expected to implement <code>Metadata</code>: <pre><code>TMetadata = TypeVar(\"TMetadata\", bound=Metadata)\n</code></pre> where <code>Metadata</code> is a protocol which requires a <code>.timestamps</code> attribute.</p> <p>Now, suppose we are declaring an interface which takes any <code>Sensor</code> which loads a (statically and dynamically checkable) <code>CameraFrame</code> type and is associated with <code>VideoMetadata</code>. We can define this interface as such:</p> <pre><code>read_frame_with_cond(\n    video: spec.Sensor[CameraFrame, VideoMetadata], cond: dict\n) -&gt; CameraFrame:\n    # `video` must have `metadata`, since `video` is a `Sensor`\n    # `video.metadata` must be a `VideoMetadata` due to the type parameter\n    metadata: VideoMetadata = video.metadata\n    idx: int = some_specific_way_to_search_the_metadata(metadata, cond)\n    # `video[idx]` must be a `CameraFrame` due to the type parameter\n    return video[idx]\n</code></pre>"},{"location":"verification/#static-type-checking","title":"Static Type Checking","text":"<p>Static type checking via tools such as pyright, which is included with the VSCode python plugin, provides a first line of defense against violating the ADL specification, either by incorrectly using compliant components, or declaring a ADL-compatible component that isn't actually compliant.</p> <p>Mypy does not fully support PEP 695</p> <p>The spec currently makes heavy use of backported (via <code>typing_extensions</code>) PEP 695 <code>TypeVar(..., infer_variance=True)</code> syntax. Unfortunately, Mypy does not support this yet, so Mypy will always raise type errors.</p> <p>Static type checking cannot verify array shapes or data types</p> <p>Since array shapes and types are determined at runtime, static type checkers cannot verify jaxtyping array annotations.</p>"},{"location":"verification/#runtime-type-checking","title":"Runtime Type Checking","text":"<p>The protocols declared in the <code>spec</code> are <code>runtime_checkable</code>, and can be <code>isinstance</code> or <code>issubclass</code> checked at runtime. As a last line of defense, this allows type checking by runtime type checkers such as beartype, which can also deeply check the data types used as inputs and returns by the various MDL components if they have a well-defined type system.</p> <p>Notably, since python is fundamentally a dynamically typed language, this enables checking of many types (or aspects of types) which cannot be checked statically. This is especially true for data with runtime-dynamic shapes.</p> <p>Runtime type checking does not deeply verify protocols</p> <p>Runtime type checkers are based on <code>isinstance</code> and <code>issubclass</code> calls; however, these builtin functions verify protocols only by checking the presence of declared methods and parameters. In typing jargon, this means that objects can only be checked against the \"type-erased\" versions of protocols at runtime.</p> <p>In practice, this means that it is still possible for false positives: objects which appear to match a protocol when checked (e.g., at initialization), but cause something to explode later (e.g., when an incompatible method is called).</p>"},{"location":"extensions/","title":"About","text":""},{"location":"extensions/#abstract_dataloader.ext","title":"abstract_dataloader.ext","text":"<p>ADL specification extensions and non-generic utilities.</p> <p>Unlike <code>abstract_dataloader.generic</code>, these implementations \"extend\" the ADL spec by imposing a particular conceptual framework on various functionality.</p> <p>Warning</p> <p>This module and its submodules are not automatically imported; you will need to explicitly import them:</p> <pre><code>from abstract_dataloader.ext import sample\n</code></pre> <p>Info</p> <p>This module is not included in the test suite or CI, and is generally held to a lower standard than the core <code>abstract_dataloader</code>.</p> <p>The current extension modules are:</p> <ul> <li><code>augment</code>: A protocol for specifying data augmentations.</li> <li><code>graph</code>: A programming model based on composing a DAG of callables     into a single transform.</li> <li><code>lightning</code>: A lightning datamodule wrapper for ADL datasets and     pipelines.</li> <li><code>objective</code>: Standardized learning objectives, and a     programming model for multi-objective learning.</li> <li><code>sample</code>: Dataset sampling utilities, including a low-discrepancy     subset sampler.</li> <li><code>types</code>: Type-related utilities which are not part of the core ADL spec.</li> </ul>"},{"location":"extensions/augment/","title":"Augment","text":""},{"location":"extensions/augment/#abstract_dataloader.ext.augment","title":"abstract_dataloader.ext.augment","text":"<p>Abstract specifications for data augmentations.</p> <p>Programming Model</p> <ul> <li>Data augmentations consist of a set of properties which correspond to     common properties of the data, such as <code>azimuth_flip</code> or <code>range_scale</code>.</li> <li>The user is responsible for defining and keeping track of each     augmentation property and its meaning.</li> <li>Multiple data augmentation specifications can be combined into a single     set of <code>Augmentations</code>, which generates a dictionary of values.</li> </ul> <p>In addition to the general framework, we include wrappers for a few common distributions:</p> <ul> <li><code>Bernoulli</code>: <code>Bernoulli(p)</code></li> <li><code>Normal</code>: <code>Normal(0, std) * Bernoulli(p)</code></li> <li><code>TruncatedLogNormal</code>:     <code>exp(clamp(Normal(0, std), -clip, clip)) * Bernoulli(p))</code></li> <li><code>Uniform</code>: <code>Unif(lower, upper) * Bernoulli(p)</code></li> </ul>"},{"location":"extensions/augment/#abstract_dataloader.ext.augment.Augmentation","title":"abstract_dataloader.ext.augment.Augmentation","text":"<p>               Bases: <code>Protocol</code>, <code>Generic[T]</code></p> <p>A generic augmentation random generation policy.</p> Source code in <code>src/abstract_dataloader/ext/augment.py</code> <pre><code>class Augmentation(Protocol, Generic[T]):\n    \"\"\"A generic augmentation random generation policy.\"\"\"\n\n    def __call__(self, rng: np.random.Generator) -&gt; T:\n        \"\"\"Sample the value of a data augmentation parameter.\"\"\"\n        ...\n</code></pre>"},{"location":"extensions/augment/#abstract_dataloader.ext.augment.Augmentation.__call__","title":"__call__","text":"<pre><code>__call__(rng: Generator) -&gt; T\n</code></pre> <p>Sample the value of a data augmentation parameter.</p> Source code in <code>src/abstract_dataloader/ext/augment.py</code> <pre><code>def __call__(self, rng: np.random.Generator) -&gt; T:\n    \"\"\"Sample the value of a data augmentation parameter.\"\"\"\n    ...\n</code></pre>"},{"location":"extensions/augment/#abstract_dataloader.ext.augment.Augmentations","title":"abstract_dataloader.ext.augment.Augmentations","text":"<p>A collection of data augmentations.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>Augmentation</code> <p>augmentations to apply, where each key is the name of a physical property (e.g., <code>azimuth_flip</code>, <code>range_scale</code>) and each value is the corresponding augmentation generator.</p> <code>{}</code> Source code in <code>src/abstract_dataloader/ext/augment.py</code> <pre><code>class Augmentations:\n    \"\"\"A collection of data augmentations.\n\n    Args:\n        kwargs: augmentations to apply, where each key is the\n            name of a physical property (e.g., `azimuth_flip`, `range_scale`)\n            and each value is the corresponding augmentation generator.\n    \"\"\"\n\n    def __init__(self, **kwargs: Augmentation) -&gt; None:\n        self.augmentations = kwargs\n        self._rng = {}\n\n    @property\n    def rng(self) -&gt; np.random.Generator:\n        \"\"\"Random number generator, using the PID as a seed.\"\"\"\n        pid = os.getpid()\n        if pid in self._rng:\n            return self._rng[pid]\n        else:\n            rng = np.random.default_rng(seed=pid)\n            self._rng[pid] = rng\n            return rng\n\n    def __call__(self, meta: dict[str, Any] = {}) -&gt; dict[str, Any]:\n        \"\"\"Generate a dictionary of augmentations.\n\n        If a `train=False` flag is passed in `meta`, no augmentations are\n        generated.\n\n        Args:\n            meta: data processing configuration inputs.\n\n        Returns:\n            Augmentation specifications.\n        \"\"\"\n        if meta.get(\"train\", True):\n            return {k: v(self.rng) for k, v in self.augmentations.items()}\n        else:\n            return {}\n</code></pre>"},{"location":"extensions/augment/#abstract_dataloader.ext.augment.Augmentations.rng","title":"rng  <code>property</code>","text":"<pre><code>rng: Generator\n</code></pre> <p>Random number generator, using the PID as a seed.</p>"},{"location":"extensions/augment/#abstract_dataloader.ext.augment.Augmentations.__call__","title":"__call__","text":"<pre><code>__call__(meta: dict[str, Any] = {}) -&gt; dict[str, Any]\n</code></pre> <p>Generate a dictionary of augmentations.</p> <p>If a <code>train=False</code> flag is passed in <code>meta</code>, no augmentations are generated.</p> <p>Parameters:</p> Name Type Description Default <code>meta</code> <code>dict[str, Any]</code> <p>data processing configuration inputs.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Augmentation specifications.</p> Source code in <code>src/abstract_dataloader/ext/augment.py</code> <pre><code>def __call__(self, meta: dict[str, Any] = {}) -&gt; dict[str, Any]:\n    \"\"\"Generate a dictionary of augmentations.\n\n    If a `train=False` flag is passed in `meta`, no augmentations are\n    generated.\n\n    Args:\n        meta: data processing configuration inputs.\n\n    Returns:\n        Augmentation specifications.\n    \"\"\"\n    if meta.get(\"train\", True):\n        return {k: v(self.rng) for k, v in self.augmentations.items()}\n    else:\n        return {}\n</code></pre>"},{"location":"extensions/augment/#abstract_dataloader.ext.augment.Bernoulli","title":"abstract_dataloader.ext.augment.Bernoulli","text":"<p>               Bases: <code>Augmentation[bool]</code></p> <p>Enable augmentation with certain probability.</p> <p>Type: <code>bool</code> (<code>True</code> if enabled).</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>float</code> <p>probability of enabling.</p> <code>0.5</code> Source code in <code>src/abstract_dataloader/ext/augment.py</code> <pre><code>class Bernoulli(Augmentation[bool]):\n    \"\"\"Enable augmentation with certain probability.\n\n    Type: `bool` (`True` if enabled).\n\n    Args:\n        p: probability of enabling.\n    \"\"\"\n\n    def __init__(self, p: float = 0.5) -&gt; None:\n        self.p = p\n\n    def __call__(self, rng: np.random.Generator) -&gt; bool:\n        return rng.random() &lt; self.p\n</code></pre>"},{"location":"extensions/augment/#abstract_dataloader.ext.augment.Normal","title":"abstract_dataloader.ext.augment.Normal","text":"<p>               Bases: <code>Augmentation[float]</code></p> <p>Normal distribution.</p> <p>Type: <code>float</code>; returns <code>0.0</code> if not enabled. Always zero-centered.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>float</code> <p>probability of enabling this augmentation (<code>True</code>).</p> <code>1.0</code> <code>std</code> <code>float</code> <p>standard deviation of the normal distribution.</p> <code>1.0</code> Source code in <code>src/abstract_dataloader/ext/augment.py</code> <pre><code>class Normal(Augmentation[float]):\n    \"\"\"Normal distribution.\n\n    Type: `float`; returns `0.0` if not enabled. Always zero-centered.\n\n    Args:\n        p: probability of enabling this augmentation (`True`).\n        std: standard deviation of the normal distribution.\n    \"\"\"\n\n    def __init__(self, p: float = 1.0, std: float = 1.0) -&gt; None:\n        self.p = p\n        self.std = std\n\n    def __call__(self, rng: np.random.Generator) -&gt; float:\n        if self.p &lt; 1.0 and rng.random() &gt; self.p:\n            return 0.0\n\n        return rng.normal(scale=self.std)\n</code></pre>"},{"location":"extensions/augment/#abstract_dataloader.ext.augment.TruncatedLogNormal","title":"abstract_dataloader.ext.augment.TruncatedLogNormal","text":"<p>               Bases: <code>Augmentation[float]</code></p> <p>Truncated log-normal distribution.</p> <p>The underlying normal is always centered around zero.</p> <p>Type: <code>float</code>; returns <code>1.0</code> if not enabled.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>float</code> <p>probability of enabling this augmentation (<code>True</code>).</p> <code>1.0</code> <code>std</code> <code>float</code> <p>standard deviation of the underlying normal distribution.</p> <code>0.2</code> <code>clip</code> <code>float</code> <p>clip to this many standard deviations; don't clip if 0.</p> <code>2.0</code> Source code in <code>src/abstract_dataloader/ext/augment.py</code> <pre><code>class TruncatedLogNormal(Augmentation[float]):\n    \"\"\"Truncated log-normal distribution.\n\n    The underlying normal is always centered around zero.\n\n    Type: `float`; returns `1.0` if not enabled.\n\n    Args:\n        p: probability of enabling this augmentation (`True`).\n        std: standard deviation of the underlying normal distribution.\n        clip: clip to this many standard deviations; don't clip if 0.\n    \"\"\"\n\n    def __init__(\n        self, p: float = 1.0, std: float = 0.2, clip: float = 2.0\n    ) -&gt; None:\n        self.p = p\n        self.std = std\n        self.clip = clip\n\n    def __call__(self, rng: np.random.Generator) -&gt; float:\n        if self.p &lt; 1.0 and rng.random() &gt; self.p:\n            return 1.0\n\n        z = rng.normal()\n        if self.clip &gt; 0:\n            z = np.clip(z, -self.clip, self.clip)\n        return np.exp(z * self.std)\n</code></pre>"},{"location":"extensions/augment/#abstract_dataloader.ext.augment.Uniform","title":"abstract_dataloader.ext.augment.Uniform","text":"<p>               Bases: <code>Augmentation[float]</code></p> <p>Uniform distribution.</p> <p>Type: <code>float</code>; returns <code>0.0</code> if not enabled.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>float</code> <p>probability of enabling this augmentation.</p> <code>1.0</code> <code>lower</code> <code>float</code> <p>lower bound.</p> <code>-pi</code> <code>upper</code> <code>float</code> <p>upper bound.</p> <code>pi</code> Source code in <code>src/abstract_dataloader/ext/augment.py</code> <pre><code>class Uniform(Augmentation[float]):\n    \"\"\"Uniform distribution.\n\n    Type: `float`; returns `0.0` if not enabled.\n\n    Args:\n        p: probability of enabling this augmentation.\n        lower: lower bound.\n        upper: upper bound.\n    \"\"\"\n\n    def __init__(\n        self, p: float = 1.0, lower: float = -np.pi, upper: float = np.pi\n    ) -&gt; None:\n        super().__init__()\n        self.p = p\n        self.lower = lower\n        self.upper = upper\n\n    def __call__(self, rng: np.random.Generator) -&gt; float:\n        if self.p &lt; 1.0 and rng.random() &gt; self.p:\n            return 0.0\n\n        return rng.uniform(self.lower, self.upper)\n</code></pre>"},{"location":"extensions/graph/","title":"Graph","text":""},{"location":"extensions/graph/#abstract_dataloader.ext.graph","title":"abstract_dataloader.ext.graph","text":"<p>Composing transforms for data processing pipelines.</p> <p>Programming Model</p> <ul> <li>Data is represented as a dictionary with string keys and arbitrary     values which are atomic from the perspective of transform composition.</li> <li>Transforms are created from a directed acyclic graph (DAG) of nodes,     where each node (<code>Node</code>) is a callable which takes a set     of inputs and produces a set of outputs.</li> </ul>"},{"location":"extensions/graph/#abstract_dataloader.ext.graph.Node","title":"abstract_dataloader.ext.graph.Node  <code>dataclass</code>","text":"<p>Node specification for a graph-based data processing transform.</p> <p>Example Hydra Config</p> <pre><code>rsp:\n  inputs:\n    iq: radar\n    aug: aug\n  output: spectrum\n  transform:\n    _target_: grt.transforms.Spectrum\n    ...\n</code></pre> <p>Attributes:</p> Name Type Description <code>transform</code> <code>Callable</code> <p>callable to apply to the inputs.</p> <code>output</code> <code>str | Sequence[str]</code> <p>output data key (or output data keys for a node which returns multiple outputs).</p> <code>inputs</code> <code>Mapping[str, str]</code> <p>mapping of data keys to input argument names.</p> <code>optional</code> <code>Mapping[str, str]</code> <p>mapping of optional data keys to input argument names (i.e., they are only passed if present).</p> Source code in <code>src/abstract_dataloader/ext/graph.py</code> <pre><code>@dataclass\nclass Node:\n    \"\"\"Node specification for a graph-based data processing transform.\n\n    !!! example \"Example Hydra Config\"\n\n        ```yaml\n        rsp:\n          inputs:\n            iq: radar\n            aug: aug\n          output: spectrum\n          transform:\n            _target_: grt.transforms.Spectrum\n            ...\n        ```\n\n    Attributes:\n        transform: callable to apply to the inputs.\n        output: output data key (or output data keys for a node which returns\n            multiple outputs).\n        inputs: mapping of data keys to input argument names.\n        optional: mapping of optional data keys to input argument names (i.e.,\n            they are only passed if present).\n    \"\"\"\n\n    transform: Callable\n    output: str | Sequence[str]\n    inputs: Mapping[str, str] = field(default_factory=dict)\n    optional: Mapping[str, str] = field(default_factory=dict)\n\n    def apply(self, data: dict[str, Any], name: str = \"\") -&gt; dict[str, Any]:\n        \"\"\"Apply the node.\n\n        Args:\n            data: input data to process.\n            name: node name (for error messages).\n\n        Returns:\n            Updated data, with any new keys added to the input data.\n        \"\"\"\n        inputs = {k: data[v] for k, v in self.inputs.items()}\n        for k, v in self.optional.items():\n            if v in data:\n                inputs[k] = data[v]\n\n        output = self.transform(**inputs)\n\n        if isinstance(self.output, str):\n            data[self.output] = output\n        else:  # Sequence[str]\n            if not isinstance(output, Sequence):\n                raise TypeError(\n                    f\"Node '{name}' output is expected to be a sequence due \"\n                    f\"to output specification {self.output}: \"\n                    f\"\\n{wl.pformat(output)}\\n\")\n            if len(self.output) != len(output):\n                raise ValueError(\n                    f\"Node '{name}' output length mismatch: expected \"\n                    f\"{len(self.output)} outputs ({self.output}), but got \"\n                    f\"{len(output)} outputs:\\n{wl.pformat(output)}\\n\")\n\n            for o, v in zip(self.output, output):\n                data[o] = v\n\n        return data\n</code></pre>"},{"location":"extensions/graph/#abstract_dataloader.ext.graph.Node.apply","title":"apply","text":"<pre><code>apply(data: dict[str, Any], name: str = '') -&gt; dict[str, Any]\n</code></pre> <p>Apply the node.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>input data to process.</p> required <code>name</code> <code>str</code> <p>node name (for error messages).</p> <code>''</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Updated data, with any new keys added to the input data.</p> Source code in <code>src/abstract_dataloader/ext/graph.py</code> <pre><code>def apply(self, data: dict[str, Any], name: str = \"\") -&gt; dict[str, Any]:\n    \"\"\"Apply the node.\n\n    Args:\n        data: input data to process.\n        name: node name (for error messages).\n\n    Returns:\n        Updated data, with any new keys added to the input data.\n    \"\"\"\n    inputs = {k: data[v] for k, v in self.inputs.items()}\n    for k, v in self.optional.items():\n        if v in data:\n            inputs[k] = data[v]\n\n    output = self.transform(**inputs)\n\n    if isinstance(self.output, str):\n        data[self.output] = output\n    else:  # Sequence[str]\n        if not isinstance(output, Sequence):\n            raise TypeError(\n                f\"Node '{name}' output is expected to be a sequence due \"\n                f\"to output specification {self.output}: \"\n                f\"\\n{wl.pformat(output)}\\n\")\n        if len(self.output) != len(output):\n            raise ValueError(\n                f\"Node '{name}' output length mismatch: expected \"\n                f\"{len(self.output)} outputs ({self.output}), but got \"\n                f\"{len(output)} outputs:\\n{wl.pformat(output)}\\n\")\n\n        for o, v in zip(self.output, output):\n            data[o] = v\n\n    return data\n</code></pre>"},{"location":"extensions/graph/#abstract_dataloader.ext.graph.Transform","title":"abstract_dataloader.ext.graph.Transform","text":"<p>               Bases: <code>Transform[dict[str, Any], dict[str, Any]]</code></p> <p>Compose multiple callables forming a DAG into a transform.</p> <p>Warning</p> <p>Since the input data specifications are not provided at initialization, the graph execution order (or graph validity) is not statically determined, and result in runtime errors if invalid.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>Mapping[str, str] | None</code> <p>output data keys to produce as a mapping of output keys to graph data keys. If <code>None</code>, all values are returned.</p> <code>None</code> <code>keep_all</code> <code>bool</code> <p>keep references to all intermediate values and return them instead of decref-ing values which are no longer needed.</p> <code>False</code> <code>nodes</code> <code>Node | dict[str, Any]</code> <p>nodes in the graph, as keyword arguments where the key indicates a reference name for the node; any <code>dict</code> arguments are passed as key/value arguments to <code>Node</code>.</p> <code>{}</code> Source code in <code>src/abstract_dataloader/ext/graph.py</code> <pre><code>class Transform(spec.Transform[dict[str, Any], dict[str, Any]]):\n    \"\"\"Compose multiple callables forming a DAG into a transform.\n\n    !!! warning\n\n        Since the input data specifications are not provided at initialization,\n        the graph execution order (or graph validity) is not statically\n        determined, and result in runtime errors if invalid.\n\n    Args:\n        outputs: output data keys to produce as a mapping of output keys to\n            graph data keys. If `None`, all values are returned.\n        keep_all: keep references to all intermediate values and return them\n            instead of decref-ing values which are no longer needed.\n        nodes: nodes in the graph, as keyword arguments where the key indicates\n            a reference name for the node; any `dict` arguments are passed as\n            key/value arguments to [`Node`][^.].\n    \"\"\"\n\n    def __init__(\n        self, outputs: Mapping[str, str] | None = None, keep_all: bool = False,\n        **nodes: Node | dict[str, Any]\n    ) -&gt; None:\n        self.nodes = {\n            k: v if isinstance(v, Node) else Node(**v)\n            for k, v in nodes.items()}\n        self.outputs = outputs\n        self.keep_all = keep_all\n\n    def _err_disconnected(\n        self, data: dict[str, Any], incomplete: dict[str, Node]\n    ) -&gt; ValueError:\n        \"\"\"Format error message for disconnected nodes.\"\"\"\n        remaining = {k: list(v.inputs.values()) for k, v in incomplete.items()}\n        return ValueError(\n            f\"There are {len(incomplete)} nodes remaining, but \"\n            f\"all remaining nodes have at least one missing input.\\n\"\n            f\"Current inputs: {list(data.keys())}\\n\"\n            f\"Remaining stage requirements: {wl.pformat(remaining)}\")\n\n    def _decref(\n        self, data: dict[str, Any], incomplete: dict[str, Node]\n    ) -&gt; dict[str, Any]:\n        \"\"\"Decref unneeded data values.\"\"\"\n        if self.keep_all or self.outputs is None:\n            return data\n        else:\n            keep = set(self.outputs.values())\n            for node in self.nodes.values():\n                keep |= set(node.inputs.values())\n            return {k: v for k, v in data.items() if k in keep}\n\n    def __call__(self, data: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"Execute the transform graph on the input data.\n\n        Args:\n            data: input data to process.\n\n        Returns:\n            Processed data.\n        \"\"\"\n        incomplete = self.nodes.copy()\n\n        # Guaranteed to terminate:\n        # Each loop removes one node, or raises an error.\n        while len(incomplete) &gt; 0:\n            for name, node in incomplete.items():\n                if all(k in data for k in node.inputs.values()):\n                    data = node.apply(data, name=name)\n                    incomplete.pop(name)\n                    data = self._decref(data, incomplete)\n                    break  # break back into the while\n            else:\n                raise self._err_disconnected(data, incomplete)\n\n        if self.outputs is not None:\n            if self.keep_all:\n                for k, v in self.outputs.items():\n                    data[k] = data[v]\n                return data\n            else:\n                return {k: data[v] for k, v in self.outputs.items()}\n        else:\n            return data\n</code></pre>"},{"location":"extensions/graph/#abstract_dataloader.ext.graph.Transform.__call__","title":"__call__","text":"<pre><code>__call__(data: dict[str, Any]) -&gt; dict[str, Any]\n</code></pre> <p>Execute the transform graph on the input data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>input data to process.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Processed data.</p> Source code in <code>src/abstract_dataloader/ext/graph.py</code> <pre><code>def __call__(self, data: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"Execute the transform graph on the input data.\n\n    Args:\n        data: input data to process.\n\n    Returns:\n        Processed data.\n    \"\"\"\n    incomplete = self.nodes.copy()\n\n    # Guaranteed to terminate:\n    # Each loop removes one node, or raises an error.\n    while len(incomplete) &gt; 0:\n        for name, node in incomplete.items():\n            if all(k in data for k in node.inputs.values()):\n                data = node.apply(data, name=name)\n                incomplete.pop(name)\n                data = self._decref(data, incomplete)\n                break  # break back into the while\n        else:\n            raise self._err_disconnected(data, incomplete)\n\n    if self.outputs is not None:\n        if self.keep_all:\n            for k, v in self.outputs.items():\n                data[k] = data[v]\n            return data\n        else:\n            return {k: data[v] for k, v in self.outputs.items()}\n    else:\n        return data\n</code></pre>"},{"location":"extensions/lightning/","title":"Lightning","text":""},{"location":"extensions/lightning/#abstract_dataloader.ext.lightning","title":"abstract_dataloader.ext.lightning","text":"<p>Dataloader / Pytorch Bridge.</p> <p>Warning</p> <p>Pytorch lightning must be installed to use this module. This is not included in any extras; you will need to <code>pip install lightning</code> or add it to your dependencies.</p> <p>The provided data module is based on the following assumptions:</p> <ul> <li>All splits use the same transform <code>Pipeline</code>,     but each have a different <code>Dataset</code>.     This means that if any data augmentations are applied by the transform, the     <code>Dataset</code> should pass some <code>meta</code> information (i.e., whether in training     mode) as part of the data.</li> <li>In-training visualizations are always rendered from the same set of a     relatively small number of samples taken from the validation set.</li> <li>The same dataloader settings should be applied to all splits.</li> </ul> <p>Info</p> <p>Only sample-to-sample (<code>.transform</code>) and sample-to-batch (<code>.collate</code>) transforms are applied in the dataloader; the training loop is responsible for applying batch-to-batch (<code>.forward</code>) transforms.</p>"},{"location":"extensions/lightning/#abstract_dataloader.ext.lightning.ADLDataModule","title":"abstract_dataloader.ext.lightning.ADLDataModule","text":"<p>               Bases: <code>LightningDataModule</code>, <code>Generic[Raw, Transformed, Collated, Processed]</code></p> <p>Pytorch dataloader wrapper for ADL-compliant datasets.</p> <p>Info</p> <p>Train/val/test splits are not all required to be present; if any are not present, the corresponding <code>.{split}_dataloader()</code> will raise an error if called. Arbitrary split names are also allowed, though <code>train</code>, <code>val</code>, and <code>test</code> are expected for the <code>ADLDataModule.{train|val|test}_dataloader()</code> methods expected by pytorch lightning.</p> <p>Note</p> <p>The underlying (transformed) dataset is cached (i.e. the same dataset object will be used on each call), but the dataloader container is not.</p> Type Parameters <ul> <li><code>Raw</code>: raw data loaded from the dataset.</li> <li><code>Transformed</code>: data following CPU-side transform.</li> <li><code>Collated</code>: data format after collation; should be in pytorch tensors.</li> <li><code>Processed</code>: data after GPU-side transform.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Mapping[str, Callable[[], Dataset[Raw]] | Dataset[Raw]]</code> <p>datasets or dataset constructors for each split.</p> required <code>transforms</code> <code>Pipeline[Raw, Transformed, Collated, Processed]</code> <p>data transforms to apply.</p> required <code>batch_size</code> <code>int</code> <p>dataloader batch size.</p> <code>32</code> <code>samples</code> <code>int | Sequence[int]</code> <p>number of validation-set samples to prefetch for visualizations (or a list of indices to use). Note that these samples are always held in memory! Set <code>samples=0</code> to disable.</p> <code>0</code> <code>num_workers</code> <code>int</code> <p>number of worker processes during data loading and CPU-side processing; use <code>num_workers=0</code> to run in the main thread.</p> <code>32</code> <code>prefetch_factor</code> <code>int | None</code> <p>number of batches to fetch per worker. Must be <code>None</code> when <code>num_workers=0</code>.</p> <code>None</code> <code>subsample</code> <code>Mapping[str, int | float | None]</code> <p>Sample only a (low-discrepancy) subset of samples on each split specified here instead of using all samples.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>transforms</code> <p>data transforms which should be applied to the data; in particular, a <code>.forward()</code> GPU batch-to-batch stage which is expected to be handled by downstream model code.</p> Source code in <code>src/abstract_dataloader/ext/lightning.py</code> <pre><code>class ADLDataModule(\n    L.LightningDataModule, Generic[Raw, Transformed, Collated, Processed]\n):\n    \"\"\"Pytorch dataloader wrapper for ADL-compliant datasets.\n\n    !!! info\n\n        Train/val/test splits are not all required to be present; if any are\n        not present, the corresponding `.{split}_dataloader()` will raise an\n        error if called. Arbitrary split names are also allowed, though\n        `train`, `val`, and `test` are expected for the\n        `ADLDataModule.{train|val|test}_dataloader()` methods\n        [expected by pytorch lightning](\n        https://lightning.ai/docs/pytorch/stable/data/datamodule.html).\n\n    !!! note\n\n        The underlying (transformed) dataset is cached (i.e. the same\n        dataset object will be used on each call), but the dataloader\n        container is not.\n\n    Type Parameters:\n        - `Raw`: raw data loaded from the dataset.\n        - `Transformed`: data following CPU-side transform.\n        - `Collated`: data format after collation; should be in pytorch tensors.\n        - `Processed`: data after GPU-side transform.\n\n    Args:\n        dataset: datasets or dataset constructors for each split.\n        transforms: data transforms to apply.\n        batch_size: dataloader batch size.\n        samples: number of validation-set samples to prefetch for\n            visualizations (or a list of indices to use). Note that these\n            samples are always held in memory! Set `samples=0` to disable.\n        num_workers: number of worker processes during data loading and\n            CPU-side processing; use `num_workers=0` to run in the main thread.\n        prefetch_factor: number of batches to fetch per worker. Must be `None`\n            when `num_workers=0`.\n        subsample: Sample only a (low-discrepancy) subset of samples on each\n            split specified here instead of using all samples.\n\n    Attributes:\n        transforms: data transforms which should be applied to the data; in\n            particular, a `.forward()` GPU batch-to-batch stage which is\n            expected to be handled by downstream model code.\n    \"\"\"\n\n    def __init__(\n        self, dataset: Mapping[\n            str, Callable[[], spec.Dataset[Raw]] | spec.Dataset[Raw]],\n        transforms: spec.Pipeline[Raw, Transformed, Collated, Processed],\n        batch_size: int = 32, samples: int | Sequence[int] = 0,\n        num_workers: int = 32, prefetch_factor: int | None = None,\n        subsample: Mapping[str, int | float | None] = {}\n    ) -&gt; None:\n        super().__init__()\n\n        self.transforms = transforms\n        self._samples = samples\n        self._subsample = subsample\n\n        self._dataset = dataset\n        self._dataloader_args = {\n            \"batch_size\": batch_size, \"num_workers\": num_workers,\n            \"prefetch_factor\": prefetch_factor, \"pin_memory\": True,\n            \"collate_fn\": transforms.collate\n        }\n\n    @classmethod\n    def from_traces(\n        cls, dataset: Callable[[Sequence[str]], spec.Dataset[Raw]],\n        traces: Mapping[str, Sequence[str]],\n        transforms: spec.Pipeline[Raw, Transformed, Collated, Processed],\n        **kwargs: dict[str, Any]\n    ) -&gt; \"ADLDataModule[Raw, Transformed, Collated, Processed]\":\n        \"\"\"Create from a dataset constructor.\n\n        Args:\n            dataset: dataset constructor which takes a list of trace names\n                and returns a dataset object.\n            traces: mapping of split names to trace names; the dataset\n                constructor will be called with the trace names for each split.\n            transforms: data transforms to apply.\n            kwargs: see the class constructor.\n        \"\"\"\n        return cls(\n            dataset={k: partial(dataset, v) for k, v in traces.items()},\n            transforms=transforms,\n            **kwargs)  # type: ignore\n\n    @cached_property\n    def samples(self) -&gt; Collated | None:\n        \"\"\"Validation samples for rendering samples.\n\n        If a simple `samples: int` is specified, these samples are taken\n        uniformly `len(val) // samples` apart with padding on either side.\n\n        !!! warning\n\n            While this property is cached, accessing this property the first\n            time triggers a full load of the dataset validation split!\n\n        Returns:\n            Pre-loaded validation samples, nominally for generating\n                visualizations. If `samples=0` was specified, or no validation\n                split is provided, then no samples are returned.\n        \"\"\"\n        if self._samples != 0:\n            try:\n                val = self.dataset(\"val\")\n\n                if isinstance(self._samples, int):\n                    m = len(val) // self._samples // 2\n                    indices = np.linspace(\n                        m, len(val) - m, self._samples, dtype=np.int64)\n                else:\n                    indices = self._samples\n\n                sampled = [val[i] for i in indices]\n                return self.transforms.collate(sampled)\n            except KeyError:\n                return None\n        return None\n\n    @cache\n    def dataset(\n        self, split: Literal[\"train\", \"val\", \"test\"] = \"train\"\n    ) -&gt; adl_torch.TransformedDataset[Raw, Transformed]:\n        \"\"\"Get dataset for a given split, with sample transformation applied.\n\n        !!! info\n\n            If the a split is requested, and `subsample` is specified for that\n            split, a subsample transform (via\n            [`SampledDataset`][abstract_dataloader.ext.sample.]) is also\n            applied.\n\n        Args:\n            split: target split.\n\n        Returns:\n            Dataset for that split, using the partially bound constructor\n                passed to the `ADLDataModule`; the dataset is cached between\n                calls.\n        \"\"\"\n        if split not in self._dataset:\n            raise KeyError(\n                f\"No `{split}` split was provided to this DataModule. Only \"\n                f\"the following splits are present: \"\n                f\"{list(self._dataset.keys())}\")\n\n        dataset = self._dataset[split]\n        if not isinstance(dataset, spec.Dataset):\n            dataset = dataset()\n\n        subsample = self._subsample.get(split)\n        if subsample is not None:\n            dataset = SampledDataset(dataset, subsample)\n\n        return adl_torch.TransformedDataset(\n            dataset, transform=self.transforms.sample)\n\n    def train_dataloader(self) -&gt; DataLoader:\n        \"\"\"Get training dataloader (`shuffle=True, drop_last=True`).\"\"\"\n        return DataLoader(\n            self.dataset(\"train\"), shuffle=True, drop_last=True,\n            **self._dataloader_args)\n\n    def val_dataloader(self) -&gt; DataLoader:\n        \"\"\"Get validation dataloader (`shuffle=False, drop_last=True`).\"\"\"\n        return DataLoader(\n            self.dataset(\"val\"), shuffle=False, drop_last=True,\n            **self._dataloader_args)\n\n    def test_dataloader(self) -&gt; DataLoader:\n        \"\"\"Get test dataloader (`shuffle=False, drop_last=False`).\"\"\"\n        return DataLoader(\n            self.dataset(\"test\"), shuffle=False, drop_last=False,\n            **self._dataloader_args)\n</code></pre>"},{"location":"extensions/lightning/#abstract_dataloader.ext.lightning.ADLDataModule.samples","title":"samples  <code>cached</code> <code>property</code>","text":"<pre><code>samples: Collated | None\n</code></pre> <p>Validation samples for rendering samples.</p> <p>If a simple <code>samples: int</code> is specified, these samples are taken uniformly <code>len(val) // samples</code> apart with padding on either side.</p> <p>Warning</p> <p>While this property is cached, accessing this property the first time triggers a full load of the dataset validation split!</p> <p>Returns:</p> Type Description <code>Collated | None</code> <p>Pre-loaded validation samples, nominally for generating visualizations. If <code>samples=0</code> was specified, or no validation split is provided, then no samples are returned.</p>"},{"location":"extensions/lightning/#abstract_dataloader.ext.lightning.ADLDataModule.dataset","title":"dataset  <code>cached</code>","text":"<pre><code>dataset(\n    split: Literal[\"train\", \"val\", \"test\"] = \"train\",\n) -&gt; TransformedDataset[Raw, Transformed]\n</code></pre> <p>Get dataset for a given split, with sample transformation applied.</p> <p>Info</p> <p>If the a split is requested, and <code>subsample</code> is specified for that split, a subsample transform (via <code>SampledDataset</code>) is also applied.</p> <p>Parameters:</p> Name Type Description Default <code>split</code> <code>Literal['train', 'val', 'test']</code> <p>target split.</p> <code>'train'</code> <p>Returns:</p> Type Description <code>TransformedDataset[Raw, Transformed]</code> <p>Dataset for that split, using the partially bound constructor passed to the <code>ADLDataModule</code>; the dataset is cached between calls.</p> Source code in <code>src/abstract_dataloader/ext/lightning.py</code> <pre><code>@cache\ndef dataset(\n    self, split: Literal[\"train\", \"val\", \"test\"] = \"train\"\n) -&gt; adl_torch.TransformedDataset[Raw, Transformed]:\n    \"\"\"Get dataset for a given split, with sample transformation applied.\n\n    !!! info\n\n        If the a split is requested, and `subsample` is specified for that\n        split, a subsample transform (via\n        [`SampledDataset`][abstract_dataloader.ext.sample.]) is also\n        applied.\n\n    Args:\n        split: target split.\n\n    Returns:\n        Dataset for that split, using the partially bound constructor\n            passed to the `ADLDataModule`; the dataset is cached between\n            calls.\n    \"\"\"\n    if split not in self._dataset:\n        raise KeyError(\n            f\"No `{split}` split was provided to this DataModule. Only \"\n            f\"the following splits are present: \"\n            f\"{list(self._dataset.keys())}\")\n\n    dataset = self._dataset[split]\n    if not isinstance(dataset, spec.Dataset):\n        dataset = dataset()\n\n    subsample = self._subsample.get(split)\n    if subsample is not None:\n        dataset = SampledDataset(dataset, subsample)\n\n    return adl_torch.TransformedDataset(\n        dataset, transform=self.transforms.sample)\n</code></pre>"},{"location":"extensions/lightning/#abstract_dataloader.ext.lightning.ADLDataModule.from_traces","title":"from_traces  <code>classmethod</code>","text":"<pre><code>from_traces(\n    dataset: Callable[[Sequence[str]], Dataset[Raw]],\n    traces: Mapping[str, Sequence[str]],\n    transforms: Pipeline[Raw, Transformed, Collated, Processed],\n    **kwargs: dict[str, Any],\n) -&gt; ADLDataModule[Raw, Transformed, Collated, Processed]\n</code></pre> <p>Create from a dataset constructor.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Callable[[Sequence[str]], Dataset[Raw]]</code> <p>dataset constructor which takes a list of trace names and returns a dataset object.</p> required <code>traces</code> <code>Mapping[str, Sequence[str]]</code> <p>mapping of split names to trace names; the dataset constructor will be called with the trace names for each split.</p> required <code>transforms</code> <code>Pipeline[Raw, Transformed, Collated, Processed]</code> <p>data transforms to apply.</p> required <code>kwargs</code> <code>dict[str, Any]</code> <p>see the class constructor.</p> <code>{}</code> Source code in <code>src/abstract_dataloader/ext/lightning.py</code> <pre><code>@classmethod\ndef from_traces(\n    cls, dataset: Callable[[Sequence[str]], spec.Dataset[Raw]],\n    traces: Mapping[str, Sequence[str]],\n    transforms: spec.Pipeline[Raw, Transformed, Collated, Processed],\n    **kwargs: dict[str, Any]\n) -&gt; \"ADLDataModule[Raw, Transformed, Collated, Processed]\":\n    \"\"\"Create from a dataset constructor.\n\n    Args:\n        dataset: dataset constructor which takes a list of trace names\n            and returns a dataset object.\n        traces: mapping of split names to trace names; the dataset\n            constructor will be called with the trace names for each split.\n        transforms: data transforms to apply.\n        kwargs: see the class constructor.\n    \"\"\"\n    return cls(\n        dataset={k: partial(dataset, v) for k, v in traces.items()},\n        transforms=transforms,\n        **kwargs)  # type: ignore\n</code></pre>"},{"location":"extensions/lightning/#abstract_dataloader.ext.lightning.ADLDataModule.test_dataloader","title":"test_dataloader","text":"<pre><code>test_dataloader() -&gt; DataLoader\n</code></pre> <p>Get test dataloader (<code>shuffle=False, drop_last=False</code>).</p> Source code in <code>src/abstract_dataloader/ext/lightning.py</code> <pre><code>def test_dataloader(self) -&gt; DataLoader:\n    \"\"\"Get test dataloader (`shuffle=False, drop_last=False`).\"\"\"\n    return DataLoader(\n        self.dataset(\"test\"), shuffle=False, drop_last=False,\n        **self._dataloader_args)\n</code></pre>"},{"location":"extensions/lightning/#abstract_dataloader.ext.lightning.ADLDataModule.train_dataloader","title":"train_dataloader","text":"<pre><code>train_dataloader() -&gt; DataLoader\n</code></pre> <p>Get training dataloader (<code>shuffle=True, drop_last=True</code>).</p> Source code in <code>src/abstract_dataloader/ext/lightning.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n    \"\"\"Get training dataloader (`shuffle=True, drop_last=True`).\"\"\"\n    return DataLoader(\n        self.dataset(\"train\"), shuffle=True, drop_last=True,\n        **self._dataloader_args)\n</code></pre>"},{"location":"extensions/lightning/#abstract_dataloader.ext.lightning.ADLDataModule.val_dataloader","title":"val_dataloader","text":"<pre><code>val_dataloader() -&gt; DataLoader\n</code></pre> <p>Get validation dataloader (<code>shuffle=False, drop_last=True</code>).</p> Source code in <code>src/abstract_dataloader/ext/lightning.py</code> <pre><code>def val_dataloader(self) -&gt; DataLoader:\n    \"\"\"Get validation dataloader (`shuffle=False, drop_last=True`).\"\"\"\n    return DataLoader(\n        self.dataset(\"val\"), shuffle=False, drop_last=True,\n        **self._dataloader_args)\n</code></pre>"},{"location":"extensions/objective/","title":"Objective","text":""},{"location":"extensions/objective/#abstract_dataloader.ext.objective","title":"abstract_dataloader.ext.objective","text":"<p>Objective base classes and specifications.</p> <p>Programming Model</p> <ul> <li>An <code>Objective</code> is a callable which returns a (batched) scalar loss   and a dictionary of metrics.</li> <li>Objectives can be combined into a higher-order objective,   <code>MultiObjective</code>, which combines their losses and aggregates their   metrics; specify these objectives using a <code>MultiObjectiveSpec</code>.</li> </ul>"},{"location":"extensions/objective/#abstract_dataloader.ext.objective.MultiObjective","title":"abstract_dataloader.ext.objective.MultiObjective","text":"<p>               Bases: <code>Objective[TArray, YTrue, YPred]</code></p> <p>Composite objective that combines multiple objectives.</p> Hydra Configuration <p>If using Hydra for dependency injection, a <code>MultiObjective</code> configuration should look like this: <pre><code>objectives:\nname:\n    objective:\n        _target_: ...\n        kwargs: ...\n    weight: 1.0\n    y_true: \"y_true_key\"\n    y_pred: \"y_pred_key\"\n...\n</code></pre></p> Type Parameters <ul> <li><code>YTrue</code>: ground truth data type.</li> <li><code>YHat</code>: model output data type.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>objectives</code> <code>Mapping | MultiObjectiveSpec</code> <p>multiple objectives, organized by name; see <code>MultiObjectiveSpec</code>. Each objective can also be provided as a dict, in which case the key/values are passed to <code>MultiObjectiveSpec</code>.</p> <code>{}</code> Source code in <code>src/abstract_dataloader/ext/objective.py</code> <pre><code>class MultiObjective(Objective[TArray, YTrue, YPred]):\n    \"\"\"Composite objective that combines multiple objectives.\n\n    ??? example \"Hydra Configuration\"\n\n        If using [Hydra](https://hydra.cc/docs/intro/) for dependency\n        injection, a `MultiObjective` configuration should look like this:\n        ```yaml\n        objectives:\n        name:\n            objective:\n                _target_: ...\n                kwargs: ...\n            weight: 1.0\n            y_true: \"y_true_key\"\n            y_pred: \"y_pred_key\"\n        ...\n        ```\n\n    Type Parameters:\n        - `YTrue`: ground truth data type.\n        - `YHat`: model output data type.\n\n    Args:\n        objectives: multiple objectives, organized by name; see\n            [`MultiObjectiveSpec`][^.]. Each objective can also be provided as\n            a dict, in which case the key/values are passed to\n            `MultiObjectiveSpec`.\n    \"\"\"\n\n    def __init__(self, **objectives: Mapping | MultiObjectiveSpec) -&gt; None:\n        if len(objectives) == 0:\n            raise ValueError(\"At least one objective must be provided.\")\n\n        self.objectives = {\n            k: v if isinstance(v, MultiObjectiveSpec)\n            else MultiObjectiveSpec(**v)\n            for k, v in objectives.items()}\n\n    def __call__(\n        self, y_true: YTrue, y_pred: YPred, train: bool = True\n    ) -&gt; tuple[Float[TArray, \"batch\"], dict[str, Float[TArray, \"batch\"]]]:\n        loss = 0.\n        metrics = {}\n        for k, v in self.objectives.items():\n            k_loss, k_metrics = v.objective(\n                v.index_y_true(y_true), v.index_y_pred(y_pred), train=train)\n            loss += k_loss * v.weight\n\n            for name, value in k_metrics.items():\n                metrics[f\"{k}/{name}\"] = value\n\n        # We assure that there's at least one objective.\n        loss = cast(Float[TArray, \"\"] | Float[TArray, \"batch\"], loss)\n        return loss, metrics\n\n    def visualizations(\n        self, y_true: YTrue, y_pred: YPred\n    ) -&gt; dict[str, UInt8[np.ndarray, \"H W 3\"]]:\n        images = {}\n        for k, v in self.objectives.items():\n            k_images = v.objective.visualizations(\n                v.index_y_true(y_true), v.index_y_pred(y_pred))\n            for name, image in k_images.items():\n                images[f\"{k}/{name}\"] = image\n        return images\n\n    def render(\n        self, y_true: YTrue, y_pred: YPred, render_gt: bool = False\n    ) -&gt; dict[str, Shaped[np.ndarray, \"batch ...\"]]:\n        rendered = {}\n        for k, v in self.objectives.items():\n            k_rendered = v.objective.render(\n                v.index_y_true(y_true), v.index_y_pred(y_pred),\n                render_gt=render_gt)\n            for name, image in k_rendered.items():\n                rendered[f\"{k}/{name}\"] = image\n        return rendered\n</code></pre>"},{"location":"extensions/objective/#abstract_dataloader.ext.objective.MultiObjectiveSpec","title":"abstract_dataloader.ext.objective.MultiObjectiveSpec  <code>dataclass</code>","text":"<p>               Bases: <code>Generic[YTrue, YPred, YTrueAll, YPredAll]</code></p> <p>Specification for a single objective in a multi-objective setup.</p> <p>The inputs and outputs for each objective are specified using <code>y_true</code> and <code>y_pred</code>:</p> <ul> <li><code>None</code>: The provided <code>y_true</code> and <code>y_pred</code> are passed directly to the     objective. This means that if multiple objectives all use <code>None</code>, they     will all receive the same data that comes from the dataloader.</li> <li><code>str</code>: The key indexes into a mapping which has the <code>y_true</code>/<code>y_pred</code> key,     or an object which has a matching attribute.</li> <li><code>Sequence[str]</code>: Each key indexes into the layers of a nested mapping or     object.</li> <li><code>Callable</code>: The callable is applied to the provided <code>y_true</code> and <code>y_pred</code>.</li> </ul> <p>Warning</p> <p>The user is responsible for ensuring that the <code>y_true</code> and <code>y_pred</code> keys or callables index the appropriate types for this objective.</p> Type Parameters <ul> <li><code>YTrue</code>: objective ground truth data type.</li> <li><code>YHat</code>: objective model prediction data type.</li> <li><code>YTrueAll</code>: type of all ground truth data (as loaded by the     dataloader).</li> <li><code>YHatAll</code>: type of all model output data (as produced by the model).</li> </ul> <p>Attributes:</p> Name Type Description <code>objective</code> <code>Objective</code> <p>The objective to use.</p> <code>weight</code> <code>float</code> <p>Weight of the objective in the overall loss.</p> <code>y_true</code> <code>str | Sequence[str] | Callable[[YTrueAll], YTrue] | None</code> <p>Key or callable to index into the ground truth data.</p> <code>y_pred</code> <code>str | Sequence[str] | Callable[[YPredAll], YPred] | None</code> <p>Key or callable to index into the model output data.</p> Source code in <code>src/abstract_dataloader/ext/objective.py</code> <pre><code>@dataclass\nclass MultiObjectiveSpec(Generic[YTrue, YPred, YTrueAll, YPredAll]):\n    \"\"\"Specification for a single objective in a multi-objective setup.\n\n    The inputs and outputs for each objective are specified using `y_true` and\n    `y_pred`:\n\n    - `None`: The provided `y_true` and `y_pred` are passed directly to the\n        objective. This means that if multiple objectives all use `None`, they\n        will all receive the same data that comes from the dataloader.\n    - `str`: The key indexes into a mapping which has the `y_true`/`y_pred` key,\n        or an object which has a matching attribute.\n    - `Sequence[str]`: Each key indexes into the layers of a nested mapping or\n        object.\n    - `Callable`: The callable is applied to the provided `y_true` and `y_pred`.\n\n    !!! warning\n\n        The user is responsible for ensuring that the `y_true` and `y_pred`\n        keys or callables index the appropriate types for this objective.\n\n    Type Parameters:\n        - `YTrue`: objective ground truth data type.\n        - `YHat`: objective model prediction data type.\n        - `YTrueAll`: type of all ground truth data (as loaded by the\n            dataloader).\n        - `YHatAll`: type of all model output data (as produced by the model).\n\n    Attributes:\n        objective: The objective to use.\n        weight: Weight of the objective in the overall loss.\n        y_true: Key or callable to index into the ground truth data.\n        y_pred: Key or callable to index into the model output data.\n    \"\"\"\n\n    objective: Objective\n    weight: float = 1.0\n    y_true: str | Sequence[str] | Callable[[YTrueAll], YTrue] | None = None\n    y_pred: str | Sequence[str] | Callable[[YPredAll], YPred] | None = None\n\n    def _index(\n        self, data: Any, key: str | Sequence[str] | Callable | None\n    ) -&gt; Any:\n        \"\"\"Index into data using the key or callable.\"\"\"\n        def dereference(obj, k):\n            if isinstance(obj, Mapping):\n                try:\n                    return obj[k]\n                except KeyError as e:\n                    raise KeyError(\n                        f\"Key {k} not found: {wl.pformat(obj)}\") from e\n            else:\n                try:\n                    return getattr(obj, k)\n                except AttributeError as e:\n                    raise AttributeError(\n                        f\"Attribute {k} not found: {wl.pformat(obj)}\") from e\n\n        if isinstance(key, str):\n            return dereference(data, key)\n        elif isinstance(key, Sequence):\n            for k in key:\n                data = dereference(data, k)\n            return data\n        elif callable(key):\n            return key(data)\n        else:   # key is None\n            return data\n\n    def index_y_true(self, y_true: YTrueAll) -&gt; YTrue:\n        \"\"\"Get indexed ground truth data.\n\n        Args:\n            y_true: All ground truth data (as loaded by the dataloader).\n\n        Returns:\n            Indexed ground truth data.\n        \"\"\"\n        return self._index(y_true, self.y_true)\n\n    def index_y_pred(self, y_pred: YPredAll) -&gt; YPred:\n        \"\"\"Get indexed model output data.\n\n        Args:\n            y_pred: All model output data (as produced by the model).\n\n        Returns:\n            Indexed model output data.\n        \"\"\"\n        return self._index(y_pred, self.y_pred)\n</code></pre>"},{"location":"extensions/objective/#abstract_dataloader.ext.objective.MultiObjectiveSpec.index_y_pred","title":"index_y_pred","text":"<pre><code>index_y_pred(y_pred: YPredAll) -&gt; YPred\n</code></pre> <p>Get indexed model output data.</p> <p>Parameters:</p> Name Type Description Default <code>y_pred</code> <code>YPredAll</code> <p>All model output data (as produced by the model).</p> required <p>Returns:</p> Type Description <code>YPred</code> <p>Indexed model output data.</p> Source code in <code>src/abstract_dataloader/ext/objective.py</code> <pre><code>def index_y_pred(self, y_pred: YPredAll) -&gt; YPred:\n    \"\"\"Get indexed model output data.\n\n    Args:\n        y_pred: All model output data (as produced by the model).\n\n    Returns:\n        Indexed model output data.\n    \"\"\"\n    return self._index(y_pred, self.y_pred)\n</code></pre>"},{"location":"extensions/objective/#abstract_dataloader.ext.objective.MultiObjectiveSpec.index_y_true","title":"index_y_true","text":"<pre><code>index_y_true(y_true: YTrueAll) -&gt; YTrue\n</code></pre> <p>Get indexed ground truth data.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>YTrueAll</code> <p>All ground truth data (as loaded by the dataloader).</p> required <p>Returns:</p> Type Description <code>YTrue</code> <p>Indexed ground truth data.</p> Source code in <code>src/abstract_dataloader/ext/objective.py</code> <pre><code>def index_y_true(self, y_true: YTrueAll) -&gt; YTrue:\n    \"\"\"Get indexed ground truth data.\n\n    Args:\n        y_true: All ground truth data (as loaded by the dataloader).\n\n    Returns:\n        Indexed ground truth data.\n    \"\"\"\n    return self._index(y_true, self.y_true)\n</code></pre>"},{"location":"extensions/objective/#abstract_dataloader.ext.objective.Objective","title":"abstract_dataloader.ext.objective.Objective","text":"<p>               Bases: <code>Protocol</code>, <code>Generic[TArray, YTrue, YPred]</code></p> <p>Composable training objective.</p> <p>Note</p> <p>Metrics should use <code>torch.no_grad()</code> to make sure gradients are not computed for non-loss metrics!</p> Type Parameters <ul> <li><code>TArray</code>: backend (<code>jax.Array</code>, <code>torch.Tensor</code>, etc.)</li> <li><code>YTrue</code>: ground truth data type.</li> <li><code>YPred</code>: model output data type.</li> </ul> Source code in <code>src/abstract_dataloader/ext/objective.py</code> <pre><code>@runtime_checkable\nclass Objective(Protocol, Generic[TArray, YTrue, YPred]):\n    \"\"\"Composable training objective.\n\n    !!! note\n\n        Metrics should use `torch.no_grad()` to make sure gradients are not\n        computed for non-loss metrics!\n\n    Type Parameters:\n        - `TArray`: backend (`jax.Array`, `torch.Tensor`, etc.)\n        - `YTrue`: ground truth data type.\n        - `YPred`: model output data type.\n    \"\"\"\n\n    @abstractmethod\n    def __call__(\n        self, y_true: YTrue, y_pred: YPred, train: bool = True\n    ) -&gt; tuple[Float[TArray, \"batch\"], dict[str, Float[TArray, \"batch\"]]]:\n        \"\"\"Training metrics implementation.\n\n        Args:\n            y_true: data channels (i.e. dataloader output).\n            y_pred: model outputs.\n            train: Whether in training mode (i.e. skip expensive metrics).\n\n        Returns:\n            A tuple containing the loss and a dict of metric values.\n        \"\"\"\n        ...\n\n    def visualizations(\n        self, y_true: YTrue, y_pred: YPred\n    ) -&gt; dict[str, UInt8[np.ndarray, \"H W 3\"]]:\n        \"\"\"Generate visualizations for each entry in a batch.\n\n        This method may return an empty dict.\n\n        !!! note\n\n            This method should be called only from a \"detached\" CPU thread so\n            as not to affect training throughput; the caller is responsible for\n            detaching gradients and sending the data to the CPU. As such,\n            implementations are free to use CPU-specific methods.\n\n        Args:\n            y_true: data channels (i.e., dataloader output).\n            y_pred: model outputs.\n\n        Returns:\n            A dict, where each key is the name of a visualization, and the\n                value is a stack of RGB images in HWC order, detached from\n                Torch and sent to a numpy array.\n        \"\"\"\n        ...\n\n    def render(\n        self, y_true: YTrue, y_pred: YPred, render_gt: bool = False\n    ) -&gt; dict[str, Shaped[np.ndarray, \"batch ...\"]]:\n        \"\"\"Render model outputs and/or ground truth for later analysis.\n\n        This method may return an empty dict.\n\n        ??? question \"How does this differ from `visualizations`?\"\n\n            Unlike `visualizations`, which is expected to return a single\n            RGB image per batch, `render` is:\n\n            - expected to return a unique rendered value per sample, and\n            - may have arbitrary types (as long as they are a numpy arrays).\n\n        Args:\n            y_true: data channels (i.e. dataloader output).\n            y_pred: model outputs.\n            render_gt: whether to render ground truth data.\n\n        Returns:\n            A dict, where each key is the name of a rendered output, and the\n                value is a numpy array of the rendered data (e.g., an image).\n        \"\"\"\n        ...\n</code></pre>"},{"location":"extensions/objective/#abstract_dataloader.ext.objective.Objective.__call__","title":"__call__  <code>abstractmethod</code>","text":"<pre><code>__call__(\n    y_true: YTrue, y_pred: YPred, train: bool = True\n) -&gt; tuple[Float[TArray, batch], dict[str, Float[TArray, batch]]]\n</code></pre> <p>Training metrics implementation.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>YTrue</code> <p>data channels (i.e. dataloader output).</p> required <code>y_pred</code> <code>YPred</code> <p>model outputs.</p> required <code>train</code> <code>bool</code> <p>Whether in training mode (i.e. skip expensive metrics).</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple[Float[TArray, batch], dict[str, Float[TArray, batch]]]</code> <p>A tuple containing the loss and a dict of metric values.</p> Source code in <code>src/abstract_dataloader/ext/objective.py</code> <pre><code>@abstractmethod\ndef __call__(\n    self, y_true: YTrue, y_pred: YPred, train: bool = True\n) -&gt; tuple[Float[TArray, \"batch\"], dict[str, Float[TArray, \"batch\"]]]:\n    \"\"\"Training metrics implementation.\n\n    Args:\n        y_true: data channels (i.e. dataloader output).\n        y_pred: model outputs.\n        train: Whether in training mode (i.e. skip expensive metrics).\n\n    Returns:\n        A tuple containing the loss and a dict of metric values.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"extensions/objective/#abstract_dataloader.ext.objective.Objective.render","title":"render","text":"<pre><code>render(\n    y_true: YTrue, y_pred: YPred, render_gt: bool = False\n) -&gt; dict[str, Shaped[ndarray, \"batch ...\"]]\n</code></pre> <p>Render model outputs and/or ground truth for later analysis.</p> <p>This method may return an empty dict.</p> How does this differ from <code>visualizations</code>? <p>Unlike <code>visualizations</code>, which is expected to return a single RGB image per batch, <code>render</code> is:</p> <ul> <li>expected to return a unique rendered value per sample, and</li> <li>may have arbitrary types (as long as they are a numpy arrays).</li> </ul> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>YTrue</code> <p>data channels (i.e. dataloader output).</p> required <code>y_pred</code> <code>YPred</code> <p>model outputs.</p> required <code>render_gt</code> <code>bool</code> <p>whether to render ground truth data.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, Shaped[ndarray, 'batch ...']]</code> <p>A dict, where each key is the name of a rendered output, and the value is a numpy array of the rendered data (e.g., an image).</p> Source code in <code>src/abstract_dataloader/ext/objective.py</code> <pre><code>def render(\n    self, y_true: YTrue, y_pred: YPred, render_gt: bool = False\n) -&gt; dict[str, Shaped[np.ndarray, \"batch ...\"]]:\n    \"\"\"Render model outputs and/or ground truth for later analysis.\n\n    This method may return an empty dict.\n\n    ??? question \"How does this differ from `visualizations`?\"\n\n        Unlike `visualizations`, which is expected to return a single\n        RGB image per batch, `render` is:\n\n        - expected to return a unique rendered value per sample, and\n        - may have arbitrary types (as long as they are a numpy arrays).\n\n    Args:\n        y_true: data channels (i.e. dataloader output).\n        y_pred: model outputs.\n        render_gt: whether to render ground truth data.\n\n    Returns:\n        A dict, where each key is the name of a rendered output, and the\n            value is a numpy array of the rendered data (e.g., an image).\n    \"\"\"\n    ...\n</code></pre>"},{"location":"extensions/objective/#abstract_dataloader.ext.objective.Objective.visualizations","title":"visualizations","text":"<pre><code>visualizations(\n    y_true: YTrue, y_pred: YPred\n) -&gt; dict[str, UInt8[ndarray, \"H W 3\"]]\n</code></pre> <p>Generate visualizations for each entry in a batch.</p> <p>This method may return an empty dict.</p> <p>Note</p> <p>This method should be called only from a \"detached\" CPU thread so as not to affect training throughput; the caller is responsible for detaching gradients and sending the data to the CPU. As such, implementations are free to use CPU-specific methods.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>YTrue</code> <p>data channels (i.e., dataloader output).</p> required <code>y_pred</code> <code>YPred</code> <p>model outputs.</p> required <p>Returns:</p> Type Description <code>dict[str, UInt8[ndarray, 'H W 3']]</code> <p>A dict, where each key is the name of a visualization, and the value is a stack of RGB images in HWC order, detached from Torch and sent to a numpy array.</p> Source code in <code>src/abstract_dataloader/ext/objective.py</code> <pre><code>def visualizations(\n    self, y_true: YTrue, y_pred: YPred\n) -&gt; dict[str, UInt8[np.ndarray, \"H W 3\"]]:\n    \"\"\"Generate visualizations for each entry in a batch.\n\n    This method may return an empty dict.\n\n    !!! note\n\n        This method should be called only from a \"detached\" CPU thread so\n        as not to affect training throughput; the caller is responsible for\n        detaching gradients and sending the data to the CPU. As such,\n        implementations are free to use CPU-specific methods.\n\n    Args:\n        y_true: data channels (i.e., dataloader output).\n        y_pred: model outputs.\n\n    Returns:\n        A dict, where each key is the name of a visualization, and the\n            value is a stack of RGB images in HWC order, detached from\n            Torch and sent to a numpy array.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"extensions/objective/#abstract_dataloader.ext.objective.VisualizationConfig","title":"abstract_dataloader.ext.objective.VisualizationConfig  <code>dataclass</code>","text":"<p>General-purpose visualization configuration.</p> <p>Objectives which make use of this configuration may ignore the provided values.</p> <p>Attributes:</p> Name Type Description <code>cols</code> <code>int</code> <p>number of columns to tile images for in-training visualizations.</p> <code>width</code> <code>int</code> <p>width of each sample when rendered.</p> <code>height</code> <code>int</code> <p>height of each sample when rendered.</p> <code>cmaps</code> <code>Mapping[str, str | UInt8[ndarray, 'N 3']]</code> <p>colormaps to use, where values correspond to the name of a matplotlib colormap or a numpy array of enumerated RGB values.</p> Source code in <code>src/abstract_dataloader/ext/objective.py</code> <pre><code>@dataclass(frozen=True)\nclass VisualizationConfig:\n    \"\"\"General-purpose visualization configuration.\n\n    Objectives which make use of this configuration may ignore the provided\n    values.\n\n    Attributes:\n        cols: number of columns to tile images for in-training visualizations.\n        width: width of each sample when rendered.\n        height: height of each sample when rendered.\n        cmaps: colormaps to use, where values correspond to the name of a\n            matplotlib colormap or a numpy array of enumerated RGB values.\n    \"\"\"\n\n    cols: int = 8\n    width: int = 512\n    height: int = 256\n    cmaps: Mapping[\n        str, str | UInt8[np.ndarray, \"N 3\"]] = field(default_factory=dict)\n</code></pre>"},{"location":"extensions/sample/","title":"Sampling","text":""},{"location":"extensions/sample/#abstract_dataloader.ext.sample","title":"abstract_dataloader.ext.sample","text":"<p>Dataset sampling, including a low discrepancy subset sampler.</p> <p>Dataset sampling is implemented using a <code>SampledDataset</code>, which transparently wraps an existing <code>Dataset</code>.</p>"},{"location":"extensions/sample/#abstract_dataloader.ext.sample.SampledDataset","title":"abstract_dataloader.ext.sample.SampledDataset","text":"<p>               Bases: <code>Dataset[TSample]</code>, <code>Generic[TSample]</code></p> <p>Dataset wrapper which only exposes a subset of values.</p> <p>The sampling <code>mode</code> can be one of:</p> <ul> <li><code>random</code>: Uniform random sampling, with <code>np.random.default_rng</code> and the   supplied seed; if <code>seed</code> is a <code>float</code>, it is converted into an integer   by multiplying by <code>len(dataset)</code> and rounding.</li> <li><code>ld</code>: Low discrepancy sampling; see <code>sample_ld</code>.</li> <li><code>uniform</code>: Uniformly spaced sampling, with <code>linspace(0, n, samples)</code>.</li> <li><code>Callable</code>: A callable which takes the total number of samples, and     returns an array of indices to sample from the dataset.</li> </ul> <p>Info</p> <p>This <code>SampledDataset</code> is fully ADL-compliant, and acts as a passthrough to an ADL-compliant <code>Dataset</code>: if the input dataset is a <code>Dataset[Sample]</code>, then the wrapped dataset is also a <code>Dataset[Sample]</code>.</p> Type Parameters <p><code>Sample</code>: dataset sample type.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset[TSample]</code> <p>underlying dataset.</p> required <code>samples</code> <code>int | float</code> <p>target number of samples.</p> required <code>seed</code> <code>int | float</code> <p>sampler seed.</p> <code>0</code> <code>mode</code> <code>Literal['ld', 'uniform', 'random'] | Callable[[int], Integer[ndarray, N]]</code> <p>sampling mode.</p> <code>'ld'</code> Source code in <code>src/abstract_dataloader/ext/sample.py</code> <pre><code>class SampledDataset(spec.Dataset[TSample], Generic[TSample]):\n    \"\"\"Dataset wrapper which only exposes a subset of values.\n\n    The sampling `mode` can be one of:\n\n    - `random`: Uniform random sampling, with `np.random.default_rng` and the\n      supplied seed; if `seed` is a `float`, it is converted into an integer\n      by multiplying by `len(dataset)` and rounding.\n    - `ld`: Low discrepancy sampling; see [`sample_ld`][^.].\n    - `uniform`: Uniformly spaced sampling, with `linspace(0, n, samples)`.\n    - `Callable`: A callable which takes the total number of samples, and\n        returns an array of indices to sample from the dataset.\n\n    !!! info\n\n        This `SampledDataset` is fully ADL-compliant, and acts as a passthrough\n        to an ADL-compliant [`Dataset`][abstract_dataloader.spec.]: if the\n        input dataset is a `Dataset[Sample]`, then the wrapped dataset is also\n        a `Dataset[Sample]`.\n\n    Type Parameters:\n        `Sample`: dataset sample type.\n\n    Args:\n        dataset: underlying dataset.\n        samples: target number of samples.\n        seed: sampler seed.\n        mode: sampling mode.\n    \"\"\"\n\n    def __init__(\n        self, dataset: spec.Dataset[TSample], samples: int | float,\n        seed: int | float = 0,\n        mode: Literal[\"ld\", \"uniform\", \"random\"]\n            | Callable[[int], Integer[np.ndarray, \"N\"]] = \"ld\"\n    ) -&gt; None:\n        self.dataset = dataset\n\n        if isinstance(samples, float):\n            samples = int(samples * len(dataset))\n\n        if mode == \"ld\":\n            self.subset = sample_ld(len(dataset), samples=samples, seed=seed)\n        elif mode == \"random\":\n            if isinstance(seed, float):\n                seed = int(seed * len(dataset))\n            self.subset = np.random.default_rng(seed).choice(\n                len(dataset), size=samples, replace=True)\n        elif mode == \"uniform\":\n            self.subset = np.linspace(\n                0, len(dataset) - 1, samples, dtype=np.int64)\n        else:  # Callable\n            self.subset = mode(len(dataset)).astype(np.int64)\n\n    def __getitem__(self, index: int | np.integer) -&gt; TSample:\n        \"\"\"Fetch item from this dataset by global index.\"\"\"\n        return self.dataset[self.subset[index]]\n\n    def __len__(self) -&gt; int:\n        \"\"\"Total number of samples in this dataset.\"\"\"\n        return self.subset.shape[0]\n</code></pre>"},{"location":"extensions/sample/#abstract_dataloader.ext.sample.SampledDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(index: int | integer) -&gt; TSample\n</code></pre> <p>Fetch item from this dataset by global index.</p> Source code in <code>src/abstract_dataloader/ext/sample.py</code> <pre><code>def __getitem__(self, index: int | np.integer) -&gt; TSample:\n    \"\"\"Fetch item from this dataset by global index.\"\"\"\n    return self.dataset[self.subset[index]]\n</code></pre>"},{"location":"extensions/sample/#abstract_dataloader.ext.sample.SampledDataset.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Total number of samples in this dataset.</p> Source code in <code>src/abstract_dataloader/ext/sample.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Total number of samples in this dataset.\"\"\"\n    return self.subset.shape[0]\n</code></pre>"},{"location":"extensions/sample/#abstract_dataloader.ext.sample.sample_ld","title":"abstract_dataloader.ext.sample.sample_ld","text":"<pre><code>sample_ld(\n    total: int,\n    samples: float | int,\n    seed: float | int = 0,\n    alpha: float | int = 2 / sqrt(5) + 1,\n) -&gt; Int64[ndarray, samples]\n</code></pre> <p>Compute deterministic low-discrepancy subset mask.</p> <p>Uses a simple <code>alpha * n % 1</code> formulation, described here, with a modification to work with integer samples:</p> <ul> <li>For a given <code>total</code>, find the integer closest to <code>total * alpha</code> which   is co-prime with the total. Use this as the step size.</li> <li>Then, <code>1...total * alpha (mod total)</code> is guaranteed to visit each index   up to <code>total</code> exactly once.</li> </ul> <p>Note</p> <p>The default <code>alpha = 1 / phi</code> where <code>phi</code> is the golden ratio <code>(1 + sqrt(5)) / 2</code> has strong low-discrepancy sampling properties ; due to the quantized nature of this function, the discrepancy may be larger when <code>total</code> is small.</p> <p>Tip</p> <p>Each of the parameters (<code>samples</code>, <code>seed</code>, <code>alpha</code>) can be specified as a float <code>[0, 1]</code>, and a proportion of the <code>total</code> will be used instead. For example, if <code>seed = 0.7</code> and <code>total=100</code>, then <code>seed = 70</code> will be used.</p> <p>Parameters:</p> Name Type Description Default <code>total</code> <code>int</code> <p>total number of samples to sample from, i.e. maximum index.</p> required <code>samples</code> <code>float | int</code> <p>number of samples to generate. Should be less than <code>total</code>.</p> required <code>seed</code> <code>float | int</code> <p>initial offset for the sampling sequence. Can leave this at <code>0</code>.</p> <code>0</code> <code>alpha</code> <code>float | int</code> <p>step size in the sequence; the default value is the inverse golden ratio <code>2 / (np.sqrt(5) + 1)</code> (which is actually equivalent to the golden ratio <code>mod 1</code>, since <code>1 - phi = 1 / phi</code>).</p> <code>2 / sqrt(5) + 1</code> <p>Returns:</p> Type Description <code>Int64[ndarray, samples]</code> <p>Array, in mixed order, of <code>sample</code> indices which form a subset of <code>0...total</code> with a guarantee of no repeats.</p> Source code in <code>src/abstract_dataloader/ext/sample.py</code> <pre><code>def sample_ld(\n    total: int, samples: float | int,\n    seed: float | int = 0, alpha: float | int = 2 / (np.sqrt(5) + 1),\n) -&gt; Int64[np.ndarray, \"samples\"]:\n    \"\"\"Compute deterministic low-discrepancy subset mask.\n\n    Uses a simple `alpha * n % 1` formulation, described\n    [here](https://extremelearning.com.au/unreasonable-effectiveness-of-quasirandom-sequences/),\n    with a modification to work with integer samples:\n\n    - For a given `total`, find the integer closest to `total * alpha` which\n      is co-prime with the total. Use this as the step size.\n    - Then, `1...total * alpha (mod total)` is guaranteed to visit each index\n      up to `total` exactly once.\n\n    !!! note\n\n        The default `alpha = 1 / phi` where `phi` is the golden ratio\n        `(1 + sqrt(5)) / 2` has strong [low-discrepancy sampling properties\n        ](https://extremelearning.com.au/unreasonable-effectiveness-of-quasirandom-sequences/);\n        due to the quantized nature of this function, the discrepancy may be\n        larger when `total` is small.\n\n    !!! tip\n\n        Each of the parameters (`samples`, `seed`, `alpha`) can be\n        specified as a float `[0, 1]`, and a proportion of the `total` will\n        be used instead. For example, if `seed = 0.7` and `total=100`, then\n        `seed = 70` will be used.\n\n    Args:\n        total: total number of samples to sample from, i.e. maximum index.\n        samples: number of samples to generate. Should be less than `total`.\n        seed: initial offset for the sampling sequence. Can leave this at `0`.\n        alpha: step size in the sequence; the default value is the inverse\n            golden ratio `2 / (np.sqrt(5) + 1)` (which is actually equivalent\n            to the golden ratio `mod 1`, since `1 - phi = 1 / phi`).\n\n    Returns:\n        Array, in mixed order, of `sample` indices which form a subset of\n            `0...total` with a guarantee of no repeats.\n    \"\"\"\n    def _get_qld_step(n: int, q: int) -&gt; int:\n        \"\"\"Get quantized low-discrepancy step size.\"\"\"\n        for i in range(min(n - q, q)):\n            query =  q + (((i + 1) // 2) * (-1 if i % 2 == 0 else 1))\n            if np.gcd(query, n) == 1:\n                return query\n        raise ValueError(\n            \"No GCD found. This should be impossible! Is `n` degenerate?\")\n\n    if isinstance(samples, float):\n        samples = int(samples * total)\n    if isinstance(seed, float):\n        seed = int(seed * total)\n    if isinstance(alpha, float):\n        alpha = _get_qld_step(total, int(total * alpha))\n\n    if samples &gt; total or samples &lt; 0:\n        raise ValueError(\n            f\"Number of samples {samples} must be in [0, {total}].\")\n    if samples * alpha &gt;= np.iinfo(np.int64).max:\n        raise NotImplementedError(\n            f\"`samples={samples}` is too large, and will cause overflow.\")\n\n    return (np.arange(samples, dtype=np.int64) * alpha + seed) % total\n</code></pre>"},{"location":"extensions/types/","title":"Types","text":""},{"location":"extensions/types/#abstract_dataloader.ext.types","title":"abstract_dataloader.ext.types","text":"<p>Data type system framework following the ADL recommendations.</p> <p>Programming Model</p> <ul> <li>Declare types as dataclasses using the provided <code>dataclass</code>     decorator, which registers them to the global optree namespace.</li> <li>Set each class as <code>Generic[TArray]</code>, where <code>TArray</code> is a <code>TypeVar</code>     which is <code>ArrayLike</code>, e.g., <code>torch.Tensor</code>, <code>jax.Array</code> or     <code>np.ndarray</code>.</li> </ul> <p>Warning</p> <p>This module requires <code>optree</code> to be installed.</p>"},{"location":"extensions/types/#abstract_dataloader.ext.types.TArray","title":"abstract_dataloader.ext.types.TArray  <code>module-attribute</code>","text":"<pre><code>TArray = TypeVar('TArray', bound=ArrayLike)\n</code></pre> <p>Type variable for <code>ArrayLike</code> types.</p>"},{"location":"extensions/types/#abstract_dataloader.ext.types.ArrayLike","title":"abstract_dataloader.ext.types.ArrayLike","text":"<p>               Bases: <code>Protocol</code></p> <p>Array type, e.g., <code>torch.Tensor | jax.Array | np.ndarray</code>.</p> <p>Use this type to specify arbitrary array types.</p> Source code in <code>src/abstract_dataloader/ext/types.py</code> <pre><code>@runtime_checkable\nclass ArrayLike(Protocol):\n    \"\"\"Array type, e.g., `torch.Tensor | jax.Array | np.ndarray`.\n\n    Use this type to specify arbitrary array types.\n    \"\"\"\n\n    @property\n    def shape(self) -&gt; tuple[int, ...]: ...\n\n    @property\n    def dtype(self) -&gt; Any: ...\n</code></pre>"},{"location":"extensions/types/#abstract_dataloader.ext.types.dataclass","title":"abstract_dataloader.ext.types.dataclass","text":"<pre><code>dataclass(cls)\n</code></pre> <p>A dataclass decorator which registers into optree's global namespace.</p> Source code in <code>src/abstract_dataloader/ext/types.py</code> <pre><code>@dataclass_transform(field_specifiers=(field,))\ndef dataclass(cls):  # noqa: D103\n    \"\"\"A dataclass decorator which registers into optree's global namespace.\"\"\"\n    return _optree_dataclass(cls, namespace='')\n</code></pre>"}]}